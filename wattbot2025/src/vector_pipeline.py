import pandas as pd
import json
import yaml
import re
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm

from src.retrieval.hybrid_search import HybridRetriever, Chunk
from src.llm.answer_generator import AnswerGenerator


class VectorDBPipeline:
    def __init__(self, config_path: str = "configs/config.yaml"):
        """Initialize pipeline"""
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)

        print("ðŸš€ Initializing Vector DB Pipeline...")

        self.retriever = HybridRetriever(
            embedding_model=self.config['models']['embedding']['model']
        )
        self.generator = AnswerGenerator(config_path)

        self.chunks_dir = Path(self.config['paths']['chunks_dir'])
        self.index_path = Path(self.config['paths']['index_path'])

        # Load training examples for few-shot prompting
        self.train_df = pd.read_csv("data/raw/train_QA.csv")
        self.few_shot_examples = self._prepare_few_shot_examples()

        # Load metadata for ref_id mapping
        self.metadata_df = pd.read_csv("data/raw/metadata.csv", encoding='latin-1')
        self.valid_ref_ids = set(self.metadata_df['id'].str.strip().str.lower().tolist())

    def _prepare_few_shot_examples(self) -> Dict[str, List[Dict]]:
        """Organize training examples by question type."""
        examples = {
            "numeric": [],
            "true_false": [],
            "named_entity": [],
            "calculation": [],
        }

        for _, row in self.train_df.iterrows():
            q = row['question'].lower()
            example = {
                "question": row['question'],
                "answer": row['answer'],
                "answer_value": row['answer_value'],
                "answer_unit": row['answer_unit'],
            }

            if q.startswith("true or false"):
                examples["true_false"].append(example)
            elif "name of" in q or ("what is" in q and "name" in q):
                examples["named_entity"].append(example)
            elif "difference" in q or "compare" in q or "factor" in q:
                examples["calculation"].append(example)
            else:
                examples["numeric"].append(example)

        return examples

    def classify_question(self, question: str) -> str:
        """Classify question type for specialized handling."""
        q_lower = question.lower()

        if q_lower.startswith("true or false"):
            return "true_false"
        elif "name of" in q_lower or "what is the term" in q_lower or "what framework" in q_lower:
            return "named_entity"
        elif any(w in q_lower for w in ["difference between", "compare", "by what factor",
                                         "how many times", "what percentage", "by what percentage"]):
            return "calculation"
        else:
            return "numeric"

    def expand_query(self, question: str) -> List[str]:
        """Generate multiple query variants for better retrieval."""
        queries = [question]

        q_lower = question.lower()

        # Simplified version
        simplified = re.sub(r'^(what|how|which|when|where|who|true or false:?)\s+', '', q_lower)
        simplified = re.sub(r'\?$', '', simplified)
        queries.append(simplified)

        # Extract entities and metrics
        entities = re.findall(r'\b(BERT|GPT-\d|LLaMA|T5|BLOOM|OPT|Gemini|Claude|JetMoE|Mixtral|BlackMamba|FLM-101B)\b', question, re.I)
        metrics = re.findall(r'\b(CO2|carbon|emission|kWh|MWh|GWh|PUE|WUE|energy|water|GPU|training|inference)\b', question, re.I)

        if entities and metrics:
            queries.append(f"{' '.join(entities)} {' '.join(metrics)}")

        # For numeric questions, add table variant
        if any(w in q_lower for w in ["how much", "how many", "what is the", "what was the"]):
            queries.append(f"{simplified} table")

        return queries[:3]

    def get_few_shot_prompt(self, question_type: str, n_examples: int = 2) -> str:
        """Get relevant few-shot examples."""
        examples = self.few_shot_examples.get(question_type, [])[:n_examples]

        if not examples:
            return ""

        prompt_parts = ["Here are examples of similar questions and their correct answers:\n"]

        for ex in examples:
            prompt_parts.append(f"""
Question: {ex['question']}
Answer: {ex['answer']}
answer_value: {ex['answer_value']}
answer_unit: {ex['answer_unit']}
---""")

        return "\n".join(prompt_parts)

    def build_prompt(self, question: str, context: str, q_type: str) -> str:
        """Build prompt specialized for question type."""

        few_shot = self.get_few_shot_prompt(q_type, n_examples=2)

        base = """You are an expert at extracting precise answers from environmental AI research papers.
Your task is to find the EXACT answer to the question from the provided context.

CRITICAL RULES:
1. Extract EXACT numbers from the text - do not round or approximate
2. For True/False questions: answer_value must be "1" for TRUE or "0" for FALSE
3. If the context does NOT contain enough information to answer, you MUST return "is_blank" for answer_value
4. For ranges, use format [low,high] without spaces
5. Do not include symbols like <, >, ~ in answer_value - only the number"""

        type_instructions = {
            "true_false": """
QUESTION TYPE: True/False
- Your answer field should say "TRUE" or "FALSE"
- answer_value MUST be "1" for TRUE or "0" for FALSE (not the words TRUE/FALSE)
- Look for explicit statements that confirm or deny the claim
- If you cannot find clear evidence, answer_value should be "is_blank" """,

            "numeric": """
QUESTION TYPE: Numeric
- Extract the EXACT number from the context - do not approximate
- answer_value should be ONLY the number (e.g., "1438" not "1438 lbs")
- Include the unit in answer_unit (e.g., "lbs", "MWh", "percent")
- For ranges, use format "[low,high]" (e.g., "[80,90]")
- If the answer is not in the context, answer_value must be "is_blank" """,

            "named_entity": """
QUESTION TYPE: Named Entity / Term
- Extract the EXACT name or term as stated in the document
- answer_value should be the name itself (e.g., "ML.ENERGY Benchmark")
- answer_unit should be "is_blank" for named entities
- If the name/term is not found, answer_value must be "is_blank" """,

            "calculation": """
QUESTION TYPE: Calculation / Comparison
- You may need to perform simple math (division, subtraction, percentages)
- Show your calculation in the explanation field
- answer_value should be the final computed number
- If you cannot find the required numbers, answer_value must be "is_blank" """
        }

        return f"""{base}

{type_instructions.get(q_type, type_instructions['numeric'])}

{few_shot}

CONTEXT FROM RESEARCH PAPERS:
{context}

QUESTION: {question}

Respond with ONLY valid JSON:
{{
    "answer": "Natural language answer (e.g., 'TRUE', '1438 lbs', 'ML.ENERGY Benchmark') or 'Unable to answer with confidence based on the provided documents.'",
    "answer_value": "Just the value: number, '1'/'0' for T/F, term name, or 'is_blank' if unanswerable",
    "answer_unit": "Unit of measurement or 'is_blank'",
    "ref_id": "Document ID(s) that support your answer, or 'is_blank'",
    "supporting_materials": "Direct quote or table/figure reference, or 'is_blank'",
    "explanation": "Your reasoning, or 'is_blank'"
}}"""

    def format_ref_id(self, ref_ids: List[str]) -> str:
        """Format ref_id to match competition format: ['id1', 'id2']"""
        if not ref_ids:
            return "is_blank"

        # Validate against metadata
        valid_ids = []
        for rid in ref_ids:
            rid_clean = rid.strip().lower()
            if rid_clean in self.valid_ref_ids:
                # Get the original casing from metadata
                for orig_id in self.metadata_df['id']:
                    if orig_id.strip().lower() == rid_clean:
                        valid_ids.append(orig_id.strip())
                        break

        if not valid_ids:
            return "is_blank"

        # Format as ['id1', 'id2']
        return "['" + "', '".join(valid_ids) + "']"

    def post_process_answer(self, answer: Dict, q_type: str, doc_ids: set) -> Dict:
        """Post-process answer to match competition format exactly."""

        # Clean up None/empty values
        for key in answer:
            if answer[key] in [None, "", "null", "none", "None", "N/A", "n/a", "unknown"]:
                answer[key] = "is_blank"

        # Handle True/False - answer_value must be "1" or "0"
        if q_type == "true_false":
            ans_lower = str(answer.get('answer', '')).lower()
            val_lower = str(answer.get('answer_value', '')).lower()

            if 'true' in ans_lower or val_lower == 'true' or val_lower == '1':
                answer['answer_value'] = "1"
            elif 'false' in ans_lower or val_lower == 'false' or val_lower == '0':
                answer['answer_value'] = "0"
            else:
                answer['answer_value'] = "is_blank"

        # Handle numeric - strip units from answer_value
        elif q_type == "numeric" or q_type == "calculation":
            val = str(answer.get('answer_value', ''))
            if val != 'is_blank':
                # Handle ranges like "[80,90]"
                if '[' in val and ']' in val:
                    # Clean up range format
                    val = re.sub(r'\s+', '', val)  # Remove spaces
                    answer['answer_value'] = val
                else:
                    # Extract just the number
                    numbers = re.findall(r'-?[\d.]+', val)
                    if numbers:
                        answer['answer_value'] = numbers[0]
                    else:
                        answer['answer_value'] = "is_blank"

        # Format ref_id correctly
        if answer.get('ref_id') and answer.get('ref_id') != 'is_blank':
            # Parse existing ref_ids
            ref_str = str(answer['ref_id'])
            # Extract IDs from various formats
            ids = re.findall(r"['\"]?([a-zA-Z0-9_]+)['\"]?", ref_str)
            answer['ref_id'] = self.format_ref_id(ids)
        elif doc_ids:
            answer['ref_id'] = self.format_ref_id(list(doc_ids)[:3])
        else:
            answer['ref_id'] = "is_blank"

        # If answer_value is is_blank, ensure other fields are also is_blank
        if answer.get('answer_value') == 'is_blank':
            answer['answer'] = "Unable to answer with confidence based on the provided documents."
            answer['answer_unit'] = "is_blank"
            answer['supporting_materials'] = "is_blank"

        return answer

    def check_if_unanswerable(self, question: str, context: str) -> bool:
        """Check if a question is likely unanswerable from the context."""
        q_lower = question.lower()
        context_lower = context.lower()

        # Questions about things clearly outside AI/environment domain
        off_topic_keywords = ['elephant', 'weight of', 'miles is the earth', 'sun']
        if any(kw in q_lower for kw in off_topic_keywords):
            return True

        # Check if key entities from question appear in context
        # Extract potential key terms from question
        key_terms = re.findall(r'\b(BERT|GPT|LLaMA|BLOOM|T5|JetMoE|FLM|Mixtral|BlackMamba|CO2|emission|PUE|WUE|energy|training|inference)\b', question, re.I)

        if key_terms:
            matches = sum(1 for term in key_terms if term.lower() in context_lower)
            if matches == 0:
                return True

        return False

    def load_chunks_from_json(self) -> List[Chunk]:
        """Load chunks from JSON files"""
        all_chunks = []

        json_files = list(self.chunks_dir.glob("*_chunks.json"))
        if not json_files:
            raise ValueError(f"No chunk files found in {self.chunks_dir}. Run chunker.py first!")

        for json_file in json_files:
            doc_id = json_file.stem.replace("_chunks", "")

            with open(json_file, 'r', encoding='utf-8') as f:
                chunk_data = json.load(f)

            for i, chunk_dict in enumerate(chunk_data):
                text = chunk_dict['content']

                contains_numeric = chunk_dict.get('contains_numeric', False)
                if not contains_numeric:
                    contains_numeric = bool(re.search(
                        r'\b\d+\.?\d*\s*(kwh|mwh|co2|tco2e?|percent|%|lbs?|kg|gpu)',
                        text.lower()
                    ))

                chunk = Chunk(
                    text=text,
                    chunk_id=f"{doc_id}_chunk_{i}",
                    doc_id=doc_id,
                    section=chunk_dict.get('section'),
                    contains_table=(chunk_dict.get('type') == 'table'),
                    contains_numeric=contains_numeric
                )
                all_chunks.append(chunk)

        print(f"ðŸ“š Loaded {len(all_chunks)} chunks from {len(json_files)} documents")
        return all_chunks

    def build_or_load_index(self):
        """Build vector index"""
        print("ðŸ“¦ Building vector index...")
        chunks = self.load_chunks_from_json()
        self.retriever.index_documents(chunks)
        print("âœ… Index built successfully")

    def answer_question(self, question: str) -> Dict:
        """Answer a single question."""

        # Classify question type
        q_type = self.classify_question(question)

        # Expand query and retrieve
        queries = self.expand_query(question)
        all_results = {}

        for query in queries:
            results = self.retriever.hybrid_search(
                query,
                top_k=self.config['retrieval']['top_k'],
                alpha=0.6,
                method="rrf"
            )

            for chunk, score in results:
                if chunk.chunk_id not in all_results:
                    all_results[chunk.chunk_id] = (chunk, score)
                else:
                    existing = all_results[chunk.chunk_id][1]
                    all_results[chunk.chunk_id] = (chunk, max(score, existing) * 1.1)

        # Sort and get top results
        sorted_results = sorted(all_results.values(), key=lambda x: x[1], reverse=True)
        top_results = sorted_results[:self.config['retrieval']['top_k']]

        if not top_results:
            return self._get_fallback_response()

        # Build context
        context_parts = []
        doc_ids = set()

        for chunk, score in top_results:
            source_info = f"[Source: {chunk.doc_id}"
            if chunk.section:
                source_info += f", Section: {chunk.section}"
            source_info += "]"

            context_parts.append(f"{source_info}\n{chunk.text}")
            doc_ids.add(chunk.doc_id)

        context = "\n\n---\n\n".join(context_parts)

        # Check if likely unanswerable
        if self.check_if_unanswerable(question, context):
            return self._get_fallback_response()

        # Build specialized prompt
        prompt = self.build_prompt(question, context, q_type)

        # Generate answer
        try:
            if self.generator.provider == "anthropic":
                response = self.generator.client.messages.create(
                    model=self.generator.model,
                    max_tokens=1000,
                    temperature=0.1,
                    messages=[{"role": "user", "content": prompt}]
                )
                content = response.content[0].text

                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    answer = json.loads(json_match.group())
                else:
                    answer = self._get_fallback_response()
            else:
                response = self.generator.client.chat.completions.create(
                    model=self.generator.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.1,
                    response_format={"type": "json_object"}
                )
                answer = json.loads(response.choices[0].message.content)

        except Exception as e:
            print(f"Error generating answer: {e}")
            answer = self._get_fallback_response()

        # Post-process to match competition format
        answer = self.post_process_answer(answer, q_type, doc_ids)

        return answer

    def _get_fallback_response(self) -> Dict:
        """Return a properly formatted fallback response for unanswerable questions."""
        return {
            "answer": "Unable to answer with confidence based on the provided documents.",
            "answer_value": "is_blank",
            "answer_unit": "is_blank",
            "ref_id": "is_blank",
            "ref_url": "is_blank",
            "supporting_materials": "is_blank",
            "explanation": "is_blank"
        }

    def test_on_training_data(self, n_samples: int = 10):
        """Test pipeline on training questions"""
        print(f"\nðŸ“Š Testing on {n_samples} training samples...")

        correct = 0
        results_detail = []

        for i in range(min(n_samples, len(self.train_df))):
            row = self.train_df.iloc[i]
            answer = self.answer_question(row['question'])

            pred_val = str(answer['answer_value']).strip()
            true_val = str(row['answer_value']).strip()

            # Numeric comparison with tolerance
            try:
                pred_num = float(pred_val)
                true_num = float(true_val)
                if true_num != 0:
                    rel_diff = abs(pred_num - true_num) / abs(true_num)
                    match = rel_diff <= 0.001  # 0.1% tolerance
                else:
                    match = pred_num == true_num
            except ValueError:
                # String comparison
                match = pred_val.lower() == true_val.lower()

            status = "âœ“" if match else "âœ—"
            print(f"{status} Q{i+1}: Pred={pred_val} | Actual={true_val}")

            results_detail.append({
                'question': row['question'][:50],
                'predicted': pred_val,
                'actual': true_val,
                'match': match
            })

            if match:
                correct += 1

        accuracy = correct / min(n_samples, len(self.train_df))
        print(f"\nðŸŽ¯ Accuracy: {correct}/{n_samples} = {accuracy:.1%}")
        return accuracy

    def process_test_questions(self, save_every=10):
        """Process all test questions"""
        test_df = pd.read_csv("data/raw/test_Q.csv")

        print(f"\nðŸ”„ Processing {len(test_df)} Test Questions...")

        results = []

        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc="Processing"):
            try:
                answer = self.answer_question(row['question'])
                answer['id'] = row['id']
                answer['question'] = row['question']
                results.append(answer)

                if (idx + 1) % save_every == 0:
                    self._save_results(results, f'checkpoint_{idx + 1}.csv')

            except Exception as e:
                print(f"\nâš ï¸ Error on {row['id']}: {e}")
                answer = self._get_fallback_response()
                answer['id'] = row['id']
                answer['question'] = row['question']
                results.append(answer)

        self._save_results(results, 'submission.csv')
        print(f"\nðŸ’¾ Saved final submission with {len(results)} answers")
        return results

    def _save_results(self, results: List[Dict], filename: str):
        """Save results to CSV"""
        df = pd.DataFrame(results)

        # Match exact column order from train_QA.csv
        required_columns = ['id', 'question', 'answer', 'answer_value', 'answer_unit',
                            'ref_id', 'ref_url', 'supporting_materials', 'explanation']

        for col in required_columns:
            if col not in df.columns:
                df[col] = 'is_blank'

        df = df[required_columns]

        output_path = Path("data/processed") / filename
        df.to_csv(output_path, index=False)
        print(f"Saved to {output_path}")
