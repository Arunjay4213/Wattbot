[
  {
    "type": "text",
    "content": "Energy Considerations of Large Language Model Inference and Efficiency\nOptimizations\nJaredFernandez*1,ClaraNa*1,VashisthTiwari*1,\nYonatanBisk1,SashaLuccioni2,EmmaStrubell1\n1CarnegieMellonUniversity,2HuggingFace,\nCorrespondence:{jaredfern,clarana,vashisthtiwari}@cmu.edu\n103\nAbstract\nAslargelanguagemodels(LLMs)scaleinsize\nandadoption,theircomputationalandenviron-\n102\nmentalcostscontinuetorise. Priorbenchmark-\ningeffortshaveprimarilyfocusedonlatency\nreductioninidealizedsettings,oftenoverlook-\ningthediversereal-worldinferenceworkloads 101\nthatshapeenergyuse. Inthiswork,wesystem-\naticallyanalyzetheenergyimplicationsofcom- BurstGPT AzureCode AzureConv.\nmoninferenceefficiencyoptimizationsacross Task\ndiverse Natural Language Processing (NLP)\nandgenerativeArtificialIntelligence(AI)work-",
    "page": 1
  },
  {
    "type": "text",
    "content": "sacross Task\ndiverse Natural Language Processing (NLP)\nandgenerativeArtificialIntelligence(AI)work-\nloads, including conversational AI and code\ngeneration. Weintroduceamodelingapproach\nthatapproximatesreal-worldLLMworkflows\nthroughabinningstrategyforinput-outputto-\nkendistributionsandbatchsizevariations. Our\nempiricalanalysisspanssoftwareframeworks,\ndecodingstrategies,GPUarchitectures,online\nandofflineservingsettings, andmodelparal-\nlelismconfigurations. Weshowthattheeffec-\ntiveness of inference optimizations is highly\nsensitivetoworkloadgeometry,softwarestack,\nandhardwareaccelerators,demonstratingthat\nnaiveenergyestimatesbasedonFLOPsorthe-\noreticalGPUutilizationsignificantlyunderes-\ntimate real-world energy consumption. Our\nfindings reveal that the proper application of",
    "page": 1
  },
  {
    "type": "text",
    "content": "lyunderes-\ntimate real-world energy consumption. Our\nfindings reveal that the proper application of\nrelevantinferenceefficiencyoptimizationscan\nreducetotalenergyusebyupto73%fromun-\noptimizedbaselines. Theseinsightsprovidea\nfoundation for sustainable LLM deployment\nand inform energy-efficient design strategies\nforfutureAIinfrastructure.\n1 Introduction\nImprovements in task performance by large lan-\nguagemodels(LLMs)havepromptedlarge-scale\ninvestmentsincomputinghardwareandenergyin-\nfrastructure to support the development and de-\nployment of LLM and related machine learning\nmodels(Isaac,2025;Smith,2025;CaiandSophia,\n*Equalcontribution\n)hWk(ygrenE\nTheoretical\nPyTorch\nvLLM\nFigure1: Properapplicationofefficiencymethodswith\noptimizedvLLM(orange)approachestheidealenergy",
    "page": 1
  },
  {
    "type": "text",
    "content": "LLM\nFigure1: Properapplicationofefficiencymethodswith\noptimizedvLLM(orange)approachestheidealenergy\nconsumption(green)ascomparedwithanunoptimized\nbaselinePyTorch(purple)implementation.\n2025). However,thegrowingprevalenceofLLMs\nyields commensurate increases in the energy de-\nmand,wateruse,andcarbonemissionsassociated\nwiththeirdevelopmentanddeployment(Morrison\net al., 2025; Li et al., 2025; Strubell et al., 2020;\nLuccionietal.,2024b). Primarilymotivatedbythe\nincreaseddemandsfromLLMandAIworkloads,\nprojectionsestimatethatthatdatacentersconsume\nbetween9.1%and11.7%ofthetotalUSenergyde-\nmandby2030(Aljbouretal.,2024;Shehabietal.,\n2024; Green et al., 2024). However, such projec-\ntionsofenergyuseprimarilyrelyuponsector-wide\nestimatesofdemandorsubstantialsimplifications",
    "page": 1
  },
  {
    "type": "text",
    "content": "projec-\ntionsofenergyuseprimarilyrelyuponsector-wide\nestimatesofdemandorsubstantialsimplifications\noftheoftheenergydemandsofindividualmodels.\nIn order to develop effective energy policy for\nthisgrowingdemand,itisnecessarytocharacterize\ntheunderlyingcomputationalworkloadsofdevel-\nopment(i.e. modeltraining)anddeployment(i.e.\ninference). Inparticular,thecostandefficiencyof\ninferenceisespeciallycrucialduetothescaleand\nincreasedfrequencyatwhichmodelsareservedfor\nrepeateduse. Concretely, Metareportsthatinfer-\nence workloads constitute up to 70% of their AI\npowerconsumption(Wuetal.,2022)whileGoogle\nattributes60%oftheirMLenergy(Pattersonetal.,\n2022)andbetween80to90%ofMLAWScloud\ncomputingdemand(Barr,2019;Leopold,2019).\n1\n5202\nrpA\n42\n]LC.sc[\n1v47671.4052:viXra",
    "page": 1
  },
  {
    "type": "text",
    "content": ",2019;Leopold,2019).\n1\n5202\nrpA\n42\n]LC.sc[\n1v47671.4052:viXra",
    "page": 1
  },
  {
    "type": "text",
    "content": "Toaddresstheproblemofinferenceefficiency, tecture,weincludetheQwen-1.5-32Bmodel(Bai\nthe NLP and machine learning research commu- et al., 2023). For architectural comparisons, we\nnitieshavedevelopedvariousoptimizationsspan- analyzethesparseOLMoEmixture-of-expert(MoE)\nning: algorithms,softwareframeworks,andhard- modelalongsideitsdensecounterparts–the1Band\nware accelerators. Such optimizations have pri- 7BOLMoarchitectures–whichmaintaincompara-\nmarilytargetedimprovementsinmodelspeed(e.g. bleactiveandtotalparametercounts,respectively\nlatency and throughput; (Leviathan et al., 2023; (Muennighoffetal.,2024;Groeneveldetal.,2024).\nKwonetal.,2023)). Moreover,thesemethodsare\nDataDimensionality Weinvestigatetheimpact\nfrequently assessed in constrained settings or on",
    "page": 2
  },
  {
    "type": "text",
    "content": "hodsare\nDataDimensionality Weinvestigatetheimpact\nfrequently assessed in constrained settings or on\nofdatadimensionalityacrossthreekeydimensions:\nsimplified datasets that fail to capture the broad\ninputsequencelengths,outputgenerationlengths,\ndiversity of real-world tasks. These tasks range\nandbatchsizes.\nfrom traditional NLP applications like sequence\ntaggingandsummarizationtomorecomputation- Inference with large language models is com-\nallydemandingworkloadssuchassyntheticdata monlydecomposedintotwostages: prefillingand\ngenerationandchain-of-thoughtreasoning. There tokengeneration,eachwithadifferentenergypro-\nremainsacriticalgapinunderstandingoftheen- file(Pateletal.,2024). Theprefillstageprocesses",
    "page": 2
  },
  {
    "type": "text",
    "content": "ergypro-\nremainsacriticalgapinunderstandingoftheen- file(Pateletal.,2024). Theprefillstageprocesses\nergycostsoflanguagemodelinference,especially promptsinparallelandistypicallycompute-bound,\nwhenefficiencyinterventionsareappliedjointlyin achieving high GPU utilization. In contrast, the\nreal-worldsettings. autoregressivedecodingstageistypicallymemory-\nboundandleadstoGPUunder-utilization. These\nIn this work, we examine the energy costs of\nbottlenecksandtheirresultingenergyprofilesshift\nLLMinferenceandpresentacomprehensiveanal-\nysisoftheimpactof: datadimensionality,decod- withinputandoutputlengths.\ning strategies, serving frameworks, compilation ToaddressGPUunder-utilizationduringgener-\ntechniques,GPUhardwareplatforms,modelpar- ation,servingsystemsemploybatchingstrategies.",
    "page": 2
  },
  {
    "type": "text",
    "content": "uringgener-\ntechniques,GPUhardwareplatforms,modelpar- ation,servingsystemsemploybatchingstrategies.\nallelism,andarchitecturalvariantsontotalenergy However,theeffectivenessofbatchingvarieswith\nuseduringinference. Basedonourenergyprofiling input-outputcharacteristics(Agrawaletal.,2024;\nacrosstheseoptimizations,weapproximateoffline Lietal.,2024). Longinputsequenceslimitmaxi-\ninferencewithLLMsbasedonreal-worldworkload mumbatchsizesduetomemoryconstraints,while\nwithvariablesequencelengthsandbatching,con- variableoutputlengthscanleadtoinefficientbatch\nsideringbothanupperboundofnaiveunoptimized utilizationassomesequencescompletebeforeoth-\ninference and a lower bound of theoretical opti- ers.\nmized inference. Our analysis reveals that while Our analysis spans batch sizes from 1 (single-",
    "page": 2
  },
  {
    "type": "text",
    "content": "rs.\nmized inference. Our analysis reveals that while Our analysis spans batch sizes from 1 (single-\nidealizedestimationsofhardwareutilizationsub- exampleinference)totask-dependentmaximums\nstantiallyunderestimatetheenergyuseoflanguage (upto1024), ensuringcoverageofabroadrange\nmodel inference, proper application of inference ofmaximallyrealisticsettings.\nefficiencyoptimizationscansubstantiallyreduce WegroundanalysisinNLPworkloadsspanning\nthe energy requirements of inference by up to textclassification,summarization,translation,and\n73% from unoptimized baselines with vanilla open-endedtextgeneration. Differenttasksexhibit\nPyTorchandHuggingfaceTransformersandto different data dimensionalities: classification in-\nwithin26.",
    "page": 2
  },
  {
    "type": "text",
    "content": "PyTorchandHuggingfaceTransformersandto different data dimensionalities: classification in-\nwithin26.6%oftheoreticalidealperformanceon volvesminimalgeneration(oftenasingletoken),\nsimulatedofflineworkloads(seeTable4). summarization pairs long contexts with medium-\nlength outputs, and translation typically assumes\n2 Methodology balancedinput-outputlengths. Inputlengthstatis-\nticsinconsidereddatasetsareshowninTable3.\nIn the following section, we describe our experi-\nInacontrolledsweep,weexplorescenarioswith\nmentalsetupforevaluatinginferenceefficiency.\nupto32kinputtokensand4koutputtokens,vary-\nModelArchitectures. Wefocusourexperiments ing sequence lengths by powers of two. We fix\non language models ranging from 1B to 32B pa- generationto64or8tokenswhenvaryingcontext",
    "page": 2
  },
  {
    "type": "text",
    "content": "two. We fix\non language models ranging from 1B to 32B pa- generationto64or8tokenswhenvaryingcontext\nrameters,primarilyevaluatingLlama-3.1-8B-Ba lengths,andassume512or64tokeninputcontext\nseandLlama-3.1-8B-Instructmodelsasrepre- whenvaryingoutputlengths. Inputcontextlength\nsentativedecoder-onlytransformers(Dubeyetal., isenforcedviatruncationoflongersequencesfrom\n2024). Toinvestigateeffectsofscalingmodelarchi- PG19(Raeetal.,2019).\n2",
    "page": 2
  },
  {
    "type": "text",
    "content": "Figure2: ControlledsweepsofinputandoutputsequencelengthsonA6000GPUs,onvLLMbackend,described\nin§3.1. Wedecomposeinferencecostsintoprefillanddecodeenergy. Atsmallbatchsizesandinputsequence\nlengths,energyintensityofaworkloadscalessub-linearlywithincreasingsequencelengthinputsequencelengths.\nDecodingismoreenergyintensivepertokenthanprefill,butenergyintensitybeginsscalinglinearlyevenforshort\ngenerationsandsmallbatchsizeswiththevLLMframework.\nDecodingStrategies. Differentdecodingstrate- encethatachievesimprovedcomputeandmemory\ngies used for generation have different computa- utilization(Paszkeetal.,2019;Kwonetal.,2023);\ntionalprofilesandcanhaveasubstantialimpacton experimentsareconductedinbfloat16precision.\nthe generation efficiency (Kwon et al., 2023).",
    "page": 3
  },
  {
    "type": "text",
    "content": "limpacton experimentsareconductedinbfloat16precision.\nthe generation efficiency (Kwon et al., 2023). In Within these frameworks, we compare with a\nordertostudytheimpactofsamplingmethodsand native PyTorch baselines with Just-in-Time com-\nauto-regressivedecodingstrategies,weinvestigate pilation via TorchInductor (i.e. torch.compile)\ngreedy decoding, beam search decoding, temper- and CUDA Graphs kernel serialization. Further-\nature sampling, top-p decoding affect the energy more,forvLLM,weevaluatecontinuousbatching\nrequirements and end-to-end latency (Holtzman which efficiently handles variable output lengths\netal.,2020). in batch processing by overlaying sequences (Yu\nInadditiontoauto-regressivedecoding,westudy etal.,2022).\ntheimpactofspeculativedecoding. Speculativede-",
    "page": 3
  },
  {
    "type": "text",
    "content": "ditiontoauto-regressivedecoding,westudy etal.,2022).\ntheimpactofspeculativedecoding. Speculativede-\ncodingiscommonlyusedasalatencyminimization HardwarePlatforms. Ourexperimentsarecon-\ninferenceoptimization(Kwonetal.,2023). Inspec- ductedusinganon-premiseheterogeneousserver\nulativedecoding,alightweightdraftmodelisused withmultipleGPUtypesandnodeconfigurations.\ntopredictmultipletokens(γ)whicharethenveri- Specifically,weconductexperimentsonmultiple\nfiedbythetargetmodelinparallel(Leviathanetal., generationsofconsumerworkstationanddatacen-\n2023; Chen et al., 2023). Speculative decoding ter GPU accelerators from the Ampere (A6000,\nprovides latency improvement by better utilizing A10080GBPCIe),andAdaLovelace(A6000Ada)\nGPUsoverautoregressivedecoding. microarchitecture.",
    "page": 3
  },
  {
    "type": "text",
    "content": "utilizing A10080GBPCIe),andAdaLovelace(A6000Ada)\nGPUsoverautoregressivedecoding. microarchitecture.\nInourexperiments,weusethefollowingtarget- Allexperimentsrunon8-GPUnodeswithstan-\ndraft model pairs with a look-ahead value γ = 4 dardizednode-andjob-levelCPUandRAMcon-\nacrossvariousbatchsizes: DeepSeek-R1-Distil figurationsforeachGPUtype. Formulti-GPUex-\nl-Qwen-32B with mobiuslabsgmbh/DeepSeek-R periments,weutilizeupto4GPUssimultaneously,\n1-ReDistill-Qwen-1.5B-v1.1(Guoetal.,2025; investigatingtensorparallelinferencewithgroup\nYangetal.,2024);Llama-3.1-8B-BasewithLla sizesof2and4devices. 1. Weexaminebothstan-\nma-3.2-1B(Dubeyetal.,2024). dard and speculative decoding approaches using\nthe Llama-3.1-8B and Qwen-32B models. Addi-\nSoftwareOptimizations.",
    "page": 3
  },
  {
    "type": "text",
    "content": "ulative decoding approaches using\nthe Llama-3.1-8B and Qwen-32B models. Addi-\nSoftwareOptimizations. Choiceinthesoftware tionaldetailsoncomputinghardwareareprovided\nframeworks used for inference significantly im- inAppendixA.\npactsbothlatencyandenergyefficiencythroughop-\ntimizedkernelimplementationsandcomputational 1Thisconfigurationleaves4-7GPUsavailableforother\nusers.WhiletheSlurmschedulerdoesnotenforcecomplete\ngraphmanagement(Georgiouetal.,2022;Fernan-\nisolationinnetwork,memory,andCPUinfrastructureacross\ndezetal.,2023). Weevaluatetwowidely-adopted jobs, concurrent workloads in practice were not CPU- or\nlibraries used in LLM inference: native PyTorch memory-intensiveenoughtoimpactourssignificantly–for\nexample, in the vast majority of cases (98%), an ambient",
    "page": 3
  },
  {
    "type": "text",
    "content": "tensiveenoughtoimpactourssignificantly–for\nexample, in the vast majority of cases (98%), an ambient\nwithHuggingFacetransformers(Wolfetal.,2020),\nmeasurementoftheRAMutilizationinanodeourjobswere\nandvLLM,anoptimizedframeworkforLLMinfer- runningonwaslessthan20%ofthetotalavailable\n3",
    "page": 3
  },
  {
    "type": "table",
    "content": "TABLE (Page 3):\n | ",
    "page": 3
  },
  {
    "type": "table",
    "content": "TABLE (Page 3):\n | ",
    "page": 3
  },
  {
    "type": "table",
    "content": "TABLE (Page 3):\n | ",
    "page": 3
  },
  {
    "type": "text",
    "content": "2−4\n2−5\n2−6\n2−7\n20 21 22 23 24 25 26 27\nBatchSize\n)hWk(ygrenE\n2−4\nSpeculativeDecoding\nGreedyDecoding\nBeamSearch 2−5\nTemperatureSampling\nTop-pSampling 2−6\n2−7\n2−8\n2−9\n2−10\n21 23 25 27 29\nBatchSize\nFigure 3: At small batch sizes, speculative decoding\nprovidesreducedlatencyandenergysavings. Atlarger\nbatchsizespeculativedecodingincreasesenergy.\nPerformance Measures. We evaluate the ef-\nficiency of inference by measuring the latency,\nthroughput,GPUenergy,andGPUpowerrequired\nfor the inference of 1,024 examples 2. Total en-\nergy use and GPU power metrics are measured\nusing Nvidia Management Library (NVML) via\ntheCodeCarbonlibrary(Courtyetal.,2024). Prior\nto evaluation, we conduct a warmup on up to 20\nbatchestoallowformemoryallocation, required\nCUDAgraphcapture,andJiTcompilation3. Re-",
    "page": 4
  },
  {
    "type": "text",
    "content": "up on up to 20\nbatchestoallowformemoryallocation, required\nCUDAgraphcapture,andJiTcompilation3. Re-\nsults are reported as the mean values energy use,\nlatency,orpowerusageofthreeruns.\n3 Results\nInthefollowingsection,weexaminetheeffectsof\nvariationsofdatadimensionality,modelarchitec-\nture, decoding strategies, and software optimiza-\ntionsoninferenceenergyuse.\n3.1 EffectsofDatasetandSequenceLength\nWe present results from our controlled sweep of\nsequencelengthsandbatchsizesinFigure2. Pre-\nfillcostsincreaseasafunctionofinputsequence\nlength, at the same rate regardless of batch sizes\nwhenscalingsequenceslargerthan128tokens. At\nshorter sequence lengths and smaller batch sizes,\ntheenergycostsofprefillareconstantregardless\nof the computational workload due to significant",
    "page": 4
  },
  {
    "type": "text",
    "content": "izes,\ntheenergycostsofprefillareconstantregardless\nof the computational workload due to significant\nundersaturation of the accelerator. Although we\nfixoutputgenerationtokensto64,weverifythat\nat this convergence in rate of energy intensity in-\ncreaseoccursatthesamepointwheninsteadfixing\ngeneration length to 8 tokens; see Figure 11 in\nAppendixE.\n2Forexperimentswithbatchsizeslargerthan256,metrics\narecomputedover4096examplesandthennormalized.\n3Duetosize,warmupislimitedto4batchesforinference\nwiththeQwen-32B.\n)hWk(ygrenE\nOLMo1B\nOLMo7B\nOLMoE1B-7B\nFigure4: Mixture-of-ExpertsLLMsrequiremoreen-\nergythandensemodelswithcomparableactiveparame-\nters;differencesarepronouncedatlargerbatchsizes.\nIn Figure 2, the energy intensity of the decode\nlikewise scales with input context length only at",
    "page": 4
  },
  {
    "type": "text",
    "content": ".\nIn Figure 2, the energy intensity of the decode\nlikewise scales with input context length only at\nlargerinputsequencelengths.\nHowever,theenergyintensityofdecodingscales\nlinearly with sequence length regardless of se-\nquencelengthorbatchsizesduetotheautoregres-\nsive,sequentialnatureofdecoding.\nGenerally,decodingenergydominatestheover-\nallworkloadinallsettingsbutthosewiththeshort-\nestgenerationlengths,suchasthoseseeninclassi-\nficationworkloadsandshortformsummarization.\nNotethelog-logscaleandtheparallellineartrends,\nwherethedifferencesininterceptsareproportion-\nate with the differences in batch size 4. In the\nfollowingsections,wediscussavarietyofalgorith-\nmicandsoftwareinterventionsthatareappropriate\nfordifferenttypesofworkloadgeometries.\n3.2 EffectsofAlgorithmicOptimizations",
    "page": 4
  },
  {
    "type": "text",
    "content": "ionsthatareappropriate\nfordifferenttypesofworkloadgeometries.\n3.2 EffectsofAlgorithmicOptimizations\nSpeculativeDecodingOnlyReducesEnergyat\nLowBatchSizes. Speculativedecodingiscom-\nmonlyusedtoachieveinferencespeedupsinlow-\nbatchinferenceinwhichautoregressivedecoding\nfailstoachievehighGPUVRAMutilization. How-\never, for large batch sizes where GPU is already\nsaturated,draftmodelspeculationandexcessverifi-\ncationsintroduceadditionaloverhead. Inthelarge\nbatch case, for short to medium contexts, LLM\ninferenceistypicallycomputebound,makingspec-\nulativedecodingslowerthanautoregressivedecod-\ning with the target model (Chen et al., 2025; Liu\netal.,2024).\nCompared to variations in energy use from al-\nternatedecodingstrategiesandsamplingmethods,\nspeculativedecodinghasthegreatesteffectonthe",
    "page": 4
  },
  {
    "type": "text",
    "content": "from al-\nternatedecodingstrategiesandsamplingmethods,\nspeculativedecodinghasthegreatesteffectonthe\n4SeeFig10inAppendixEforadditionalresultsonvanilla\nPyTorch backend, and Figure 12 for comparison with real\nenergyintensitymeasurementsforasampleofclassicalNLP\ntasks\n4",
    "page": 4
  },
  {
    "type": "table",
    "content": "TABLE (Page 4):\n |  |  |  |  |  | Spec\nGree | ulativeD\ndyDeco | ecoding\nding | \n |  |  |  |  |  | Beam\nTemp\nTop- | Search\nerature\npSampli | Samplin\nng | g\n |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  | ",
    "page": 4
  },
  {
    "type": "table",
    "content": "TABLE (Page 4):\n |  |  |  | OLM | o1B |  | \n |  |  |  | OLM\nOLM | o7B\noE1B-7B |  | \n |  |  |  |  |  |  | \n |  |  |  |  |  |  | \n |  |  |  |  |  |  | \n |  |  |  |  |  |  | ",
    "page": 4
  },
  {
    "type": "text",
    "content": "10−1\n10−2\n21 23 25 27 29\nBatchsize\n)hWk(ygrenE\n10−1\nPyTorch(compile=False)\nPyTorch(compile=True)\nvLLM(eager=False)\nvLLM(eager=True)\n10−2\n20 21 22 23 24 25 26 27 28\nBatchsize\n(a)A10080GBPCIe\n)hWk(ygrenE\nPyTorch(compile=False) 10−1\nPyTorch(compile=True)\nvLLM(eager=False)\nvLLM(eager=True)\n10−2\n20 21 22 23 24 25 26 27 28\nBatchsize\n(b)A6000Ada\n)hWk(ygrenE\nPyTorch(compile=False)\nPyTorch(compile=True)\nvLLM(eager=False)\nvLLM(eager=True)\n(c)A6000\nFigure5: EnergyconsumptioncomparisonacrossdifferentGPUsforinferencewithPyTorchandvLLMbackends\nof1024samplesfor64outputtokens. ForeachGPU,wecomparePyTorchwithandwithoutcompilation,and\nvLLMwithandwithoutCUDAGraphserialization. Thelineinblackrepresentsthemaximumallowablebatchsize\nforPyTorch.",
    "page": 5
  },
  {
    "type": "text",
    "content": "thandwithoutCUDAGraphserialization. Thelineinblackrepresentsthemaximumallowablebatchsize\nforPyTorch. RelativesavingsaremostapparentinthelowbatchsizeregimeandthatvLLMduetoitsoptimizations\ncanservealargerbatchsize.\nenergy use and latency of language model infer- 3.3 EffectsofSoftwareOptimizations\nence. At smaller batch sizes ( 16) speculative\nPagedAttentionwithvLLMImprovesEfficiency.\n≤\ndecoding is effective in reducing the total energy\nComparedtonativePyTorch,thevLLMinference\ncostofinferencewithupto+29.14%comparedto\nservingengineimprovesboththethroughputand\nsingle-exampleinference(Figure3). However,au-\ntheenergyefficiency. ThevLLMframeworkuses\ntoregressivedecodingmethodsaremoreefficient\nPagedAttentiontoimplementnon-contiguousKV\nat larger batch sizes, with speculative decoding",
    "page": 5
  },
  {
    "type": "text",
    "content": "fficient\nPagedAttentiontoimplementnon-contiguousKV\nat larger batch sizes, with speculative decoding\ncacheblockswhichreducesmemoryfragmentation\nrequiring 25.65% more energy when performing\nandallocationofredundantmemoryinthecaseof\ninferenceatabatchsizeof128.\nsparsesequences(Kwonetal.,2023).\nTheseoptimizationsallowforimprovedmemory\nefficiency and the vLLM framework to support\nlargerbatchsizesonfixedmemoryGPUs.\nMixtureofExpertsIncursHigherInferenceEn-\nergyCosts. Sparsemixture-of-expertsareoften CompilationandKernelSerializationImproves\nutilized as an alternative architecture due to their Efficiency. The graph compilation and kernel\nincreased sample efficiency during training and serialization increase hardware utilization by re-",
    "page": 5
  },
  {
    "type": "text",
    "content": "increased sample efficiency during training and serialization increase hardware utilization by re-\nincreasedperformancerelativetodenseneuralnet- movingredundantoperationsinthecomputational\nworkswiththesamenumberofactiveparameters. graph and reducing the kernel launch overhead\nAlthoughbothdenseOLMo-1BandtheOLMoE1B-7 (Fernandezetal.,2023),respectively. Weobserve\nBmixture-of-expertsmodelsusesubstantiallyless thatbothtorch.compileandCUDAgraphserial-\nenergythanthedenseOLMo-7Bmodel,theOLMoE ization (eager=False) improve throughput at no\narchitecture utilizes up to 54.24% more energy additional energy cost in Figure 5. However, we\nthan the base OLMo 1B model, despite having a note that the benefits of CUDA graphs are more\nsimilarnumberofactiveparameters.",
    "page": 5
  },
  {
    "type": "text",
    "content": "el, despite having a note that the benefits of CUDA graphs are more\nsimilarnumberofactiveparameters. apparentatlowerbatchsizes,astherelativecostof\nkernel launch is larger for smaller computational\nWeidentifythattheincreasedenergyandlatency\nworkloads.\nofMoE’scanbeattributedtothefusedkernelused\nin the expert layers which is substantially slower Continuous Batching Reduces Energy Use.\nthanthecorrespondingGEMMoperationinlinear LLMinferenceisinherentlyautoregressive,requir-\nlayersinthedensemodel;19.70%sloweratbatch ing many sequential operations. Static batching\nsize 1 and 63% slower at batch size 8. Notably, maintainsafixedbatchsizethroughoutinference,\nweobservethattheadditionalroutingoperations whichleadstoGPUunder-utilizationwhengener-",
    "page": 5
  },
  {
    "type": "text",
    "content": "utinference,\nweobservethattheadditionalroutingoperations whichleadstoGPUunder-utilizationwhengener-\nintheMoEmodelintroduceminimallatency;and ation lengths vary and idle compute accumulates\nthattheincreasedoverheadofmoreCUDAgraph afterearlyterminations. Continuousbatchingmiti-\nandkernellaunchoperationsarelargelymitigated gatesthisbydynamicallyreplacingcompletedre-\nthroughkernelserializationandgraphcompilation questswithnewones,improvingGPUutilization\noptimizations(i.e. vLLMwithCUDAGraphs). andreducingidletime(Yuetal.,2022). Thisap-\n5",
    "page": 5
  },
  {
    "type": "text",
    "content": "2−3\n2−4\n2−5\n2−6\n2−7\n21 23 25 27 29\nBatchSize\n)hWk(ygrenE\nLlama3.18B\nSingleGPU\nTensorParallel: 2\nTensorParallel: 4\n2−2\n2−3\n2−4\n21 23 25 27 29\nBatchSize\n)hWk(ygrenE\nInFigure6,weobservethatutilizingtensorpar-\nallelismtoscalefrominferencewithasingleGPU\ntofourGPUsreduceslatencyandper-devicepower\nutilizationfortheLlama-3.18Bmodel. However,\nincreasing parallelism yields higher total energy\nuseduetothelargernumberofaccelerators. Con-\ncretely, parallelizing a fixed workload over two\nandfour GPUsdecreaseslatencyby40.16% and\n61.34% but increases total energy use by 29.3%\nand 55.23% at single batch inference due to the\nintroductionofadditionaldevices.\nDeepSeekDistillQwen32B\nEffectsofHardwareSpeed Theeffectivenessof\nTensorParallel: 2\nTensorParallel: 4 optimizationtechniquesvariessignificantlyacross",
    "page": 6
  },
  {
    "type": "text",
    "content": "effectivenessof\nTensorParallel: 2\nTensorParallel: 4 optimizationtechniquesvariessignificantlyacross\nhardwareplatforms,withfasteracceleratorsshow-\ninggreaterbenefitsfromoptimizationsthattarget\ncomputationalefficiency. Ourresultsdemonstrate\nthat graph compilation, kernel serialization, and\nspeculative decoding achieve their maximum im-\npactontheA100GPU.\nSpecifically, PyTorch compilation yields a\n29.90% improvement on the A100, which drops\nFigure6: EnergyUseofLlama-3.18BandQwen32B\nto 13.28% on the RTX 6000 Ada and further to\nwithvaryingdegreesofTensorParallelism.\n1.96% on the A6000. Similarly, vLLM’s eager\nmodeoptimizationshowsa25.47%improvement\nproach is particularly effective when generation\non the A100 versus 2.97% on the A6000. This\nlengths have high variance, yielding significant",
    "page": 6
  },
  {
    "type": "text",
    "content": "ration\non the A100 versus 2.97% on the A6000. This\nlengths have high variance, yielding significant\npatternsuggeststhatashardwarecomputationalca-\nspeedupsatlargerbatchsizes.\npabilitiesincrease,therelativeimpactofsoftware\nWeobservethatatsmallerbatchsizestheover-\noptimizationstargetingkernelefficiencybecomes\nheadofonlineschedulingoutweighsitsbenefitsbut\nmorepronounced.\natlargerbatchsizes,onlineservingwithcontinuous\nbatchingrequireslessenergy;detailsinAppendix\n4 TheImpactofOptimizationson\nD. We note that the numbers under-represent the\nInferenceEnergyUse\nimpactofcontinuousbatchinggiventhesamples\naredrawnfromthesamedataset,therebyreducing Inthissection,weoutlineourapproachtomodel-\nthevarianceininputandoutputlengths. ingtheenergyconsumptionofanLLMunderboth",
    "page": 6
  },
  {
    "type": "text",
    "content": "lineourapproachtomodel-\nthevarianceininputandoutputlengths. ingtheenergyconsumptionofanLLMunderboth\nsyntheticandrealisticworkloaddistributions. We\n3.4 EffectsofHardwareDesignChoices\nleverageclassicalNLPtasksanddatasetsofinfer-\nMulti-GPUTensorParallelismReducesLatency encerequeststoestimateenergyusageacrossdif-\nfor Increased Power Use Model parallelism ferentexecutionenvironments,includingPyTorch-\ntechniquessuchastensorandpipelineparallelism nativeandvLLMbackendswithsoftwareoptimiza-\nare frequently used to alleviate the memory pres- tionsonasingleA6000GPU.\nsure of large sets of model parameters and batch\nsizes,aswellastoleveragemultiplehardwareac- 4.1 ModelingEnergyRequirementsUsing\ncelerators in order to speed up workload execu- OfflineServing\ntion (Narayanan et al., 2021).",
    "page": 6
  },
  {
    "type": "text",
    "content": "sUsing\ncelerators in order to speed up workload execu- OfflineServing\ntion (Narayanan et al., 2021). Additionally, for We consider the energy required to process a\nfixed workloads, tensor parallelism reduces both dataset = R ,R ,...,R in an offline set-\n1 2 N\nD { }\nthe per-device computational intensity and per- tinginwhichallrequestscanbebatchprocessed\ndevicepowerutilizationastheworkloadissharded freely, and where each request R consists of a\nk\nacross accelerator. However, the speedups from tuple(i ,o ),representingtheinputtokenlength\nk k\nadditionalacceleratorsareinsufficienttooffsetthe i andtheoutputgenerationlengtho :\nk k\nenergycostofutilizingmoredevices(i.e. utilizing\ntwicetheGPUsfailstoyieldatwo-foldspeedup). R = (i ,o ), k 1,...,N .\nk k k\n∀ ∈ { }\n6",
    "page": 6
  },
  {
    "type": "text",
    "content": "yieldatwo-foldspeedup). R = (i ,o ), k 1,...,N .\nk k k\n∀ ∈ { }\n6",
    "page": 6
  },
  {
    "type": "table",
    "content": "TABLE (Page 6):\nLlama3.18B\n2−3\nSingleGPU\nTensorParallel: 2\n2−4\n)hWk(ygrenE TensorParallel: 4\n2−5\n2−6\n2−7\n21 23 25 27 29\nBatchSize\nDeepSeekDistillQwen32B\nTensorParallel: 2\nTensorParallel: 4\n2−2\n)hWk(ygrenE\n2−3\n2−4\n21 23 25 27 29\nBatchSize",
    "page": 6
  },
  {
    "type": "table",
    "content": "TABLE (Page 6):\n |  |  |  |  | \n |  |  |  | Single\nTensor | GPU\nParallel: 2\n |  |  |  | Tensor | Parallel: 4\n |  |  |  |  | \n |  |  |  |  | \n |  |  |  |  | \n |  |  |  |  | ",
    "page": 6
  },
  {
    "type": "table",
    "content": "TABLE (Page 6):\n |  |  |  |  | Parallel: 2\nParallel: 4\n |  |  |  | Tensor\nTensor | \n |  |  |  |  | \n |  |  |  |  | \n |  |  |  |  | ",
    "page": 6
  },
  {
    "type": "text",
    "content": "1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n21 23 25 27 29 211 213\nTokenCount\nytilibaborPevitalumuC\nAzureConversationInputTokens-CDF\nRealDistribution 1.0\nBinnedApproximation\n0.8\n0.6\n0.4\n0.2\n0.0\n21 23 25 27 29 211\nTokenCount\nytilibaborPevitalumuC\nAzureConversationOutputTokens-CDF\nRealDistribution\nBinnedApproximation\nFigure 7: Comparison of the real token length distributions (blue) with the binned approximation (orange) for\nAzureconversationinput(left)andoutput(right)tokenlengths. TheCDFplotsillustratehowourbinningstrategy\napproximatestheempiricaldistributionwhileensuringcomputationalefficiencyforenergyestimation.\nSincei ando varysignificantlyacrossrequests, We collect real energy measurements\nk k\nweutilizedatasetstatistics—includingthemedian Ereal (I ,O ), representing the observed en-\nbatch ∗ ∗",
    "page": 7
  },
  {
    "type": "text",
    "content": "eutilizedatasetstatistics—includingthemedian Ereal (I ,O ), representing the observed en-\nbatch ∗ ∗\nand 99th percentile of input and output lengths ergy usage when processing a full batch of size\n(discussedin§4.3)toinformourbinningstrategy. B(I ,O )withinputlengthsI andoutputlengths\n∗ ∗ ∗\nO . Thus,thetotalestimatedenergyconsumption\n∗\nBinning Strategy. To effectively handle the\nacrosstheworkloadtoserveN requeststhatfallin\nbroadrangeof(i ,o )values,wedefinediscrete\nk k thebinisgivenby:\nbinsetsforinputandoutputlengths:\nI bins =\n=\n{ 2\n32\nm\n,1\n|\n2\nm\n8,\n∈\n25\nN\n6\n,\n,\n4\n51\n≤\n2,\nm\n102\n≤\n4,\n1\n2\n3\n0\n}\n48,4096,8192 ,\nE(cid:98)total = (cid:88) (cid:18) N\nB\nre\n(\nal\nI\n(I\n,\n∗\nO\n,O\n)\n∗ ) (cid:19) E\nb\nre\na\na\ntc\nl\nh\n(I\n∗\n,O\n∗\n),\n{ } (I∗,O∗) ∗ ∗\nO = 2n n N,3 m 9\nbins\n{ | ∈ ≤ ≤ }\n= 8,16,32,64,128,256,512 .",
    "page": 7
  },
  {
    "type": "text",
    "content": "a\ntc\nl\nh\n(I\n∗\n,O\n∗\n),\n{ } (I∗,O∗) ∗ ∗\nO = 2n n N,3 m 9\nbins\n{ | ∈ ≤ ≤ }\n= 8,16,32,64,128,256,512 .\n{ } where Nreal(I ,O ) is the total number of ob-\n∗ ∗\nThesebinchoicesensuresufficientcoverageacross served requests mapped to bin (I ∗ ,O ∗ ), and\nrealisticrequestdistributions. Notably,weexclude\nNreal(I∗,O∗)\nrepresents the number of batches re-\nB(I∗,O∗)\nextremely long input requests (> 8k tokens) and quiredtoprocessthem.\ngenerationoutputsbeyond512tokens.\nMapping Requests to Bins. Given a request 4.2 IdealizedBaseline\nR = (i,o),wemapittotheclosestceilingbin:\nAs a naive baseline, we estimate an upper bound\nI ∗ = min I I bins I i , oftheenergyefficiencyoftheseworkloadswitha\n{ ∈ | ≥ }\nO = min O O O o . baselinederivedfromthemanufacturer-ratedhard-\n∗ bins\n{ ∈ | ≥ }",
    "page": 7
  },
  {
    "type": "text",
    "content": "adswitha\n{ ∈ | ≥ }\nO = min O O O o . baselinederivedfromthemanufacturer-ratedhard-\n∗ bins\n{ ∈ | ≥ }\nware speeds (FLOPS ), power draw (TDP)\nHW\nWegrouprequestswithinthesame(I ,O )bin ,andfloatingpointoperations(FLOPs)requiredfor\n∗ ∗\ninto batches of size B(I ,O ), the maximum al- inferenceFLOPs5. Thisapproximationassumes\n∗ ∗\nlowablebatchsizeforthegivenhardwareandback- hardwareisbeingutilizedasmaximumefficiency\nendconfiguration. EachbatchprocessesB(I ,O ) bothinthroughidealizedfloatingpointoperation\n∗ ∗\nrequestsinparallel,allowingformoreefficienten- throughputandmaximumpowerdraw.\nergy utilization, which is more representative of\nreal-worldinferencesetups.\nGivenourhardwareconfigurationandbackend,\n(cid:18) (cid:19)\nTDP\nwe collect the estimates of E (I ,O ), which\nbatch ∗ ∗ E(cid:98)Optimal =",
    "page": 7
  },
  {
    "type": "text",
    "content": "nd,\n(cid:18) (cid:19)\nTDP\nwe collect the estimates of E (I ,O ), which\nbatch ∗ ∗ E(cid:98)Optimal =\ncorrespondstotheenergyusedtoservearequest FLOPS HW\n(cid:88)\nof batch size B with input prompts of length I Nreal(I ,O ) FLOPs(I ,O )\n∗ × ∗ ∗ × ∗ ∗\nandoutputlengthsO .\n(I∗,O∗)\n∗\n7",
    "page": 7
  },
  {
    "type": "table",
    "content": "TABLE (Page 7):\n | RealDi\nBinned | stribution\nApproxim | ation |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  | ",
    "page": 7
  },
  {
    "type": "table",
    "content": "TABLE (Page 7):\n | RealDistributi\nBinnedApprox | on\nimation |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  | ",
    "page": 7
  },
  {
    "type": "text",
    "content": "Dataset Mean±Std Median 99th Task Mean±Std Max Output\nBurstGPT 256.80±242.27 215 1038 Translation 49.96±39.39 550 64\nAzureChat 1631.58±1529.64 928 6683 Generation 136.89±93.13 547 64\nAzureCode 2511.28±2133.54 1930 7685 Classification 292.48±239.94 3112 1\nSummarization 838.49±400.70 2386 64\nTable1: InputSequenceLengthStatisticsAcrossReal-\nWorldLLMWorkloads Table3: TokenizedInputandOutputLengthStatistics\nAcrossNLPTasksusedforEnergyBenchmarking\nDataset Mean±Std Median 99th\nBurstGPT 35.10±108.59 7 478\nAzureChat 105.51±158.25 41 694 0.05\nAzureCode 22.69±74.78 8 271\n0.04\nTable2:OutputSequenceLengthStatisticsAcrossReal-\n0.03\nWorldLLMWorkloads\n0.02\n4.3 Evaluations\n0.01\nWe examine a suite of classical NLP tasks and\nLLMinferenceworkloads,eachcharacterizedby wiki imdb CNN wmt\nTask",
    "page": 8
  },
  {
    "type": "text",
    "content": "a suite of classical NLP tasks and\nLLMinferenceworkloads,eachcharacterizedby wiki imdb CNN wmt\nTask\narangeofdifferentinputcontextandoutputgen-\nerationsequences;withdatasetstatisticsprovided\ninTables3,1,2. Wesimulatealarge-scaleoffline\nprocessing setting on the RTX A6000 GPUs, in\nwhich examples are binned by sequence lengths\n(asdescribedin§4andprocessedinparallelinthe\nlargestpossiblebatchesthatfitinGPUmemory.\nUtilizingthesimulatedworkloadsdescribedin\nSec4.1,weestimatetheeffectivenessoftheinfer-\nenceefficiencyoptimizationsevaluatedinSection\n4.1. Basedontheseresults,weselectaninference\nframeworkwithefficiencyoptimizationstargeting\nlargebatchinference. Concretely,weconsiderin-\nference with a dense model utilizing vLLM with\nCUDA graph serialization (eager mode off) on a",
    "page": 8
  },
  {
    "type": "text",
    "content": "erin-\nference with a dense model utilizing vLLM with\nCUDA graph serialization (eager mode off) on a\nsingle GPU and compare it to unoptimized infer-\nence native PyTorch as a lower bound on energy\nefficiency. Inaddition,wealsomodeltheidealized\nenergybaselinebasedonthemodelandhardware\nconfigurations.\nClassicalNLPTasks. Webenchmarktheenergy\nuse in a set of classical natural language process-\ning tasks in the English language: text classifica-\ntion (IMDB, Maas et al., 2011), machine transla-\ntion(WMT-14,Bojaretal.,2014),summarization\n(CNN-DailyMail,Nallapatietal.,2016),andtext\ngeneration(Wikitext-2(Merityetal.,2016)).\nFor each of these tasks, we sample a subset of\n1024 examples with statistics of each dataset for\ntheinputandtheoutputtokensprovidedinTable3.",
    "page": 8
  },
  {
    "type": "text",
    "content": "et of\n1024 examples with statistics of each dataset for\ntheinputandtheoutputtokensprovidedinTable3.\n5BasedontheNvidiadatasheetfortheRTXA6000GPU,\nwe utilize consider FLOPS of 309.7 TFLOPS and a\nHW\n300WTDPpowerdraw;andestimatetheoreticalinference\nFLOPswiththeDeepSpeedprofiler(Rasleyetal.,2020).\n)hWk(ygrenE\nPyTorch\nvLLM\nFigure8: EnergyComparisonindoinginferenceover1024\nsamplesbetweenPyTorchwithCompilationoffandvLLM\nwitheagermodeloff.\nWe note that the input sequences were padded to\nthemaximumsequencelength. Theenergyprofiles\nforthebestrun,characterizedbytheleastenergy\naresummarizedinFigure8,withconsistentreduc-\ntionsinenergyuseprovidedbyinferenceefficiency\noptimizations.\nReal-WorldLLMWorkloads Additionally,we\nestimatetheenergyintensityandeffectivenessof",
    "page": 8
  },
  {
    "type": "text",
    "content": "optimizations.\nReal-WorldLLMWorkloads Additionally,we\nestimatetheenergyintensityandeffectivenessof\nefficiencyoptimizationsonreal-worldLLMwork-\nloads. WesimulatetheofflineprocessingofLLM\ninferencerequestsasusedinapplicationsforshort-\nform conversations with the Burst-GPT dataset\n(Wangetal.,2024)andlongcontextconversations\nand code completion with the Azure LLM Infer-\nencechatandcodetraces(Stojkovicetal.,2024b).\nEach dataset provides a traces of LLM inference\nrequestswiththeircorrespondinginputcontextand\noutputgenerationlengths. Ascomparedwiththe\nclassicalNLPtasks,modernLLMworkloadstend\ntobelongerinbothinputcontextandoutputgener-\nationtokenlengths,withcode-assistapplications\nhavinglongercontexts,whereasconversationalset-\ntingsresultinginlongergenerations.",
    "page": 8
  },
  {
    "type": "text",
    "content": "ssistapplications\nhavinglongercontexts,whereasconversationalset-\ntingsresultinginlongergenerations.\nDue to the larger number of requests and in-\ncreased sequence lengths, we observe that these\nworkloadsrequiresubstantiallylargeramountsof\nenergy. However,wefindthatproperapplications\nofinferenceefficiencyoptimizationscansubstan-\ntiallyreduceenergycostswithsavingsof73.00%,\n8",
    "page": 8
  },
  {
    "type": "table",
    "content": "TABLE (Page 8):\n | PyTorch\nvLLM",
    "page": 8
  },
  {
    "type": "text",
    "content": "Dataset PyTorch%∆ vLLM%∆ due to the large singular costs of model develop-\nBurstGPT 506.52% 63.75% ments (Strubell et al., 2020; Wang et al., 2023;\nAzureCode 102.79% 26.59% Luccionietal.,2023;Faizetal.,2023);withlarge\nAzureConversation 490.23% 64.22%\nindustrialdeveloperssimilarlyreportingtheenergy\nrequiredforpretraining(OLMoetal.,2024;Morri-\nTable4: Percentagedifferencesofenergyconsumption\nrelative to theoretical values for Various Tasks with sonetal.,2025;Dubeyetal.,2024).\nOfflineInference. Incontrasttotraining,inferenceworkloadsare\nhigher in variability with variation in request fre-\n37.58%, and 72.18% on BurstGPT, Azure Code quencies, batching, input and output sequence\nandConversation,respectively. lengthsexecutedoverdiversehardwareplatforms",
    "page": 9
  },
  {
    "type": "text",
    "content": "input and output sequence\nandConversation,respectively. lengthsexecutedoverdiversehardwareplatforms\nat scale; and more complex energy use profiles\n5 RelatedWork duetovariationsinpowerdrawduringprefilland\ndecodingstagesofgeneration(Pateletal.,2024).\nEfficientMethodsforLLMInference Tomeet\nPrevious work has investigated the comparative\ntheservice-level-objective(SLO)servingrequire-\nenergycostofmachinelearningmodelsacrossvar-\nmentsofrealdeploymentsettings,efficiencyopti-\nious tasks (Luccioni et al., 2024b,a), the energy\nmizationsforLLMinferenceareoftendesignedto\ncostsofLMsofvarioussizes(Samsietal.,2023;\noptimizemodelservingspeed,asmeasuredbyla-\nWu et al., 2025), the effects of hardware config-\ntencyandtime-to-first-token. Avarietyofmethods\nurations (i.e. GPU power capping and frequency",
    "page": 9
  },
  {
    "type": "text",
    "content": "nfig-\ntencyandtime-to-first-token. Avarietyofmethods\nurations (i.e. GPU power capping and frequency\nhave been developed to meet these latency con-\nscaling; (Samsi et al., 2023)), and the impact of\nstraints,including: continuousbatching(Yuetal.,\nsequencelengthvariabilityandbatchingstrategies\n2022),modelparallelism(Narayananetal.,2021;\n(Pateletal.,2024;Stojkovicetal.,2024a;Wilkins\nHuang et al., 2019; Li et al., 2020), speculative\net al., 2024). However, such evaluations of infer-\ndecoding(Liuetal.,2024;Leviathanetal.,2023;\nence energy use often rely on simplified deploy-\nChenetal.,2023,2025),anddisaggregatedserving\nmentsettingswithlimitedsetsofmodelarchitec-\n(Zhongetal.,2024).\nturesandservingframeworks.\nSolelyoptimizingsystemperformanceforspeed",
    "page": 9
  },
  {
    "type": "text",
    "content": "elarchitec-\n(Zhongetal.,2024).\nturesandservingframeworks.\nSolelyoptimizingsystemperformanceforspeed\nis insufficient in characterizing and does not pro-\n6 Conclusion\nvideinsightintothemodelenergyuseandresult-\ning carbon emissions of LLM inference; as such In this work, we evaluate the impact of common\nmethods may require additional computation or inference efficiency optimizations on the energy\nexhibitlowcorrelationbetweenefficiencycostin- requirementsoflargelanguagemodelserving. We\ndicators(Dehghanietal.,2022). Recentworkhas examineavarietyofoptimizationtechniquesand\nexploredmethodsforexplicitlyreducingenergyre- evaluate on representative data corresponding to\nquirementsandcarbonemissionsforLLMserving classical NLP tasks as well as modern LLM de-",
    "page": 9
  },
  {
    "type": "text",
    "content": "sponding to\nquirementsandcarbonemissionsforLLMserving classical NLP tasks as well as modern LLM de-\nviadisaggregatedservingoverheterogeneoushard- ploymentsettings. Weconcludethattheeffective-\nware (Shi et al., 2024), system-wide scheduling ness of latency optimizations in reducing energy\nandrequestroutingtoenergy-optimizedinstances use is highly sensitive to the shape of the input\n(Stojkovic et al., 2024b), and prompt directives data, underlying hardware architecture, and soft-\nto induce shorter sequence generations (Li et al., ware framework implementations; and that opti-\n2024). However,theexactimpactorimprovements mizationscannotbeapplieduniformly.\ninenergyrequirementsforlatency-optimizedmeth- Additionally,weconductacasestudyofclassi-\nodsremainsnotfullycharacterized.",
    "page": 9
  },
  {
    "type": "text",
    "content": "forlatency-optimizedmeth- Additionally,weconductacasestudyofclassi-\nodsremainsnotfullycharacterized. calNLPtasksandreal-worldLLMinferencework-\nloadsandfindthatproperapplicationofthestudied\nEstimationsandMeasurementofofEnergyUse\ninferenceoptimizationscanreducetotalenergyuse\ninNLP Theenergyandcarbonemissionsofma-\nbyupto73%ontheBurstGPTchatdataset.\nchine learning models have been a growing con-\ncernintheresearchcommunityandindustryasthe\nLimitationsandRisks\nscaleofmodelsandprevalenceofdeploymenthas\nincreased(Schwartzetal.,2020;Wuetal.,2022). Inthiswork,weevaluatetheenergyefficiencyand\nEstimationsof theenergy requirementsand envi- carbon emissions of LLM inference as approxi-\nronmentalimpactofLLMshaslargelyfocusedon matedbytotalGPUpowerusage. AlthoughGPUs",
    "page": 9
  },
  {
    "type": "text",
    "content": "erence as approxi-\nronmentalimpactofLLMshaslargelyfocusedon matedbytotalGPUpowerusage. AlthoughGPUs\nestimationofcostsforpretrainingandfinetuning themajorityofarithmeticoperationsrequiredfor\n9",
    "page": 9
  },
  {
    "type": "text",
    "content": "inferenceandoperateatahigherTDPthanother ChristofMonz,PavelPecina,MattPost,HerveSaint-\ncomponents,wedonotaccountfortheenergyuse Amand,RaduSoricut,LuciaSpecia,andAlesTam-\nchyna. 2014. Findings of the 2014 workshop on\nbyotherothercomponentsofthehardwaresystem\nstatisticalmachinetranslation. InProceedingsofthe\nsuchaspowerusefromCPU,memory,ordiskstor-\nNinthWorkshoponStatisticalMachineTranslation,\nage(McAllisteretal.,2024;Pateletal.,2024);or pages12–58,Baltimore,Maryland,USA.Associa-\nestimatetheenergyrequirementsofotherhardware tionforComputationalLinguistics.\nacceleratorarchitectures(e.g. TPUs,NPUs,etc.).\nKendrickCaiandDeborahMarySophia.2025. Alpha-\nLikewise, we conduct an investigation of com-\nbetplansmassivecapexhike,reportscloudrevenue",
    "page": 10
  },
  {
    "type": "text",
    "content": ". Alpha-\nLikewise, we conduct an investigation of com-\nbetplansmassivecapexhike,reportscloudrevenue\nmonly used inference software frameworks and growthslowed. Reuters.\nstandardefficiencyoptimizations. However,there\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving,\nremainothersettingsandcomputationaloptimiza-\nJean-Baptiste Lespiau, Laurent Sifre, and John\ntions that can be applied to LLM inference, such Jumper. 2023. Accelerating large language model\nasutilizing: reducedormixedprecision,adaptive decodingwithspeculativesampling. arXivpreprint\nadjustmentofGPUfrequency,additionalformsof arXiv:2302.01318.\nmodelparallelism,orotherformsofloadmanage-\nJian Chen, Vashisth Tiwari, Ranajoy Sadhukhan,\nmentandworkloadscheduling;whichremainout ZhuomingChen,JinyuanShi,IanEn-HsuYen,and",
    "page": 10
  },
  {
    "type": "text",
    "content": "anajoy Sadhukhan,\nmentandworkloadscheduling;whichremainout ZhuomingChen,JinyuanShi,IanEn-HsuYen,and\nofthescopeofthiswork(Stojkovicetal.,2024b). BeidiChen.2025. Magicdec: Breakingthelatency-\nthroughputtradeoffforlongcontextgenerationwith\nInthiswork,weprimarilyfocusontheoperation\nspeculative decoding. In The Thirteenth Interna-\nenergyuseofmachinelearninginference. Estima-\ntionalConferenceonLearningRepresentations.\ntion of the embodied costs of inference; and the\ncosts of machine learning training remain out of BenoitCourty,VictorSchmidt,SashaLuccioni,Goyal-\nKamal,MarionCoutarel,BorisFeld,JérémyLecourt,\nthescopeofthiswork.\nLiamConnell, Amine Saboni, Inimaz, supatomic,\nAlthough improved characterization of the en-\nMathilde Léval, Luis Blanche, Alexis Cruveiller,",
    "page": 10
  },
  {
    "type": "text",
    "content": "mic,\nAlthough improved characterization of the en-\nMathilde Léval, Luis Blanche, Alexis Cruveiller,\nergyuseofLLMinferencecanbeusedtodesign ouminasara, Franklin Zhao, Aditya Joshi, Alexis\nmoreefficientservingsettingsandreducetheen- Bogroff, Hugues de Lavoreille, Niko Laskaris,\nEdoardoAbati,DouglasBlank,ZiyaoWang,Armin\nergy needs of inference, it is possible that reduc-\nCatovic, Marc Alencon, Michał Ste˛chły, Christian\ntionsinthecostofpretrainingmaythenleadmore\nBauer,LucasOtávioN.deAraújo,JPW,andMinerv-\nindividualsandorganizationstopursuelargemodel aBooks.2024. mlco2/codecarbon: v2.4.1.\npretraining(i.e. JevonsParadox).\nMostafaDehghani,YiTay,AnuragArnab,LucasBeyer,\nandAshishVaswani.2022. Theefficiencymisnomer.\nInInternationalConferenceonLearningRepresenta-\nReferences\ntions.",
    "page": 10
  },
  {
    "type": "text",
    "content": "swani.2022. Theefficiencymisnomer.\nInInternationalConferenceonLearningRepresenta-\nReferences\ntions.\nAmeyAgrawal,NitinKedia,AshishPanwar,Jayashree\nAbhimanyuDubey,AbhinavJauhri,AbhinavPandey,\nMohan,NipunKwatra,BhargavSGulavani,Alexey\nAbhishekKadian,AhmadAl-Dahle,AieshaLetman,\nTumanov,andRamachandranRamjee.2024. Tam-\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\ningthroughput-latencytradeoffinllminferencewith\nFan,etal.2024. Thellama3herdofmodels. arXiv\nsarathi-serve. Proceedingsof18thUSENIXSympo-\npreprintarXiv:2407.21783.\nsiumonOperatingSystemsDesignandImplementa-\ntion,2024,SantaClara.\nAhmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi,\nPrateek Sharma, Fan Chen, and Lei Jiang. 2023.\nJordanAljbour,TomWilson,andPPatel.2024. Power-\nLlmcarbon: Modeling the end-to-end carbon foot-",
    "page": 10
  },
  {
    "type": "text",
    "content": "023.\nJordanAljbour,TomWilson,andPPatel.2024. Power-\nLlmcarbon: Modeling the end-to-end carbon foot-\ningintelligence: Analyzingartificialintelligenceand print of large language models. arXiv preprint\ndatacenterenergyconsumption. EPRIWhitePaper arXiv:2309.14393.\nno.3002028905.\nJaredFernandez,JacobKahn,ClaraNa,YonatanBisk,\nJinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang, andEmmaStrubell.2023. Theframeworktax: Dis-\nXiaodongDeng,YangFan,WenbinGe,YuHan,Fei paritiesbetweeninferenceefficiencyinnlpresearch\nHuang, et al. 2023. Qwen technical report. arXiv and deployment. In Proceedings of the 2023 Con-\npreprintarXiv:2309.16609. ferenceonEmpiricalMethodsinNaturalLanguage\nProcessing,pages1588–1600.\nJeff Barr. 2019. Amazon ec2 update-infl instances",
    "page": 10
  },
  {
    "type": "text",
    "content": "thodsinNaturalLanguage\nProcessing,pages1588–1600.\nJeff Barr. 2019. Amazon ec2 update-infl instances\nwithawsinferentiachipsforhighperformancecost- Stefanos Georgiou, Maria Kechagia, Tushar Sharma,\neffectiveinferencing. FedericaSarro,andYingZou.2022. Greenai: Do\ndeeplearningframeworkshavedifferentcosts? In\nOndrej Bojar, Christian Buck, Christian Federmann, Proceedingsofthe44thInternationalConferenceon\nBarryHaddow, PhilippKoehn, JohannesLeveling, SoftwareEngineering,pages1082–1094.\n10",
    "page": 10
  },
  {
    "type": "text",
    "content": "Alistair Green, Humayun Tai, Jesse Noffsinger, and distributed: Experiencesonacceleratingdataparallel\nPankajSachdeva.2024. Howdatacentersandtheen- training. arXivpreprintarXiv:2006.15704.\nergysectorcansateai’shungerforpower. McKinsey\nandCompany. Xiaoxuan Liu, Cade Daniel, Langxiang Hu, Woosuk\nKwon, Zhuohan Li, Xiangxi Mo, Alvin Cheung,\nDirkGroeneveld,IzBeltagy,PeteWalsh,AkshitaBha- Zhijie Deng, Ion Stoica, and Hao Zhang. 2024.\ngia,RodneyKinney,OyvindTafjord,AnanyaHarsh Optimizing speculative decoding for serving large\nJha,HamishIvison,IanMagnusson,YizhongWang, language models using goodput. arXiv preprint\netal.2024. Olmo: Acceleratingthescienceoflan- arXiv:2406.14066.\nguagemodels. arXivpreprintarXiv:2402.00838.\nAlexandraSashaLuccioni,SylvainViguier,andAnne-",
    "page": 11
  },
  {
    "type": "text",
    "content": "6.14066.\nguagemodels. arXivpreprintarXiv:2402.00838.\nAlexandraSashaLuccioni,SylvainViguier,andAnne-\nDayaGuo,DejianYang,HaoweiZhang,JunxiaoSong, LaureLigozat.2023. Estimatingthecarbonfootprint\nRuoyuZhang,RunxinXu,QihaoZhu,ShirongMa, ofbloom,a176bparameterlanguagemodel. Journal\nPeiyiWang,XiaoBi,etal.2025. Deepseek-r1: In- ofMachineLearningResearch,24(253):1–15.\ncentivizingreasoningcapabilityinllmsviareinforce-\nmentlearning. arXivpreprintarXiv:2501.12948. Sasha Luccioni, Boris Gamazaychikov, Sara Hooker,\nRégisPierrard,EmmaStrubell,YacineJernite,and\nAriHoltzman,JanBuys,LiDu,MaxwellForbes,and Carole-Jean Wu. 2024a. Light bulbs have en-\nYejinChoi.2020. Thecuriouscaseofneuraltextde- ergy ratings—so why can’t ai chatbots? Nature,\ngeneration. InInternationalConferenceonLearning 632(8026):736–738.",
    "page": 11
  },
  {
    "type": "text",
    "content": "o why can’t ai chatbots? Nature,\ngeneration. InInternationalConferenceonLearning 632(8026):736–738.\nRepresentations.\nSasha Luccioni, Yacine Jernite, and Emma Strubell.\nYanpingHuang,YoulongCheng,AnkurBapna,Orhan\n2024b. Powerhungryprocessing: Wattsdrivingthe\nFirat,DehaoChen,MiaChen,HyoukJoongLee,Ji-\ncostofaideployment? InProceedingsofthe2024\nquanNgiam,QuocVLe,YonghuiWu,etal.2019.\nACM Conference on Fairness, Accountability, and\nGpipe: Efficient training of giant neural networks\nTransparency, FAccT ’24, page 85–99, New York,\nusingpipelineparallelism. Advancesinneuralinfor-\nNY,USA.AssociationforComputingMachinery.\nmationprocessingsystems,32.\nAndrewMaas, RaymondEDaly, PeterTPham, Dan\nMike Isaac. 2025. Meta to increase spending to $65\nHuang,AndrewYNg,andChristopherPotts.2011.\nbillionthisyearina.",
    "page": 11
  },
  {
    "type": "text",
    "content": "2025. Meta to increase spending to $65\nHuang,AndrewYNg,andChristopherPotts.2011.\nbillionthisyearina.i.push. NewYorkTimes.\nLearning word vectors for sentiment analysis. In\nProceedingsofthe49thannualmeetingoftheassoci-\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nationforcomputationallinguistics:Humanlanguage\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\ntechnologies,pages142–150.\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-\ncientmemorymanagementforlargelanguagemodel\nSaraMcAllister,FiodarKazhamiaka,DanielSBerger,\nservingwithpagedattention. InProceedingsofthe\nRodrigoFonseca,KaliFrost,AaronOgus,Maneesh\nACMSIGOPS29thSymposiumonOperatingSystems\nSah,RicardoBianchini,GeorgeAmvrosiadis,Nathan\nPrinciples.\nBeckmann,etal.2024. Acallforresearchonstorage\nemissions. InProceedingsofthe3rdWorkshopon",
    "page": 11
  },
  {
    "type": "text",
    "content": "rinciples.\nBeckmann,etal.2024. Acallforresearchonstorage\nemissions. InProceedingsofthe3rdWorkshopon\nGeorge Leopold. 2019. Aws to offer nvidia’s t4\nSustainableComputerSystems(HotCarbon).\ngpusforaiinferencing. URL:https://web.archive.\norg/web/20220309000921/https://www. hpcwire.\nStephenMerity,CaimingXiong,JamesBradbury,and\ncom/2019/03/19/aws-upgrades-its-gpu-backed-ai-\nRichardSocher.2016. Pointersentinelmixturemod-\ninference-platform/(visitedon2022-04-19).\nels. Preprint,arXiv:1609.07843.\nYaniv Leviathan, Matan Kalman, and Yossi Matias.\nJacob Morrison, Clara Na, Jared Fernandez, Tim\n2023. Fast inference from transformers via spec-\nDettmers, Emma Strubell, and Jesse Dodge. 2025.\nulative decoding. In International Conference on\nHolisticallyevaluatingtheenvironmentalimpactof",
    "page": 11
  },
  {
    "type": "text",
    "content": "25.\nulative decoding. In International Conference on\nHolisticallyevaluatingtheenvironmentalimpactof\nMachineLearning,pages19274–19286.PMLR.\ncreatinglanguagemodels. InTheThirteenthInterna-\nBaolinLi,YankaiJiang,VijayGadepally,andDevesh tionalConferenceonLearningRepresentations.\nTiwari. 2024. Sprout: Green generative ai with\ncarbon-efficientllminference. InProceedingsofthe NiklasMuennighoff,LucaSoldaini,DirkGroeneveld,\n2024ConferenceonEmpiricalMethodsinNatural Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi,\nLanguageProcessing,pages21799–21813. PeteWalsh,OyvindTafjord,NathanLambert,etal.\n2024. Olmoe: Open mixture-of-experts language\nPengfei Li, Jianyi Yang, Mohammad A. Islam, and models. arXivpreprintarXiv:2409.02060.\nShaoleiRen.2025. MakingAILess\"Thirsty\": Un-",
    "page": 11
  },
  {
    "type": "text",
    "content": "ad A. Islam, and models. arXivpreprintarXiv:2409.02060.\nShaoleiRen.2025. MakingAILess\"Thirsty\": Un-\ncoveringandAddressingtheSecretWaterFootprint Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,\nof AI Models. arXiv preprint. ArXiv:2304.03271 Çag˘lar Gu˙lçehre, and Bing Xiang. 2016. Abstrac-\n[cs]. tivetextsummarizationusingsequence-to-sequence\nRNNs and beyond. In Proceedings of the 20th\nShenLi, YanliZhao, RohanVarma, OmkarSalpekar, SIGNLLConferenceonComputationalNaturalLan-\nPieterNoordhuis,TengLi,AdamPaszke,JeffSmith, guage Learning, pages 280–290, Berlin, Germany.\nBrianVaughan,PritamDamania,etal.2020. Pytorch AssociationforComputationalLinguistics.\n11",
    "page": 11
  },
  {
    "type": "text",
    "content": "DeepakNarayanan,MohammadShoeybi,JaredCasper, TianyaoShi,YanranWu,SihangLiu,andYiDing.2024.\nPatrick LeGresley, Mostofa Patwary, Vijay Kor- Greenllm:Disaggregatinglargelanguagemodelserv-\nthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, ing on heterogeneous gpus for lower carbon emis-\nJulie Bernauer, Bryan Catanzaro, et al. 2021. Ef- sions. arXivpreprintarXiv:2412.20322.\nficient large-scale language model training on gpu\nclusters using megatron-lm. In Proceedings of the BradSmith.2025. Thegoldenopportunityforamerican\nInternationalConferenceforHighPerformanceCom- ai.\nputing,Networking,StorageandAnalysis,pages1–\nJovanStojkovic,EshaChoukse,ChaojieZhang,Inigo\n15.\nGoiri,andJosepTorrellas.2024a. Towardsgreener",
    "page": 12
  },
  {
    "type": "text",
    "content": "s1–\nJovanStojkovic,EshaChoukse,ChaojieZhang,Inigo\n15.\nGoiri,andJosepTorrellas.2024a. Towardsgreener\nTeamOLMo,PeteWalsh,LucaSoldaini,DirkGroen- llms: Bringingenergy-efficiencytotheforefrontof\neveld,KyleLo,ShaneArora,AkshitaBhagia,Yuling llminference. arXivpreprintarXiv:2403.20306.\nGu,ShengyiHuang,MattJordan,etal.2024. 2olmo\n2furious. arXivpreprintarXiv:2501.00656. JovanStojkovic,ChaojieZhang,ÍñigoGoiri,JosepTor-\nrellas,andEshaChoukse.2024b. Dynamollm: De-\nAdam Paszke, Sam Gross, Francisco Massa, Adam signingllminferenceclustersforperformanceand\nLerer, James Bradbury, Gregory Chanan, Trevor energyefficiency. arXivpreprintarXiv:2408.00741.\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style, EmmaStrubell,AnanyaGanesh,andAndrewMcCal-",
    "page": 12
  },
  {
    "type": "text",
    "content": ", Luca\nAntiga, et al. 2019. Pytorch: An imperative style, EmmaStrubell,AnanyaGanesh,andAndrewMcCal-\nhigh-performancedeeplearninglibrary. Advancesin lum. 2020. Energy and policy considerations for\nneuralinformationprocessingsystems,32. modern deep learning research. In Proceedings of\nthe AAAI conference on artificial intelligence, vol-\nPratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo ume34,pages13693–13696.\nGoiri,BrijeshWarrier,NithishMahalingam,andRi-\ncardoBianchini.2024. Characterizingpowerman- Xiaorong Wang, Clara Na, Emma Strubell, Sorelle\nagement opportunities for llms in the cloud. In Friedler,andSashaLuccioni.2023. Energyandcar-\nProceedings of the 29th ACM International Con- bon considerations of fine-tuning BERT. In Find-",
    "page": 12
  },
  {
    "type": "text",
    "content": "ar-\nProceedings of the 29th ACM International Con- bon considerations of fine-tuning BERT. In Find-\nferenceonArchitecturalSupportforProgramming ingsoftheAssociationforComputationalLinguis-\nLanguagesandOperatingSystems,Volume3,pages tics: EMNLP 2023, pages 9058–9069, Singapore.\n207–222. AssociationforComputationalLinguistics.\nDavid Patterson, Joseph Gonzalez, Urs Hölzle, Quoc YuxinWang,YuhanChen,ZeyuLi,XuezeKang,Zhen-\nLe, Chen Liang, Lluis-Miquel Munguia, Daniel hengTang,XinHe,RuiGuo,XinWang,QiangWang,\nRothchild, David So, Maud Texier, and Jeff Dean. AmelieChiZhou,andXiaowenChu.2024. Burst-\n2022. The carbon footprint of machine learn- gpt: Areal-worldworkloaddatasettooptimizellm\ning training will plateau, then shrink. Preprint, servingsystems. Preprint,arXiv:2401.17644.\narXiv:2204.05149.",
    "page": 12
  },
  {
    "type": "text",
    "content": "g will plateau, then shrink. Preprint, servingsystems. Preprint,arXiv:2401.17644.\narXiv:2204.05149.\nGrantWilkins,SrinivasanKeshav,andRichardMortier.\nJackWRae,AnnaPotapenko,SiddhantMJayakumar, 2024. Offlineenergy-optimalllmserving: Workload-\nChloeHillier,andTimothyPLillicrap.2019. Com- basedenergymodelsforllminferenceonheteroge-\npressivetransformersforlong-rangesequencemod- neoussystems. ACMSigEnergynewletter.\nelling. arXivpreprint.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nJeffRasley,SamyamRajbhandari,OlatunjiRuwase,and Chaumond,ClementDelangue,AnthonyMoi,Pier-\nYuxiong He. 2020. Deepspeed: System optimiza- ricCistac,TimRault,RémiLouf,MorganFuntowicz,\ntionsenabletrainingdeeplearningmodelswithover et al. 2020. Transformers: State-of-the-art natural\n100billionparameters.",
    "page": 12
  },
  {
    "type": "text",
    "content": "deeplearningmodelswithover et al. 2020. Transformers: State-of-the-art natural\n100billionparameters. InProceedingsofthe26th languageprocessing. InProceedingsofthe2020con-\nACMSIGKDDInternationalConferenceonKnowl- ference on empirical methods in natural language\nedgeDiscovery&DataMining,pages3505–3506. processing: systemdemonstrations,pages38–45.\nSiddharthSamsi,DanZhao,JosephMcDonald,Baolin Carole-Jean Wu, Ramya Raghavendra, Udit Gupta,\nLi,AdamMichaleas,MichaelJones,WilliamBerg- BilgeAcun,NewshaArdalani,KiwanMaeng,Glo-\neron,JeremyKepner,DeveshTiwari,andVijayGade- ria Chang, Fiona Aga, Jinshi Huang, Charles Bai,\npally.2023. Fromwordstowatts: Benchmarkingthe etal.2022. Sustainableai: Environmentalimplica-\nenergycostsoflargelanguagemodelinference. In tions,challengesandopportunities.",
    "page": 12
  },
  {
    "type": "text",
    "content": "Environmentalimplica-\nenergycostsoflargelanguagemodelinference. In tions,challengesandopportunities. Proceedingsof\n2023IEEEHighPerformanceExtremeComputing MachineLearningandSystems,4:795–813.\nConference(HPEC),pages1–9.IEEE.\nYanran Wu, Inez Hua, and Yi Ding. 2025. Unveil-\nRoySchwartz,JesseDodge,NoahASmith,andOren ingenvironmentalimpactsoflargelanguagemodel\nEtzioni. 2020. Green ai. Communications of the serving: A functional unit view. arXiv preprint\nACM,63(12):54–63. arXiv:2502.11256.\nArmanShehabi,AlexHubbard,AlexNewkirk,Nuoa AnYang,BaosongYang,BeichenZhang,BinyuanHui,\nLei,MdAbuBakkarSiddik,BillieHolecek,Jonathan BoZheng,BowenYu,ChengyuanLi,DayihengLiu,\nKoomey,EricMasanet,DaleSartor,etal.2024. 2024 FeiHuang,HaoranWei,etal.2024. Qwen2.5tech-\nunitedstatesdatacenterenergyusagereport.",
    "page": 12
  },
  {
    "type": "text",
    "content": ",etal.2024. 2024 FeiHuang,HaoranWei,etal.2024. Qwen2.5tech-\nunitedstatesdatacenterenergyusagereport. nicalreport. arXivpreprintarXiv:2412.15115.\n12",
    "page": 12
  },
  {
    "type": "text",
    "content": "Gyeong-InYu,JooSeongJeong,Geon-WooKim,Soo-\njeong Kim, and Byung-Gon Chun. 2022. Orca: A\ndistributedservingsystemfor Transformer-Based\n{ }\ngenerative models. In 16th USENIX Symposium\non Operating Systems Design and Implementation\n(OSDI22),pages521–538.\nYinminZhong,ShengyuLiu,JundaChen,JianboHu,\nYibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang.\n2024. DistServe : Disaggregating prefill and de-\n{ }\ncodingforgoodput-optimizedlargelanguagemodel\nserving. In 18th USENIX Symposium on Operat-\ningSystemsDesignandImplementation(OSDI24),\npages193–210.\nA HardwareDetails\nIn Table 5, we provide additional details on the\nhardwareconfigurationsofthenodesusedinour\nbenchmarkingexperiments.\nB DatasetLicenses\nTheCNN-DailyMaildatasetusedforsummariza-\ntionisreleasedundertheApache-2.0License. The",
    "page": 13
  },
  {
    "type": "text",
    "content": "tasetLicenses\nTheCNN-DailyMaildatasetusedforsummariza-\ntionisreleasedundertheApache-2.0License. The\ndataset Wikitext-2 dataset for text generation is\navailableundertheCreativeCommonsAttribution-\nShareAlike License. The WMT-14 translation\ndatasetsarereleasedfornon-commercialuse. The\nBurstGPT and Azure trace datasets are released\nunderCC-BY-4.0licenses.\nC AcknowledgmentofAIAssistance\nArtificial intelligence assistance was used to as-\nsist in literature review and for code completion\nassistance,specificallyduringthecreationofvisu-\nalizations.\nD AdditionalOptimzations: Continuous\nBatching\nIn Figure 9, we present additional results on the\nimpactofvLLM’scontinuousbatchingforonline\ninferenceinwhichweobservethatatlargebatch\nsizescontinuousbatchingyieldsreductionsinen-\nergyuse.",
    "page": 13
  },
  {
    "type": "text",
    "content": "ine\ninferenceinwhichweobservethatatlargebatch\nsizescontinuousbatchingyieldsreductionsinen-\nergyuse.\nE AdditionalSequenceLengthResults\nInFigure10,wepresentadditionalresultsonthe\neffectsofscalinginputandoutputsequencelengths\nwiththePyTorchframework.\n13",
    "page": 13
  },
  {
    "type": "text",
    "content": "CPU RAM GPU GPUTDP FP32TFLOPS Bfloat16TFLOPS\n256xAMDEPYC7763 1TB NvidiaRTXA6000 300W 38.7 –\n128xAMDEPYC7513 500GB NvidiaRTXA6000Ada 300W 91.1 –\n128xAMDEPYC7763 1TB NvidiaRTXA100-80GB 300W 156 312\nTable5: NodeHardwareSpecifications\n2\n0\n2 −\n4\n−\n6\n−\n8 −\n10 −\n12\n−\n21 23 25 27 29\nBatchsize\n)%(noitcudeRygrenE\n5.0\n2.5\n0.0\n2.5\n−\n5.0\n− 7.5\n− 10.0\n−\n12.5\n−\n20 21 22 23 24 25 26 27 28\nBatchsize\n(a)A10080GBPCIe\n)%(noitcudeRygrenE\n5.0\n2.5\n0.0\n2.5\n−\n5.0 −\n7.5 −\n10.0\n−\n20 21 22 23 24 25 26 27 28\nBatchsize\n(b)A6000Ada\n)%(noitcudeRygrenE\n(c)A6000\nFigure 9: Energy reduction comparison between online and offline serving modes across different GPUs\n(E E ) 100/E ). Theoptimizationsemployedforonlineservingsaveupto5%energyat\noffline online offline\n− ∗\nlargerbatchsizes\n102\n101\n100\n10−1\n10−2\n10−3\n10−4\n10−5",
    "page": 14
  },
  {
    "type": "text",
    "content": "saveupto5%energyat\noffline online offline\n− ∗\nlargerbatchsizes\n102\n101\n100\n10−1\n10−2\n10−3\n10−4\n10−5\n100 101 102 103 104\nInputsequencelength(tokens)\n)hWk(.xe4201rofygrenE\nPrefillenergyvs.inputsequencelength\n102\nbs=1 8outputtoks\nbs=8 64outputtoks 101\nbs=64\n100\n10−1\n10−2\n10−3\n10−4\n10−5\n100 101 102 103 104\nInputsequencelength(tokens)\n)hWk(.xe4201rofygrenE\nDecodeenergyvs.inputsequencelength\nbs=1 8outputtoks\nbs=8 64outputtoks\nbs=64 100\n10−1\n10−2\n100 101 102 103 104\nInputoroutputsequencelength(tokens)\n)hWk(selpmaxe4201rofygrenE\nEnergyscalingforinputandoutputcontextlengths\nbs=1 Input\nbs=8 Output\nbs=64\nFigure10: ControlledsweepsofinputandoutputsequencelengthsonA6000GPUs,withvanillaPyTorchbackend.\n14",
    "page": 14
  },
  {
    "type": "table",
    "content": "TABLE (Page 14):\n2\n0\n)%(noitcudeRygrenE\n2\n−\n4\n−\n6\n−\n8\n−\n10\n−\n12\n−\n21 23 25 27 29\nBatchsize |  |  |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  | ",
    "page": 14
  },
  {
    "type": "table",
    "content": "TABLE (Page 14):\n |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  | ",
    "page": 14
  },
  {
    "type": "table",
    "content": "TABLE (Page 14):\n5.0\n2.5 )%(noitcudeRygrenE\n0.0\n2.5\n−\n5.0\n−\n7.5\n−\n10.0\n−\n20 21 22 23 24 25 26 27 28\nBatchsize |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  | ",
    "page": 14
  },
  {
    "type": "table",
    "content": "TABLE (Page 14):\nPrefillenergyvs.inputsequencelength\n102\nbs=1 8outputtoks\n101 bs=8 64outputtoks\n)hWk(.xe4201rofygrenE bs=64\n100\n10−1\n10−2\n10−3\n10−4\n10−5\n100 101 102 103 104\nInputsequencelength(tokens) | Decodeenergyvs.inputsequencelength\n102\nbs=1 8outputtoks\n101 bs=8 64outputtoks\n)hWk(.xe4201rofygrenE bs=64\n100\n10−1\n10−2\n10−3\n10−4\n10−5\n100 101 102 103 104\nInputsequencelength(tokens) | Energyscalingforinputandoutputcontextlengths\nbs=1 Input\n)hWk(selpmaxe4201rofygrenE bs=8 Output\n100 bs=64\n10−1\n10−2\n100 101 102 103 104\nInputoroutputsequencelength(tokens)",
    "page": 14
  },
  {
    "type": "text",
    "content": "102\n101\n100\n10−1\n10−2\n10−3\n10−4\n10−5\n100 101 102 103 104\nInputsequencelength(tokens)\n)hWk(.xe4201rofygrenE\nPrefillenergyvs.inputsequencelength\n102\nbs=1 8outputtoks\nbs=8 64outputtoks 101\nbs=64\n100\n10−1\n10−2\n10−3\n10−4\n10−5\n100 101 102 103 104\nInputsequencelength(tokens)\n)hWk(.xe4201rofygrenE\nDecodeenergyvs.inputsequencelength\nbs=1 8outputtoks\nbs=8 64outputtoks\nbs=64\n102\n101\n100\n10−1\n10−2\n10−3\n10−4\n10−5\n100 101 102 103 104\nOutputsequencelength(tokens)\n)hWk(.xe4201rofygrenE\nDecodeenergyvs.outputsequencelength\n102\nbs=1\nbs=8 101\nbs=64\n100\n10−1\n10−2\n10−3\n10−4\n10−5\n100 101 102 103 104\nOutputsequencelength(tokens)\n)s(.xe4201rofygrenE\nGenerationdurationvs.outputsequencelength\nbs=1 64prompttoks\nbs=8 512prompttoks\nbs=64",
    "page": 15
  },
  {
    "type": "text",
    "content": "e4201rofygrenE\nGenerationdurationvs.outputsequencelength\nbs=1 64prompttoks\nbs=8 512prompttoks\nbs=64\nFigure11: ControlledsweepsofinputandoutputsequencelengthsonA6000GPUs,withvLLMofflineinference.\nHere,wedisplaymultiplefixedsequencelengthsizesforcomparisonaswesweepacrossbatchsizeandtheother\ndimensionofsequencelength.\n15",
    "page": 15
  },
  {
    "type": "table",
    "content": "TABLE (Page 15):\nPrefillenergyvs.inputsequencelength\n102\nbs=1 8outputtoks\n101 bs=8 64outputtoks\n)hWk(.xe4201rofygrenE bs=64\n100\n10−1\n10−2\n10−3\n10−4\n10−5\n100 101 102 103 104\nInputsequencelength(tokens) | Decodeenergyvs.inputsequencelength\n102\nbs=1 8outputtoks\n101 bs=8 64outputtoks\n)hWk(.xe4201rofygrenE bs=64\n100\n10−1\n10−2\n10−3\n10−4\n10−5\n100 101 102 103 104\nInputsequencelength(tokens)\nDecodeenergyvs.outputsequencelength\n102\nbs=1\n101 bs=8\n)hWk(.xe4201rofygrenE bs=64\n100\n10−1\n10−2\n10−3\n10−4\n10−5\n100 101 102 103 104\nOutputsequencelength(tokens) | Generationdurationvs.outputsequencelength\n102\nbs=1 64prompttoks\n101 bs=8 512prompttoks\nbs=64\n100 )s(.xe4201rofygrenE\n10−1\n10−2\n10−3\n10−4\n10−5\n100 101 102 103 104\nOutputsequencelength(tokens)",
    "page": 15
  },
  {
    "type": "text",
    "content": "0.10\n0.08\n0.06\n0.04\n0.02\nclassification summarization textgen translation\n)hWk(selpmaxe4201rofygrenE\n0.030\n0.025\n0.020\n0.015\n0.010\nclassification summarization textgen translation\n)hWk(selpmaxe4201rofygrenE\n0.025\n0.020\n0.015\n0.010\n0.005\nclassification summarization textgen translation\n)hWk(selpmaxe4201rofygrenE\nFigure12: ClassicalNLPtasksandtheirenergyinten-\nsities with vLLM backends. From top to bottom, the\nbatchsizevariesfrom1,8,to128\n16",
    "page": 16
  },
  {
    "type": "table",
    "content": "TABLE (Page 16):\n0.10\n)hWk(selpmaxe4201rofygrenE\n0.08\n0.06\n0.04\n0.02\nclassification summarization textgen translation | \n0.030\n0.025 )hWk(selpmaxe4201rofygrenE\n0.020\n0.015\n0.010\nclassification summarization textgen translation | \n0.025\n)hWk(selpmaxe4201rofygrenE\n0.020\n0.015\n0.010\n0.005\nclassification summarization textgen translation | ",
    "page": 16
  }
]