[
  {
    "type": "text",
    "content": "Carbon Emissions and Large Neural Network Training\nDavid Patterson1 , 2, Joseph Gonzalez2 , Quoc Le1 , Chen Liang1 , Lluis-Miquel Munguia1 ,\nDaniel Rothchild2 , David So1 , Maud Texier1 , and Jeff Dean1\n{davidpatterson, qvl, crazydonkey, llmunguia, davidso, maudt, jeff}@google.com,\n{pattrsn, jegonzal, drothchild}@berkeley.edu\nAbstract: The computation demand for machine learning (ML) h as grown rapidly recently, which comes with a\nnumber of costs. Estimating the energy cost helps measure its environmental impact and finding greener\nstrategies, yet it is c hallenging without detailed information.\nWe calculate the energy use and carbon footprint of several recent large models—T 5, M eena, G Shard,",
    "page": 1
  },
  {
    "type": "text",
    "content": "calculate the energy use and carbon footprint of several recent large models—T 5, M eena, G Shard,\nSwitch Transformer, and G PT-3— and refine earlier estimates for the neural architecture search that found\nEvolved Transformer.\nWe highlight the following opportunities to improve energy efficiency and C O equivalent emissions (C O e) :\n2 2\n● Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without\nsacrificing accuracy despite using as many or even more parameters.\n● Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and\nresulting CO e vary ~5X-10X, even within the same country and the same organization. We are now\n2\noptimizing where and when large models are trained.",
    "page": 1
  },
  {
    "type": "text",
    "content": "country and the same organization. We are now\n2\noptimizing where and when large models are trained.\n● Specific datacenter infrastructure matters, as Cloud datacenters can be ~1.4-2X more energy efficient\nthan typical datacenters, and the ML-oriented accelerators inside them can be ~2-5X more effective\nthan off-the-shelf systems.\nRemarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to ~100-1000X.\nThese large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we\nbelieve ML papers requiring large computational resources should make energy consumption and CO e\n2\nexplicit when practical. We are working to be more transparent about energy use and CO e in our future\n2\nresearch.",
    "page": 1
  },
  {
    "type": "text",
    "content": "practical. We are working to be more transparent about energy use and CO e in our future\n2\nresearch. To help reduce the carbon footprint of ML, we believe energy usage and CO e should be a key\n2\nmetric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during\ntraining and inference in this industry standard benchmark.\n1. Introduction\nAs ML models increase in scale, a general trend is that they become more accurate and more capable.\nHowever, larger models translate to greater computing demands and, by extension, greater energy demands.\nWe focus on natural language processing (NLP) because it is important in Google products and because of the\nrecent development of many large NLP models, e.g., T5 [Raf19], Meena [Adi20], GShard [Lep20], Switch",
    "page": 1
  },
  {
    "type": "text",
    "content": "ecent development of many large NLP models, e.g., T5 [Raf19], Meena [Adi20], GShard [Lep20], Switch\nTransformer [Fed21], and GPT-3 [Bro20]. R ecent studies attempt to evaluate the environmental impact of this\ntrend in NLP, which is difficult [Str19]. Here we investigate and share the estimates of the energy consumed\nand CO e3 of these recent and large NLP models. We also reduce by 88X an earlier estimate of the CO e for\n2 2\nthe neural architecture search for Evolved Transformer [So19, Str19] by characterizing the actual search\nprocess on the hardware and datacenter on which it was performed (see Appendices C and D).\nOur investigation into CO e revealed surprises and misunderstandings about the full Deep Neural Network\n2",
    "page": 1
  },
  {
    "type": "text",
    "content": "vestigation into CO e revealed surprises and misunderstandings about the full Deep Neural Network\n2\n(DNN) lifecycle, the datacenters and hardware that run them, the variations in energy mix, and the difficulty of\nassessing CO e accurately. Note that we are evaluating the CO e of o perating computers and datacenters, but\n2 2\nnot fabricating and recycling them (see [Gup20] for the latter topic).\nTo make it easier for the ML community to understand the real impact of training and how to reduce it, we\nendorse prior calls for new publication norms for computationally intensive ML models:\n1 Google\n2 University of California, Berkeley\n3 “CO e” means CO e quivalent emissions, accounting for carbon dioxide and all the other greenhouse gases as well:\n2 2\nmethane, nitrous oxide, ...",
    "page": 1
  },
  {
    "type": "text",
    "content": "nting for carbon dioxide and all the other greenhouse gases as well:\n2 2\nmethane, nitrous oxide, ... (calculated from Equation A-1 in 4 0 Code of Federal Regulations 98) . “CO emissions” is only\n2\ncarbon dioxide. t CO e stands for 1000 kg (metric ton) of CO e quivalent emissions.\n2 2\n1",
    "page": 1
  },
  {
    "type": "table",
    "content": "TABLE (Page 1):\nCarbon Emissions and Large Neural Network Training\nDavid Patterson1 , 2, Joseph Gonzalez2 , Quoc Le1 , Chen Liang1 , Lluis-Miquel Munguia1 ,\nDaniel Rothchild2 , David So1 , Maud Texier1 , and Jeff Dean1\n{davidpatterson, qvl, crazydonkey, llmunguia, davidso, maudt, jeff}@google.com,\n{pattrsn, jegonzal, drothchild}@berkeley.edu\nAbstract: The computation demand for machine learning (ML) h as grown rapidly recently, which comes with a\nnumber of costs. Estimating the energy cost helps measure its environmental impact and finding greener\nstrategies, yet it is c hallenging without detailed information.\nWe calculate the energy use and carbon footprint of several recent large models—T 5, M eena, G Shard,",
    "page": 1
  },
  {
    "type": "table",
    "content": "calculate the energy use and carbon footprint of several recent large models—T 5, M eena, G Shard,\nSwitch Transformer, and G PT-3— and refine earlier estimates for the neural architecture search that found\nEvolved Transformer.\nWe highlight the following opportunities to improve energy efficiency and C O equivalent emissions (C O e) :\n2 2\n● Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without\nsacrificing accuracy despite using as many or even more parameters.\n● Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and\nresulting CO e vary ~5X-10X, even within the same country and the same organization. We are now\n2\noptimizing where and when large models are trained.",
    "page": 1
  },
  {
    "type": "table",
    "content": "country and the same organization. We are now\n2\noptimizing where and when large models are trained.\n● Specific datacenter infrastructure matters, as Cloud datacenters can be ~1.4-2X more energy efficient\nthan typical datacenters, and the ML-oriented accelerators inside them can be ~2-5X more effective\nthan off-the-shelf systems.\nRemarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to ~100-1000X.\nThese large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we\nbelieve ML papers requiring large computational resources should make energy consumption and CO e\n2\nexplicit when practical. We are working to be more transparent about energy use and CO e in our future\n2\nresearch.",
    "page": 1
  },
  {
    "type": "table",
    "content": "practical. We are working to be more transparent about energy use and CO e in our future\n2\nresearch. To help reduce the carbon footprint of ML, we believe energy usage and CO e should be a key\n2\nmetric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during\ntraining and inference in this industry standard benchmark.\n1. Introduction\nAs ML models increase in scale, a general trend is that they become more accurate and more capable.\nHowever, larger models translate to greater computing demands and, by extension, greater energy demands.\nWe focus on natural language processing (NLP) because it is important in Google products and because of the\nrecent development of many large NLP models, e.g., T5 [Raf19], Meena [Adi20], GShard [Lep20], Switch",
    "page": 1
  },
  {
    "type": "table",
    "content": "ecent development of many large NLP models, e.g., T5 [Raf19], Meena [Adi20], GShard [Lep20], Switch\nTransformer [Fed21], and GPT-3 [Bro20]. R ecent studies attempt to evaluate the environmental impact of this\ntrend in NLP, which is difficult [Str19]. Here we investigate and share the estimates of the energy consumed\nand CO e3 of these recent and large NLP models. We also reduce by 88X an earlier estimate of the CO e for\n2 2\nthe neural architecture search for Evolved Transformer [So19, Str19] by characterizing the actual search\nprocess on the hardware and datacenter on which it was performed (see Appendices C and D).\nOur investigation into CO e revealed surprises and misunderstandings about the full Deep Neural Network\n2",
    "page": 1
  },
  {
    "type": "table",
    "content": "vestigation into CO e revealed surprises and misunderstandings about the full Deep Neural Network\n2\n(DNN) lifecycle, the datacenters and hardware that run them, the variations in energy mix, and the difficulty of\nassessing CO e accurately. Note that we are evaluating the CO e of o perating computers and datacenters, but\n2 2\nnot fabricating and recycling them (see [Gup20] for the latter topic).\nTo make it easier for the ML community to understand the real impact of training and how to reduce it, we\nendorse prior calls for new publication norms for computationally intensive ML models:\n1 Google\n2 University of California, Berkeley\n3 “CO e” means CO e quivalent emissions, accounting for carbon dioxide and all the other greenhouse gases as well:\n2 2\nmethane, nitrous oxide, ...",
    "page": 1
  },
  {
    "type": "table",
    "content": "nting for carbon dioxide and all the other greenhouse gases as well:\n2 2\nmethane, nitrous oxide, ... (calculated from Equation A-1 in 4 0 Code of Federal Regulations 98) . “CO emissions” is only\n2\ncarbon dioxide. t CO e stands for 1000 kg (metric ton) of CO e quivalent emissions.\n2 2\n1",
    "page": 1
  },
  {
    "type": "text",
    "content": "1. We must assess CO e correctly, but it is hard to quantify precisely in part because all the required\n2\ninformation is rarely reported or publicly available (e.g., datacenter, hardware, energy mix) and in part\nbecause it is hard to uncover important details afterwards (see Section 4.1). To make the carbon costs\nof training transparent, we encourage more researchers to measure energy usage and CO e—or to get\n2\na rough estimate using a tool like ML Emissions Calculator [Lac19] (Section 4.3)—and publish the data.\n2. We agree with [Str19,Sch20,Hen20] that efficiency should be an evaluation criterion for publishing ML\nresearch on computationally intensive models besides accuracy and related measures, since we need",
    "page": 2
  },
  {
    "type": "text",
    "content": "L\nresearch on computationally intensive models besides accuracy and related measures, since we need\nto encourage advances across the board as t he most sustainable energy is the energy you don’t use.\n3. And even if we could bring CO e to zero in cloud datacenters, reducing training time matters, both\n2\nbecause “time is money,” and because cheaper training lets more people participate. Hence, we also\nsecond the recommendation of [Str19] for more researchers to publish the number of accelerators and\ntheir time to train computationally intensive models to inspire progress in reducing training costs.\nWe believe such new incentives could lead to a virtuous cycle where ML practitioners compete to increase",
    "page": 2
  },
  {
    "type": "text",
    "content": "lieve such new incentives could lead to a virtuous cycle where ML practitioners compete to increase\naccuracy while lowering energy consumption and CO e that could bend the curve of ML carbon footprint\n2\ngrowth for computationally intensive NLP models.\nThe following sections summarize the findings that led to these recommendations. They also document our\nCO e estimates, highlight recent advances that curb the CO e of ML, and estimate the CO e from training the\n2 2 2\nfive recent large NLP models mentioned above. We end by updating the results of [Str19] on the emissions of\nthe Evolved Transformer neural architecture search and discussing common misperceptions.\nWe start with an overview of the carbon footprint over the DNN lifecycle and show ways to improve a",
    "page": 2
  },
  {
    "type": "text",
    "content": "We start with an overview of the carbon footprint over the DNN lifecycle and show ways to improve a\nconcrete example by nearly two orders of magnitude.\n2. Energy Consumption and Carbon Footprint of an NLP Model\nElectricity required to run an ML model is a function of the algorithm, the program that implements it, the\nnumber of processors that run the program, the speed and power of those processors, a datecenter’s\nefficiency in delivering power and cooling the processors, and the energy supply mix (renewable, gas, coal,\netc.). A simplified formula for the carbon footprint of an ML model that takes these factors into account is:\nFootprint = (electrical energy +queries ×electrical energy )×CO2e /KWh\ntrain inference datacenter",
    "page": 2
  },
  {
    "type": "text",
    "content": ":\nFootprint = (electrical energy +queries ×electrical energy )×CO2e /KWh\ntrain inference datacenter\nMost companies spend more energy on serving a DNN model (performing inference) than on training it.\nFor example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19]. Similarly,\nAmazon Web services claimed that 9 0% of the ML demand in the cloud is for inference [Bar19]. Given its\nsubstantial role in the ML model lifecycle, Alibaba, Amazon, Google, and NVIDIA designed ML accelerators\nsolely for inference. If the total ML energy is split 10% on training and 90% on serving, then even if a given ML\nmodel required double the energy cost of training, it could reduce overall total carbon emissions if that model\nalso cut serving energy by 20%.",
    "page": 2
  },
  {
    "type": "text",
    "content": "aining, it could reduce overall total carbon emissions if that model\nalso cut serving energy by 20%. Because energy usage during training is more isolated and thus easier to\ninvestigate than inference, we focus on it in this paper, but keep in mind that the carbon footprint of inference is\nsignificant.\nAn ML practitioner is often improving the quality of an existing model rather than starting from scratch. We\nwill use as a running example (found in [Str19]) the CO e impact of going from training a Transformer model\n2\nusing off-the-shelf hardware in an average datacenter to training an Evolved Transformer model on Google’s\ncustom hardware for DNNs in Google’s energy optimized datacenters. The large impact of each factor in this",
    "page": 2
  },
  {
    "type": "text",
    "content": "hardware for DNNs in Google’s energy optimized datacenters. The large impact of each factor in this\nexample demonstrates why we suggest that the trainers of a model be involved in the calculation of its costs.\nTable 1 shows the CO e breakdown, which we explain further in the next subsections along with the\n2\nbusiness rationale for these improvements, demonstrating the cross-cutting incentives for more efficient ML.\nFigure 1 illustrates the gains per step; the overall improvement in CO e is 57X. This large gain demonstrates\n2\nwhy the selection of the DNN model, processor, datacenter, and geographic location are critical to improve\nCO e. Table 2 shows the units for CO e and a running example that puts these units into perspective.\n2 2",
    "page": 2
  },
  {
    "type": "text",
    "content": "Table 2 shows the units for CO e and a running example that puts these units into perspective.\n2 2\nWe next go over the four factors in more detail that contribute to the carbon footprint of training.\n2",
    "page": 2
  },
  {
    "type": "table",
    "content": "TABLE (Page 2):\n\n1. We must assess CO e correctly, but it is hard to quantify precisely in part because all the required\n2\ninformation is rarely reported or publicly available (e.g., datacenter, hardware, energy mix) and in part\nbecause it is hard to uncover important details afterwards (see Section 4.1). To make the carbon costs\nof training transparent, we encourage more researchers to measure energy usage and CO e—or to get\n2\na rough estimate using a tool like ML Emissions Calculator [Lac19] (Section 4.3)—and publish the data.\n2. We agree with [Str19,Sch20,Hen20] that efficiency should be an evaluation criterion for publishing ML\nresearch on computationally intensive models besides accuracy and related measures, since we need",
    "page": 2
  },
  {
    "type": "table",
    "content": "L\nresearch on computationally intensive models besides accuracy and related measures, since we need\nto encourage advances across the board as t he most sustainable energy is the energy you don’t use.\n3. And even if we could bring CO e to zero in cloud datacenters, reducing training time matters, both\n2\nbecause “time is money,” and because cheaper training lets more people participate. Hence, we also\nsecond the recommendation of [Str19] for more researchers to publish the number of accelerators and\ntheir time to train computationally intensive models to inspire progress in reducing training costs.\nWe believe such new incentives could lead to a virtuous cycle where ML practitioners compete to increase",
    "page": 2
  },
  {
    "type": "table",
    "content": "lieve such new incentives could lead to a virtuous cycle where ML practitioners compete to increase\naccuracy while lowering energy consumption and CO e that could bend the curve of ML carbon footprint\n2\ngrowth for computationally intensive NLP models.\nThe following sections summarize the findings that led to these recommendations. They also document our\nCO e estimates, highlight recent advances that curb the CO e of ML, and estimate the CO e from training the\n2 2 2\nfive recent large NLP models mentioned above. We end by updating the results of [Str19] on the emissions of\nthe Evolved Transformer neural architecture search and discussing common misperceptions.\nWe start with an overview of the carbon footprint over the DNN lifecycle and show ways to improve a",
    "page": 2
  },
  {
    "type": "table",
    "content": "We start with an overview of the carbon footprint over the DNN lifecycle and show ways to improve a\nconcrete example by nearly two orders of magnitude.\n2. Energy Consumption and Carbon Footprint of an NLP Model\nElectricity required to run an ML model is a function of the algorithm, the program that implements it, the\nnumber of processors that run the program, the speed and power of those processors, a datecenter’s\nefficiency in delivering power and cooling the processors, and the energy supply mix (renewable, gas, coal,\netc.). A simplified formula for the carbon footprint of an ML model that takes these factors into account is:\nFootprint = (electrical energy +queries ×electrical energy )×CO2e /KWh\ntrain inference datacenter",
    "page": 2
  },
  {
    "type": "table",
    "content": ":\nFootprint = (electrical energy +queries ×electrical energy )×CO2e /KWh\ntrain inference datacenter\nMost companies spend more energy on serving a DNN model (performing inference) than on training it.\nFor example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19]. Similarly,\nAmazon Web services claimed that 9 0% of the ML demand in the cloud is for inference [Bar19]. Given its\nsubstantial role in the ML model lifecycle, Alibaba, Amazon, Google, and NVIDIA designed ML accelerators\nsolely for inference. If the total ML energy is split 10% on training and 90% on serving, then even if a given ML\nmodel required double the energy cost of training, it could reduce overall total carbon emissions if that model\nalso cut serving energy by 20%.",
    "page": 2
  },
  {
    "type": "table",
    "content": "aining, it could reduce overall total carbon emissions if that model\nalso cut serving energy by 20%. Because energy usage during training is more isolated and thus easier to\ninvestigate than inference, we focus on it in this paper, but keep in mind that the carbon footprint of inference is\nsignificant.\nAn ML practitioner is often improving the quality of an existing model rather than starting from scratch. We\nwill use as a running example (found in [Str19]) the CO e impact of going from training a Transformer model\n2\nusing off-the-shelf hardware in an average datacenter to training an Evolved Transformer model on Google’s\ncustom hardware for DNNs in Google’s energy optimized datacenters. The large impact of each factor in this",
    "page": 2
  },
  {
    "type": "table",
    "content": "hardware for DNNs in Google’s energy optimized datacenters. The large impact of each factor in this\nexample demonstrates why we suggest that the trainers of a model be involved in the calculation of its costs.\nTable 1 shows the CO e breakdown, which we explain further in the next subsections along with the\n2\nbusiness rationale for these improvements, demonstrating the cross-cutting incentives for more efficient ML.\nFigure 1 illustrates the gains per step; the overall improvement in CO e is 57X. This large gain demonstrates\n2\nwhy the selection of the DNN model, processor, datacenter, and geographic location are critical to improve\nCO e. Table 2 shows the units for CO e and a running example that puts these units into perspective.\n2 2",
    "page": 2
  },
  {
    "type": "table",
    "content": "Table 2 shows the units for CO e and a running example that puts these units into perspective.\n2 2\nWe next go over the four factors in more detail that contribute to the carbon footprint of training.\n2",
    "page": 2
  },
  {
    "type": "text",
    "content": "Evolved Evolved\nTransformer\nModel Transformer (Big) Transformer Transformer\n(Big)\n(Medium) (Medium)\nNumber of Parameters (B) 0.21 0.13 0.21 0.13\nDatacenter US Average Google Iowa Council Bluffs\nDatacenter Gross CO e/KWh (kg/KWh) 2020\n2 0.429 0.478\n(Section 2.4 and Appendix D)\nDatacenter Net CO e/KWh (kg/KWh) 2020 (Section\n2 0.429 0.080\n2.4 and Appendix D)\nDatacenter PUE (Latest quarter 2020) 1.59 1.11\nProcessor P100 TPU v2\nChip Thermal Design Power (TDP in Watts) 300 280\nMeasured System Average Power including\n296 271 229 227\nmemory, network interface, fans, host CPU (Watts)\nMeasured Performance (TFLOPS/s)5 6.7 4.7 28.8 24.0\nNumber of Chips 8\nTraining time to accuracy goal (days) 3.5 3.2 0.81 0.62\nTotal Computation (floating point operations) 1.61E+19 1.03E+19 1.61E+19 1.03E+19",
    "page": 3
  },
  {
    "type": "text",
    "content": "3.5 3.2 0.81 0.62\nTotal Computation (floating point operations) 1.61E+19 1.03E+19 1.61E+19 1.03E+19\nEnergy consumption (KWh) 316 221 185 40 30\nGross CO e for Model Training (metric ton) (Section\n2 0.1357 0.1055 0.0883 0.0189 0.0143\n2.4 and Appendix D)\nNet CO e for Model Training (metric ton) (Section\n2 0.1357 0.0177 0.0148 0.0032 0.0024\n2.4 and Appendix D)\n% 24/7 net carbon free energy (CY 2019) N/A 78%\nTable 1. See Appendix A for more detail4 . Estimates of CO e for Transformer and Evolved Transformer\n2\nfor P100 and TPU v2 are based on power measurements.5 Evolved Transformer (Medium) reached the\nsame accuracy as Transformer (Big) in [So19]. CO e i s shown both before (“gross”) and after (“net”)\n2",
    "page": 3
  },
  {
    "type": "text",
    "content": "e accuracy as Transformer (Big) in [So19]. CO e i s shown both before (“gross”) and after (“net”)\n2\naccounting for 24/7 reduction via real time, local carbon free energy purchases (Appendix B). To help\nput the CO e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO e (Table 2).\n2 2\nFigure 1. Improvement in CO e over Transformer (Big) on P100 GPUs in an average US datacenter\n2\nversus Evolved Transformer (Medium) on TPU v2s in the Google Iowa datacenter.\nSmall Unit Large Unit\nEnergy Consumption Kilowatt hours (KWh) Megawatt hours (MWh = 1000 KWh)\nCarbon Footprint (CO e or CO ) Kilograms (kg) Metric ton (t = 1000 kg)\n2 2\nSingle passenger round\nPerspective (see Appendix A) Passenger jet plane round trip SF-NY (180t CO e)\ntrip SF-NY (1.2t CO e) 2\n2\nTable 2.",
    "page": 3
  },
  {
    "type": "text",
    "content": "see Appendix A) Passenger jet plane round trip SF-NY (180t CO e)\ntrip SF-NY (1.2t CO e) 2\n2\nTable 2. Small and large units for energy and carbon footprint in this paper, plus airline travel CO e\n2\nused for perspective on the relative size of ML emissions compared to other activities (Section 4.8).\n4 The peak TeraFLOPS/second is 19 for P100 and 46 for TPU v2.\n5 Training on TPU v3 instead of TPU v2 takes Transformer (Big) 0.44 days (averaging 61 TFLOPS/s) and 0.37 days (47\nTFLOPS/s) for Evolved Transformer (Medium). For TPU v4, the respective numbers are 0.25 days (93 TFLOPS/s) and\n0.19 days (73 TFLOPS/s). TPU v3 shrinks energy consumed and gross and net CO e from TPU v2 by ~1.4X for\n2\nTransformer and by ~1.3X for Evolved Transformer.\n3",
    "page": 3
  },
  {
    "type": "text",
    "content": "rmer and by ~1.3X for Evolved Transformer.\n3",
    "page": 3
  },
  {
    "type": "table",
    "content": "TABLE (Page 3):\n\nEvolved Evolved\nTransformer\nModel Transformer (Big) Transformer Transformer\n(Big)\n(Medium) (Medium)\nNumber of Parameters (B) 0.21 0.13 0.21 0.13\nDatacenter US Average Google Iowa Council Bluffs\nDatacenter Gross CO e/KWh (kg/KWh) 2020\n2 0.429 0.478\n(Section 2.4 and Appendix D)\nDatacenter Net CO e/KWh (kg/KWh) 2020 (Section\n2 0.429 0.080\n2.4 and Appendix D)\nDatacenter PUE (Latest quarter 2020) 1.59 1.11\nProcessor P100 TPU v2\nChip Thermal Design Power (TDP in Watts) 300 280\nMeasured System Average Power including\n296 271 229 227\nmemory, network interface, fans, host CPU (Watts)\nMeasured Performance (TFLOPS/s)5 6.7 4.7 28.8 24.0\nNumber of Chips 8\nTraining time to accuracy goal (days) 3.5 3.2 0.81 0.62\nTotal Computation (floating point operations) 1.61E+19 1.03E+19 1.61E+19 1.",
    "page": 3
  },
  {
    "type": "table",
    "content": "(days) 3.5 3.2 0.81 0.62\nTotal Computation (floating point operations) 1.61E+19 1.03E+19 1.61E+19 1.03E+19\nEnergy consumption (KWh) 316 221 185 40 30\nGross CO e for Model Training (metric ton) (Section\n2 0.1357 0.1055 0.0883 0.0189 0.0143\n2.4 and Appendix D)\nNet CO e for Model Training (metric ton) (Section\n2 0.1357 0.0177 0.0148 0.0032 0.0024\n2.4 and Appendix D)\n% 24/7 net carbon free energy (CY 2019) N/A 78%\nTable 1. See Appendix A for more detail4 . Estimates of CO e for Transformer and Evolved Transformer\n2\nfor P100 and TPU v2 are based on power measurements.5 Evolved Transformer (Medium) reached the\nsame accuracy as Transformer (Big) in [So19]. CO e i s shown both before (“gross”) and after (“net”)\n2",
    "page": 3
  },
  {
    "type": "table",
    "content": "e accuracy as Transformer (Big) in [So19]. CO e i s shown both before (“gross”) and after (“net”)\n2\naccounting for 24/7 reduction via real time, local carbon free energy purchases (Appendix B). To help\nput the CO e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO e (Table 2).\n2 2\nFigure 1. Improvement in CO e over Transformer (Big) on P100 GPUs in an average US datacenter\n2\nversus Evolved Transformer (Medium) on TPU v2s in the Google Iowa datacenter.\nSmall Unit Large Unit\nEnergy Consumption Kilowatt hours (KWh) Megawatt hours (MWh = 1000 KWh)\nCarbon Footprint (CO e or CO ) Kilograms (kg) Metric ton (t = 1000 kg)\n2 2\nSingle passenger round\nPerspective (see Appendix A) Passenger jet plane round trip SF-NY (180t CO e)\ntrip SF-NY (1.2t CO e) 2\n2\nTable 2.",
    "page": 3
  },
  {
    "type": "table",
    "content": "see Appendix A) Passenger jet plane round trip SF-NY (180t CO e)\ntrip SF-NY (1.2t CO e) 2\n2\nTable 2. Small and large units for energy and carbon footprint in this paper, plus airline travel CO e\n2\nused for perspective on the relative size of ML emissions compared to other activities (Section 4.8).\n4 The peak TeraFLOPS/second is 19 for P100 and 46 for TPU v2.\n5 Training on TPU v3 instead of TPU v2 takes Transformer (Big) 0.44 days (averaging 61 TFLOPS/s) and 0.37 days (47\nTFLOPS/s) for Evolved Transformer (Medium). For TPU v4, the respective numbers are 0.25 days (93 TFLOPS/s) and\n0.19 days (73 TFLOPS/s). TPU v3 shrinks energy consumed and gross and net CO e from TPU v2 by ~1.4X for\n2\nTransformer and by ~1.3X for Evolved Transformer.\n3",
    "page": 3
  },
  {
    "type": "table",
    "content": "rmer and by ~1.3X for Evolved Transformer.\n3",
    "page": 3
  },
  {
    "type": "table",
    "content": "TABLE (Page 3):\nModel | Transformer (Big) |  | Evolved\nTransformer\n(Medium) | Transformer\n(Big) | Evolved\nTransformer\n(Medium)\nNumber of Parameters (B) | 0.21 |  | 0.13 | 0.21 | 0.13\nDatacenter | US Average | Google Iowa Council Bluffs |  |  | \nDatacenter Gross CO e/KWh (kg/KWh) 2020\n2\n(Section 2.4 and Appendix D) | 0.429 | 0.478 |  |  | \nDatacenter Net CO e/KWh (kg/KWh) 2020 (Section\n2\n2.4 and Appendix D) | 0.429 | 0.080 |  |  | \nDatacenter PUE (Latest quarter 2020) | 1.59 | 1.11 |  |  | \nProcessor | P100 |  |  | TPU v2 | \nChip Thermal Design Power (TDP in Watts) | 300 |  |  | 280 | \nMeasured System Average Power including\nmemory, network interface, fans, host CPU (Watts) | 296 |  | 271 | 229 | 227\nMeasured Performance (TFLOPS/s)5 | 6.7 |  | 4.7 | 28.8 | 24.0",
    "page": 3
  },
  {
    "type": "table",
    "content": "PU (Watts) | 296 |  | 271 | 229 | 227\nMeasured Performance (TFLOPS/s)5 | 6.7 |  | 4.7 | 28.8 | 24.0\nNumber of Chips | 8 |  |  |  | \nTraining time to accuracy goal (days) | 3.5 |  | 3.2 | 0.81 | 0.62\nTotal Computation (floating point operations) | 1.61E+19 |  | 1.03E+19 | 1.61E+19 | 1.03E+19\nEnergy consumption (KWh) | 316 | 221 | 185 | 40 | 30\nGross CO e for Model Training (metric ton) (Section\n2\n2.4 and Appendix D) | 0.1357 | 0.1055 | 0.0883 | 0.0189 | 0.0143\nNet CO e for Model Training (metric ton) (Section\n2\n2.4 and Appendix D) | 0.1357 | 0.0177 | 0.0148 | 0.0032 | 0.0024\n% 24/7 net carbon free energy (CY 2019) | N/A | 78% |  |  |",
    "page": 3
  },
  {
    "type": "table",
    "content": "TABLE (Page 3):\n | Small Unit | Large Unit\nEnergy Consumption | Kilowatt hours (KWh) | Megawatt hours (MWh = 1000 KWh)\nCarbon Footprint (CO e or CO )\n2 2 | Kilograms (kg) | Metric ton (t = 1000 kg)\nPerspective (see Appendix A) | Single passenger round\ntrip SF-NY (1.2t CO e)\n2 | Passenger jet plane round trip SF-NY (180t CO e)\n2",
    "page": 3
  },
  {
    "type": "text",
    "content": "2.1 Algorithm/program improvement\nThe Evolved Transformer (Medium) model discovered by So et al. [So19] using neural architecture search\n(see Section 4.1) uses 1.6X fewer FLOPS and 1.1X–1.3X less time than Transformer (Big) at slightly higher\naccuracy (see Table 1 and Appendix A)6 .\nBusiness Rationale. Training faster saves ML researchers time as well as saves their organizations money\nand reduces CO e.\n2\n2.2 Processor improvement\nGoogle’s custom TPU v2 processor runs Transformer (Big) 4.3X faster than P100 GPUs and Evolved\nTransformer (Medium) 5.2X faster.7 TPU v2 also uses less power: 1.3X less for Transformer and 1.2X less for\nEvolved Transformer. The net gain in performance/Watt is 5.6X and 6.2X, respectively.\nBusiness Rationale.",
    "page": 4
  },
  {
    "type": "text",
    "content": "ed Transformer. The net gain in performance/Watt is 5.6X and 6.2X, respectively.\nBusiness Rationale. The substantial increase in the scope and scale of deep learning over the past decade\nhas created the opportunity to build customized hardware that is tailored to the kinds of computations involved\nin training and serving DNN models. Instead of using GPUs like many other organizations, over the past seven\nyears Google has designed, built, and deployed four generations of custom Tensor Processing Unit (TPU)\nhardware for DNNs to accelerate model training and serving [Jou21]. To get a better return on their investment,\ncloud companies actually aim for improved cost-performance, as opposed to simply performance. Cost here",
    "page": 4
  },
  {
    "type": "text",
    "content": "d companies actually aim for improved cost-performance, as opposed to simply performance. Cost here\nmeans T otal Cost of Ownership (T CO) , which includes the annual operating costs such as electricity consumed\nand amortization of capital expenditures for the computer, cooling, power distribution, and the building. Jouppi\net al. show that power consumption is nearly perfectly linearly correlated with TCO8 [Jou21], so\nperformance/TCO gains also help performance/Watt, saving money and reducing CO e.\n2\n2.3 Datacenter improvement\nA useful quantitative metric of datacenter efficiency is the energy overhead above and beyond what directly\npowers the computing equipment inside the datacenters. If the overhead were 50%, the P ower Usage\nEffectiveness (P UE) would be 1.50.",
    "page": 4
  },
  {
    "type": "text",
    "content": "side the datacenters. If the overhead were 50%, the P ower Usage\nEffectiveness (P UE) would be 1.50. The US national datacenter average in 2018 was 1.58, which is the value\n[Str19] u sed; I n 2020, it was 1.59. Google p ublishes its datacenter PUE online every quarter. The PUE for the\nIowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better. Cloud datacenters are\nroughly 2X as energy efficient as a typical enterprise datacenter due to other factors like server utilization (see\n[Höl20]), but we’ll limit the quantitative improvement in this paper to the easy-to-measure PUE.\nMore broadly, since cloud datacenters are much more energy efficient, the long-feared explosion of\ndatacenter energy usage has not materialized.",
    "page": 4
  },
  {
    "type": "text",
    "content": "ch more energy efficient, the long-feared explosion of\ndatacenter energy usage has not materialized. A recent paper in S cience [Mas20] found that global datacenter\nenergy consumption increased by only 6% compared with 2010, despite computing capacity increasing by\n550% over the same time period [Mas21].\nBusiness Rationale. Cloud companies strive for energy efficient datacenters since it saves money and\nlowers emissions. Perhaps we should add “energy is money” to Ben Franklin’s “time is money” advice?\n2.4 Energy mix improvement\nThe gross carbon intensity of energy according to the U.S. average mix is 0 .429 kg of CO e/KWh [USE21].\n2\nAfter matching Google’s clean energy purchase per its 24/7 carbon-free energy framework (see Appendix B),\nthe net CO e drops to 0.",
    "page": 4
  },
  {
    "type": "text",
    "content": "energy purchase per its 24/7 carbon-free energy framework (see Appendix B),\nthe net CO e drops to 0.080 for the Iowa datacenter where we ran Evolved Transformer, which is 5.4X better.\n2\nBusiness Rationale. Transmitting electricity long distances is more expensive and less efficient than\nsending information as photons over optical fibers [Arm10]. Cloud computing allows companies like Google to\nhave a global portfolio of datacenters, many of which are placed where the grid is cleaner (e.g., Finland) or\nwhere companies can purchase clean energy directly (e.g., Iowa). In 2020 Google announced a new objective\nin its energy strategy: by 2030, it aims to run all Google datacenters and offices on carbon-free energy 24/7.",
    "page": 4
  },
  {
    "type": "text",
    "content": "gy strategy: by 2030, it aims to run all Google datacenters and offices on carbon-free energy 24/7.\nFor our 24/7 carbon-free energy accounting (see Appendix B), we deduct from the hourly consumption all\n6 Their neural architecture search also found another version that had the same performance but better accuracy.\n7 [Str19] used P100s, which are contemporary GPUs to TPU v2s.\n8 The correlation coefficient R between TCO and TDP is 0.99 out of 1.00 across four generations of TPUs.\n4",
    "page": 4
  },
  {
    "type": "table",
    "content": "TABLE (Page 4):\n\n2.1 Algorithm/program improvement\nThe Evolved Transformer (Medium) model discovered by So et al. [So19] using neural architecture search\n(see Section 4.1) uses 1.6X fewer FLOPS and 1.1X–1.3X less time than Transformer (Big) at slightly higher\naccuracy (see Table 1 and Appendix A)6 .\nBusiness Rationale. Training faster saves ML researchers time as well as saves their organizations money\nand reduces CO e.\n2\n2.2 Processor improvement\nGoogle’s custom TPU v2 processor runs Transformer (Big) 4.3X faster than P100 GPUs and Evolved\nTransformer (Medium) 5.2X faster.7 TPU v2 also uses less power: 1.3X less for Transformer and 1.2X less for\nEvolved Transformer. The net gain in performance/Watt is 5.6X and 6.2X, respectively.\nBusiness Rationale.",
    "page": 4
  },
  {
    "type": "table",
    "content": "ed Transformer. The net gain in performance/Watt is 5.6X and 6.2X, respectively.\nBusiness Rationale. The substantial increase in the scope and scale of deep learning over the past decade\nhas created the opportunity to build customized hardware that is tailored to the kinds of computations involved\nin training and serving DNN models. Instead of using GPUs like many other organizations, over the past seven\nyears Google has designed, built, and deployed four generations of custom Tensor Processing Unit (TPU)\nhardware for DNNs to accelerate model training and serving [Jou21]. To get a better return on their investment,\ncloud companies actually aim for improved cost-performance, as opposed to simply performance. Cost here",
    "page": 4
  },
  {
    "type": "table",
    "content": "d companies actually aim for improved cost-performance, as opposed to simply performance. Cost here\nmeans T otal Cost of Ownership (T CO) , which includes the annual operating costs such as electricity consumed\nand amortization of capital expenditures for the computer, cooling, power distribution, and the building. Jouppi\net al. show that power consumption is nearly perfectly linearly correlated with TCO8 [Jou21], so\nperformance/TCO gains also help performance/Watt, saving money and reducing CO e.\n2\n2.3 Datacenter improvement\nA useful quantitative metric of datacenter efficiency is the energy overhead above and beyond what directly\npowers the computing equipment inside the datacenters. If the overhead were 50%, the P ower Usage\nEffectiveness (P UE) would be 1.50.",
    "page": 4
  },
  {
    "type": "table",
    "content": "side the datacenters. If the overhead were 50%, the P ower Usage\nEffectiveness (P UE) would be 1.50. The US national datacenter average in 2018 was 1.58, which is the value\n[Str19] u sed; I n 2020, it was 1.59. Google p ublishes its datacenter PUE online every quarter. The PUE for the\nIowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better. Cloud datacenters are\nroughly 2X as energy efficient as a typical enterprise datacenter due to other factors like server utilization (see\n[Höl20]), but we’ll limit the quantitative improvement in this paper to the easy-to-measure PUE.\nMore broadly, since cloud datacenters are much more energy efficient, the long-feared explosion of\ndatacenter energy usage has not materialized.",
    "page": 4
  },
  {
    "type": "table",
    "content": "ch more energy efficient, the long-feared explosion of\ndatacenter energy usage has not materialized. A recent paper in S cience [Mas20] found that global datacenter\nenergy consumption increased by only 6% compared with 2010, despite computing capacity increasing by\n550% over the same time period [Mas21].\nBusiness Rationale. Cloud companies strive for energy efficient datacenters since it saves money and\nlowers emissions. Perhaps we should add “energy is money” to Ben Franklin’s “time is money” advice?\n2.4 Energy mix improvement\nThe gross carbon intensity of energy according to the U.S. average mix is 0 .429 kg of CO e/KWh [USE21].\n2\nAfter matching Google’s clean energy purchase per its 24/7 carbon-free energy framework (see Appendix B),\nthe net CO e drops to 0.",
    "page": 4
  },
  {
    "type": "table",
    "content": "energy purchase per its 24/7 carbon-free energy framework (see Appendix B),\nthe net CO e drops to 0.080 for the Iowa datacenter where we ran Evolved Transformer, which is 5.4X better.\n2\nBusiness Rationale. Transmitting electricity long distances is more expensive and less efficient than\nsending information as photons over optical fibers [Arm10]. Cloud computing allows companies like Google to\nhave a global portfolio of datacenters, many of which are placed where the grid is cleaner (e.g., Finland) or\nwhere companies can purchase clean energy directly (e.g., Iowa). In 2020 Google announced a new objective\nin its energy strategy: by 2030, it aims to run all Google datacenters and offices on carbon-free energy 24/7.",
    "page": 4
  },
  {
    "type": "table",
    "content": "gy strategy: by 2030, it aims to run all Google datacenters and offices on carbon-free energy 24/7.\nFor our 24/7 carbon-free energy accounting (see Appendix B), we deduct from the hourly consumption all\n6 Their neural architecture search also found another version that had the same performance but better accuracy.\n7 [Str19] used P100s, which are contemporary GPUs to TPU v2s.\n8 The correlation coefficient R between TCO and TDP is 0.99 out of 1.00 across four generations of TPUs.\n4",
    "page": 4
  },
  {
    "type": "text",
    "content": "clean energy purchased on that same geographically local grid and the same hour, which results in the net\nCO e/KWh value. As Iowa has strong nighttime winds, Google’s wind portfolio lowered Iowa's datacenter g ross\n2\naverage CO e/KWh in December 2020 by 6X, from the local grid’s 0.478 kg to a n et average of 0.080 kg.\n2\n2.5 Summary: Formulas for energy consumption and carbon footprint of training\nReducing CO e is not only a moral obligation but ultimately sound business. To decrease the footprint of\n2\ntraining, an ML researcher should pick the DNN model, the processor, and the datacenter carefully.9 Cutting\nenergy saves money and CO e and improving the energy mix reduces CO e. We refactor the equation above\n2 2",
    "page": 5
  },
  {
    "type": "text",
    "content": "saves money and CO e and improving the energy mix reduces CO e. We refactor the equation above\n2 2\nfor training into energy consumption and its carbon footprint (tCO e m eans metric tons of CO e):\n2 2\nKWh = Hours to train ×Number of Processors×Average Power per Processor×PUE ÷1000\ntCO2e = KWh×kg CO2e per KWh÷1000\nWe believe it is straightforward for ML practitioners to calculate energy consumption. They already know\nhours to train and number of processors. Google and Facebook publish PUE of their datacenters, so that is\neasy to look up for those clouds. If cloud providers don’t share PUE, use the US average PUE as in [Str19].\nWe measured the power of the processors during training, which is ideal, but using the average of the training",
    "page": 5
  },
  {
    "type": "text",
    "content": "the power of the processors during training, which is ideal, but using the average of the training\nof several similar models is probably sufficient and much easier.1 0 Table 3 shows the average power and\nstandard deviation for the processors and DNNs that we measured in this paper.\nThe final piece is the CO e of the datacenter at the time the model was run. Google calculates the average\n2\nper month, which is close enough, and it is now available for Google employees to look up. Without access to\nsuch a dashboard, use the ML Emissions Calculator [Lac19] or Green Algorithms tool [Lan20] that estimate the\nCO e mix by region (see Figure 6 below)1 1. While not absolutely necessary, we hope the ML community will\n2",
    "page": 5
  },
  {
    "type": "text",
    "content": "by region (see Figure 6 below)1 1. While not absolutely necessary, we hope the ML community will\n2\nlobby all cloud providers to reveal the actual energy mix, since it can vary within a region. F or example, t o let\ncustomers pick the datacenter based on CO e, G oogle Cloud recently released the percentage of carbon-free\n2\nenergy and gross CO e of its datacenters and committed to publishing updated figures going forward.\n2\nWe next show the impact of these three choices on much larger NLP models.\nProcessor Average (Watts) StDev % DNNs used to calculate average power\nTransformer (Big), Evolved Transformer (Medium), Neural Architecture\nTPU v2 221 5%\nSearch [So19]\nTPU v3 283 10% T5, Meena, Gshard, Switch Transformer\nTransformer (Big), Evolved Transformer (Medium), Neural Architecture",
    "page": 5
  },
  {
    "type": "text",
    "content": "na, Gshard, Switch Transformer\nTransformer (Big), Evolved Transformer (Medium), Neural Architecture\nP100 GPU 271 11%\nSearch [So19]\nV100 GPU 325 2% Transformer (Big), GPT-3 [Sut21]\nTable 3. Average system power per processor and standard deviation for DNNs in this paper. We\nmeasured the Google DNNs (see Tables 1 and 4). OpenAI measured GPT-3 in a Microsoft Azure\ndatacenter [Sut21].\n3. Energy Usage and CO e Emissions of Five Recent Large NLP Models\n2\nA natural question that follows is what about the training CO e of much larger NLP models? Table 4 and\n2\nAppendix A show a CO e calculation1 1 for five of them: T5, Meena, GShard, and Switch Transformer from\n2\nGoogle plus GPT-3 from Open AI that runs on Microsoft Azure Cloud:",
    "page": 5
  },
  {
    "type": "text",
    "content": "d, and Switch Transformer from\n2\nGoogle plus GPT-3 from Open AI that runs on Microsoft Azure Cloud:\n● T5 is a pre-trained language model that casts all NLP problems in a unified text-to-text format to enable\napplication of transfer learning techniques to reduce the cost of training [Raf19]. The largest size has\n11B parameters, and training used 86 MWh and produced 47 tCO e.\n2\n● Meena is a multi-turn open-domain chatbot [Adi20]. This 2.6B parameter DNN is trained to minimize\nperplexity of the next token. The year-old companion paper has ~150 citations. Training Meena used\n9 PUE and kg CO e per KWh are functions of the datacenter where the model is run.\n2\n10 The ML Emissions Calculator [Lac19] also estimates power per processor. It now uses the values in Table 3 for TPU v2",
    "page": 5
  },
  {
    "type": "text",
    "content": "Calculator [Lac19] also estimates power per processor. It now uses the values in Table 3 for TPU v2\nand TPU v3 [Luc21]. At the time of this writing, the calculator shows CO e produced but not the estimated power per\n2\nprocessor, energy consumed, or CO e/KWh.\n2\n11 The Google models happen to be run in datacenters where the gross and net CO e were the same or close.\n2\n5",
    "page": 5
  },
  {
    "type": "table",
    "content": "TABLE (Page 5):\n\nclean energy purchased on that same geographically local grid and the same hour, which results in the net\nCO e/KWh value. As Iowa has strong nighttime winds, Google’s wind portfolio lowered Iowa's datacenter g ross\n2\naverage CO e/KWh in December 2020 by 6X, from the local grid’s 0.478 kg to a n et average of 0.080 kg.\n2\n2.5 Summary: Formulas for energy consumption and carbon footprint of training\nReducing CO e is not only a moral obligation but ultimately sound business. To decrease the footprint of\n2\ntraining, an ML researcher should pick the DNN model, the processor, and the datacenter carefully.9 Cutting\nenergy saves money and CO e and improving the energy mix reduces CO e. We refactor the equation above\n2 2",
    "page": 5
  },
  {
    "type": "table",
    "content": "saves money and CO e and improving the energy mix reduces CO e. We refactor the equation above\n2 2\nfor training into energy consumption and its carbon footprint (tCO e m eans metric tons of CO e):\n2 2\nKWh = Hours to train ×Number of Processors×Average Power per Processor×PUE ÷1000\ntCO2e = KWh×kg CO2e per KWh÷1000\nWe believe it is straightforward for ML practitioners to calculate energy consumption. They already know\nhours to train and number of processors. Google and Facebook publish PUE of their datacenters, so that is\neasy to look up for those clouds. If cloud providers don’t share PUE, use the US average PUE as in [Str19].\nWe measured the power of the processors during training, which is ideal, but using the average of the training",
    "page": 5
  },
  {
    "type": "table",
    "content": "the power of the processors during training, which is ideal, but using the average of the training\nof several similar models is probably sufficient and much easier.1 0 Table 3 shows the average power and\nstandard deviation for the processors and DNNs that we measured in this paper.\nThe final piece is the CO e of the datacenter at the time the model was run. Google calculates the average\n2\nper month, which is close enough, and it is now available for Google employees to look up. Without access to\nsuch a dashboard, use the ML Emissions Calculator [Lac19] or Green Algorithms tool [Lan20] that estimate the\nCO e mix by region (see Figure 6 below)1 1. While not absolutely necessary, we hope the ML community will\n2",
    "page": 5
  },
  {
    "type": "table",
    "content": "by region (see Figure 6 below)1 1. While not absolutely necessary, we hope the ML community will\n2\nlobby all cloud providers to reveal the actual energy mix, since it can vary within a region. F or example, t o let\ncustomers pick the datacenter based on CO e, G oogle Cloud recently released the percentage of carbon-free\n2\nenergy and gross CO e of its datacenters and committed to publishing updated figures going forward.\n2\nWe next show the impact of these three choices on much larger NLP models.\nProcessor Average (Watts) StDev % DNNs used to calculate average power\nTransformer (Big), Evolved Transformer (Medium), Neural Architecture\nTPU v2 221 5%\nSearch [So19]\nTPU v3 283 10% T5, Meena, Gshard, Switch Transformer\nTransformer (Big), Evolved Transformer (Medium), Neural Architecture",
    "page": 5
  },
  {
    "type": "table",
    "content": "na, Gshard, Switch Transformer\nTransformer (Big), Evolved Transformer (Medium), Neural Architecture\nP100 GPU 271 11%\nSearch [So19]\nV100 GPU 325 2% Transformer (Big), GPT-3 [Sut21]\nTable 3. Average system power per processor and standard deviation for DNNs in this paper. We\nmeasured the Google DNNs (see Tables 1 and 4). OpenAI measured GPT-3 in a Microsoft Azure\ndatacenter [Sut21].\n3. Energy Usage and CO e Emissions of Five Recent Large NLP Models\n2\nA natural question that follows is what about the training CO e of much larger NLP models? Table 4 and\n2\nAppendix A show a CO e calculation1 1 for five of them: T5, Meena, GShard, and Switch Transformer from\n2\nGoogle plus GPT-3 from Open AI that runs on Microsoft Azure Cloud:",
    "page": 5
  },
  {
    "type": "table",
    "content": "d, and Switch Transformer from\n2\nGoogle plus GPT-3 from Open AI that runs on Microsoft Azure Cloud:\n● T5 is a pre-trained language model that casts all NLP problems in a unified text-to-text format to enable\napplication of transfer learning techniques to reduce the cost of training [Raf19]. The largest size has\n11B parameters, and training used 86 MWh and produced 47 tCO e.\n2\n● Meena is a multi-turn open-domain chatbot [Adi20]. This 2.6B parameter DNN is trained to minimize\nperplexity of the next token. The year-old companion paper has ~150 citations. Training Meena used\n9 PUE and kg CO e per KWh are functions of the datacenter where the model is run.\n2\n10 The ML Emissions Calculator [Lac19] also estimates power per processor. It now uses the values in Table 3 for TPU v2",
    "page": 5
  },
  {
    "type": "table",
    "content": "Calculator [Lac19] also estimates power per processor. It now uses the values in Table 3 for TPU v2\nand TPU v3 [Luc21]. At the time of this writing, the calculator shows CO e produced but not the estimated power per\n2\nprocessor, energy consumed, or CO e/KWh.\n2\n11 The Google models happen to be run in datacenters where the gross and net CO e were the same or close.\n2\n5",
    "page": 5
  },
  {
    "type": "table",
    "content": "TABLE (Page 5):\nProcessor | Average (Watts) | StDev % | DNNs used to calculate average power\nTPU v2 | 221 | 5% | Transformer (Big), Evolved Transformer (Medium), Neural Architecture\nSearch [So19]\nTPU v3 | 283 | 10% | T5, Meena, Gshard, Switch Transformer\nP100 GPU | 271 | 11% | Transformer (Big), Evolved Transformer (Medium), Neural Architecture\nSearch [So19]\nV100 GPU | 325 | 2% | Transformer (Big), GPT-3 [Sut21]",
    "page": 5
  },
  {
    "type": "text",
    "content": "232 MWh and emissions was 96 tCO e. As Evolved Transformer saved 48 tCO e alone for the single\n2 2\nuse case of developing Meena (see Table 4), the 3.2 net tCO e cost for its development returned 15:1.\n2\n● GShard is composed of a set of lightweight annotation APIs that provide an elegant way to express a\nwide range of parallel computation patterns with minimal changes to the existing model code [Lep20]. It\nenabled scaling up of a multilingual neural machine translation Transformer model with sparsely gated\nmixture-of-experts (MoE) [Sha17] using automatic sharding. The GShard-600B model is a particular\nuse of that framework for training a multi-lingual translation model with 600B total parameters. Sparse",
    "page": 6
  },
  {
    "type": "text",
    "content": "of that framework for training a multi-lingual translation model with 600B total parameters. Sparse\nmodels can have many model parameters while requiring much less computation than dense models.\nTraining GShard-600B used 24 MWh and produced 4.3 net tCO e.\n2\n● Switch Transformer simplifies the Mixture of Expert (MoE) routing algorithm to design intuitive improved\nmodels with reduced communication and computational costs [Fed21]. The authors show large sparse\nmodels—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in\npre-training speed with the same computational resources. We estimated it used 179 MWh and\nproduced 59 net tCO e.\n2\nEvolved\nSwitch\nTrans- Gshard\nModel T5 Meena Trans- GPT-3\nformer -600B\nformer\nNAS\n0.064 per\nNumber of Parameters (B) 11 2.",
    "page": 6
  },
  {
    "type": "text",
    "content": "Gshard\nModel T5 Meena Trans- GPT-3\nformer -600B\nformer\nNAS\n0.064 per\nNumber of Parameters (B) 11 2.6 619 1500 175\nmodel\nPercent of model activated on every token 100% 100% 100% 0.25% 0.10% 100%\nDeveloper Google OpenAI\nGoogle\nGoogle Google Google Google\nDatacenter of original experiment North Microsoft\nGeorgia Taiwan Georgia Georgia\nCarolina\nWhen model ran Dec 2018 Sep 2019 Dec 2019 Apr 2020 Oct 2020 2020\nDatacenter Gross CO e/KWh (kg/KWh when it was run) 0.431 0.545 0.415 0.201 0.403 0.429\n2\nDatacenter Net CO2e/KWh (kg/KWh when it was run) 0.431 0.545 0.415 0.177 0.330 0.429\nDatacenter PUE (when it was run) 1.10 1.12 1.09 1.09 1.10 1.10\nProcessor TPU v2 TPU v3 V100\nChip Thermal Design Power (TDP in Watts) 280 450 300\nMeasured System Average Power per Accelerator,\n208 310 289 288 245 330",
    "page": 6
  },
  {
    "type": "text",
    "content": "r (TDP in Watts) 280 450 300\nMeasured System Average Power per Accelerator,\n208 310 289 288 245 330\nincluding memory, network interface, fans, host CPU (W)\nMeasured Performance (TFLOPS/s)1 2 24.8 45.6 42.3 48.0 34.4 24.6\nNumber of Chips 200 512 1024 1024 1024 10,000\nTraining time (days) 6.8 20 30 3.1 27 14.8\nTotal Computation (floating point operations) 2.91E+21 4.05E+22 1.12E+23 1.33E+22 8.22E+22 3.14E+23\nEnergy Consumption (MWh) 7.5 85.7 232 24.1 179 1,287\n% of Google 2019 total energy consumption (12.2 TWh\n0.00006% 0.00070% 0.00190% 0.00020% 0.00147% 0.01055%\n= 12,200,000 MWh) [Goo20]\nGross tCO e for Model Training 3.2 46.7 96.4 4.8 72.2 552.1\n2\nNet tCO e for Model Training 3.2 46.7 96.4 4.3 59.1 552.1\n2\nFraction of NAS Estimate in [Str19] (284 tCO2e) 0.011 0.164 0.340 0.015 0.208 1.",
    "page": 6
  },
  {
    "type": "text",
    "content": ".4 4.3 59.1 552.1\n2\nFraction of NAS Estimate in [Str19] (284 tCO2e) 0.011 0.164 0.340 0.015 0.208 1.944\nFraction of equivalent jet plane CO e round trip San\n2 0.018 0.258 0.533 0.024 0.327 3.054\nFrancisco ↔ New York (~180 t; see Ap. A)\ntCO e savings by Meena using Evolved Transformer -- -- 48.5 -- -- --\n2\n% 24/x7 carbon free energy (when run) 31% 19% 30% 73% 43% N/A\nTable 4. CO e for NLP models (see Appendix A)1 2. V100’s TDP is closer to average power due to T urbo\n2\nmode and D VFS. TPUs don’t offer them, so their TDP is much higher than their average power.\n12 The peak TeraFLOPS/second is 46 for TPU v2, 123 for TPU v3, and 125 for V100.\n6",
    "page": 6
  },
  {
    "type": "table",
    "content": "TABLE (Page 6):\n\n232 MWh and emissions was 96 tCO e. As Evolved Transformer saved 48 tCO e alone for the single\n2 2\nuse case of developing Meena (see Table 4), the 3.2 net tCO e cost for its development returned 15:1.\n2\n● GShard is composed of a set of lightweight annotation APIs that provide an elegant way to express a\nwide range of parallel computation patterns with minimal changes to the existing model code [Lep20]. It\nenabled scaling up of a multilingual neural machine translation Transformer model with sparsely gated\nmixture-of-experts (MoE) [Sha17] using automatic sharding. The GShard-600B model is a particular\nuse of that framework for training a multi-lingual translation model with 600B total parameters. Sparse",
    "page": 6
  },
  {
    "type": "table",
    "content": "of that framework for training a multi-lingual translation model with 600B total parameters. Sparse\nmodels can have many model parameters while requiring much less computation than dense models.\nTraining GShard-600B used 24 MWh and produced 4.3 net tCO e.\n2\n● Switch Transformer simplifies the Mixture of Expert (MoE) routing algorithm to design intuitive improved\nmodels with reduced communication and computational costs [Fed21]. The authors show large sparse\nmodels—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in\npre-training speed with the same computational resources. We estimated it used 179 MWh and\nproduced 59 net tCO e.\n2\nEvolved\nSwitch\nTrans- Gshard\nModel T5 Meena Trans- GPT-3\nformer -600B\nformer\nNAS\n0.064 per\nNumber of Parameters (B) 11 2.",
    "page": 6
  },
  {
    "type": "table",
    "content": "Gshard\nModel T5 Meena Trans- GPT-3\nformer -600B\nformer\nNAS\n0.064 per\nNumber of Parameters (B) 11 2.6 619 1500 175\nmodel\nPercent of model activated on every token 100% 100% 100% 0.25% 0.10% 100%\nDeveloper Google OpenAI\nGoogle\nGoogle Google Google Google\nDatacenter of original experiment North Microsoft\nGeorgia Taiwan Georgia Georgia\nCarolina\nWhen model ran Dec 2018 Sep 2019 Dec 2019 Apr 2020 Oct 2020 2020\nDatacenter Gross CO e/KWh (kg/KWh when it was run) 0.431 0.545 0.415 0.201 0.403 0.429\n2\nDatacenter Net CO2e/KWh (kg/KWh when it was run) 0.431 0.545 0.415 0.177 0.330 0.429\nDatacenter PUE (when it was run) 1.10 1.12 1.09 1.09 1.10 1.10\nProcessor TPU v2 TPU v3 V100\nChip Thermal Design Power (TDP in Watts) 280 450 300\nMeasured System Average Power per Accelerator,\n208 310 289 288 245 330",
    "page": 6
  },
  {
    "type": "table",
    "content": "r (TDP in Watts) 280 450 300\nMeasured System Average Power per Accelerator,\n208 310 289 288 245 330\nincluding memory, network interface, fans, host CPU (W)\nMeasured Performance (TFLOPS/s)1 2 24.8 45.6 42.3 48.0 34.4 24.6\nNumber of Chips 200 512 1024 1024 1024 10,000\nTraining time (days) 6.8 20 30 3.1 27 14.8\nTotal Computation (floating point operations) 2.91E+21 4.05E+22 1.12E+23 1.33E+22 8.22E+22 3.14E+23\nEnergy Consumption (MWh) 7.5 85.7 232 24.1 179 1,287\n% of Google 2019 total energy consumption (12.2 TWh\n0.00006% 0.00070% 0.00190% 0.00020% 0.00147% 0.01055%\n= 12,200,000 MWh) [Goo20]\nGross tCO e for Model Training 3.2 46.7 96.4 4.8 72.2 552.1\n2\nNet tCO e for Model Training 3.2 46.7 96.4 4.3 59.1 552.1\n2\nFraction of NAS Estimate in [Str19] (284 tCO2e) 0.011 0.164 0.340 0.015 0.208 1.",
    "page": 6
  },
  {
    "type": "table",
    "content": ".4 4.3 59.1 552.1\n2\nFraction of NAS Estimate in [Str19] (284 tCO2e) 0.011 0.164 0.340 0.015 0.208 1.944\nFraction of equivalent jet plane CO e round trip San\n2 0.018 0.258 0.533 0.024 0.327 3.054\nFrancisco ↔ New York (~180 t; see Ap. A)\ntCO e savings by Meena using Evolved Transformer -- -- 48.5 -- -- --\n2\n% 24/x7 carbon free energy (when run) 31% 19% 30% 73% 43% N/A\nTable 4. CO e for NLP models (see Appendix A)1 2. V100’s TDP is closer to average power due to T urbo\n2\nmode and D VFS. TPUs don’t offer them, so their TDP is much higher than their average power.\n12 The peak TeraFLOPS/second is 46 for TPU v2, 123 for TPU v3, and 125 for V100.\n6",
    "page": 6
  },
  {
    "type": "table",
    "content": "TABLE (Page 6):\nModel | Evolved\nTrans-\nformer\nNAS | T5 | Meena | Gshard\n-600B | Switch\nTrans-\nformer | GPT-3\nNumber of Parameters (B) | 0.064 per\nmodel | 11 | 2.6 | 619 | 1500 | 175\nPercent of model activated on every token | 100% | 100% | 100% | 0.25% | 0.10% | 100%\nDeveloper | Google |  |  |  |  | OpenAI\nDatacenter of original experiment | Google\nGeorgia | Google\nTaiwan | Google\nGeorgia | Google\nNorth\nCarolina | Google\nGeorgia | Microsoft\nWhen model ran | Dec 2018 | Sep 2019 | Dec 2019 | Apr 2020 | Oct 2020 | 2020\nDatacenter Gross CO e/KWh (kg/KWh when it was run)\n2 | 0.431 | 0.545 | 0.415 | 0.201 | 0.403 | 0.429\nDatacenter Net CO2e/KWh (kg/KWh when it was run) | 0.431 | 0.545 | 0.415 | 0.177 | 0.330 | 0.429\nDatacenter PUE (when it was run) | 1.10 | 1.12 | 1.09 | 1.09 | 1.10 | 1.10",
    "page": 6
  },
  {
    "type": "table",
    "content": "| 0.177 | 0.330 | 0.429\nDatacenter PUE (when it was run) | 1.10 | 1.12 | 1.09 | 1.09 | 1.10 | 1.10\nProcessor | TPU v2 | TPU v3 |  |  |  | V100\nChip Thermal Design Power (TDP in Watts) | 280 | 450 |  |  |  | 300\nMeasured System Average Power per Accelerator,\nincluding memory, network interface, fans, host CPU (W) | 208 | 310 | 289 | 288 | 245 | 330\nMeasured Performance (TFLOPS/s)1 2 | 24.8 | 45.6 | 42.3 | 48.0 | 34.4 | 24.6\nNumber of Chips | 200 | 512 | 1024 | 1024 | 1024 | 10,000\nTraining time (days) | 6.8 | 20 | 30 | 3.1 | 27 | 14.8\nTotal Computation (floating point operations) | 2.91E+21 | 4.05E+22 | 1.12E+23 | 1.33E+22 | 8.22E+22 | 3.14E+23\nEnergy Consumption (MWh) | 7.5 | 85.7 | 232 | 24.1 | 179 | 1,287\n% of Google 2019 total energy consumption (12.2 TWh\n= 12,200,000 MWh) [Goo20] | 0.",
    "page": 6
  },
  {
    "type": "table",
    "content": "4.1 | 179 | 1,287\n% of Google 2019 total energy consumption (12.2 TWh\n= 12,200,000 MWh) [Goo20] | 0.00006% | 0.00070% | 0.00190% | 0.00020% | 0.00147% | 0.01055%\nGross tCO e for Model Training\n2 | 3.2 | 46.7 | 96.4 | 4.8 | 72.2 | 552.1\nNet tCO e for Model Training\n2 | 3.2 | 46.7 | 96.4 | 4.3 | 59.1 | 552.1\nFraction of NAS Estimate in [Str19] (284 tCO2e) | 0.011 | 0.164 | 0.340 | 0.015 | 0.208 | 1.944\nFraction of equivalent jet plane CO e round trip San\n2\nFrancisco ↔ New York (~180 t; see Ap. A) | 0.018 | 0.258 | 0.533 | 0.024 | 0.327 | 3.054\ntCO e savings by Meena using Evolved Transformer\n2 | -- | -- | 48.5 | -- | -- | --\n% 24/x7 carbon free energy (when run) | 31% | 19% | 30% | 73% | 43% | N/A",
    "page": 6
  },
  {
    "type": "table",
    "content": "N/A",
    "page": 6
  },
  {
    "type": "text",
    "content": "● GPT-3 is an autoregressive language model with 175B parameters, 10x more than any non-sparse\nlanguage model at the time [Bro20]. It achieves strong performance on many NLP datasets. A winner of\nthe best paper award at NeurIPS 2020, this 8-month-old paper already has ~700 citations and m ade\nmainstream media headlines. 1 3 It is now available for commercial use. One potential energy benefit of a\nlarge language model like GPT-3 is that they exhibit f ew-shot generalization, which means that they\ndon’t need to be retrained for every new task like smaller models [Wan20]. Its estimated carbon\nemissions due to training are 552 tCO e and its energy consumption is 1287 MWh.1 4\n2\nTable 4 also lists the neural architecture search for Evolved Transformer, discussed shortly.\nFigure 2.",
    "page": 7
  },
  {
    "type": "text",
    "content": "le 4 also lists the neural architecture search for Evolved Transformer, discussed shortly.\nFigure 2. Total FLOPS versus number of parameters relative to Transformer (Big) in a log-log graph\n(Table 1). While all are not doing the same tasks, a reason T5 has relatively lower FLOPS relative to its\nnumber of parameters is that it trains until the accuracy is good enough instead of to the best possible\naccuracy. [Kap20] notes that some architectures have a much lower footprint than others at equivalent\naccuracy and suggests that significant power might be saved by revisiting accuracy requirements.\nFigure 3. Accelerator years of computation, energy consumption, and CO e for five large NLP DNNs.\n2\n13 Metz, C., Meet GPT-3.",
    "page": 7
  },
  {
    "type": "text",
    "content": "ars of computation, energy consumption, and CO e for five large NLP DNNs.\n2\n13 Metz, C., Meet GPT-3. It Has Learned to Code (and Blog and Argue), November 24, 2020, N ew York Times.\n14 We measured all the data for Google models. OpenAI measured V100 performance, V100 power, total FLOPS, and\nPUE for GPT-3. We used the US average CO e/KWh for GPT-3 at Microsoft Azure (see Appendix A).\n2\n7",
    "page": 7
  },
  {
    "type": "table",
    "content": "TABLE (Page 7):\n\n● GPT-3 is an autoregressive language model with 175B parameters, 10x more than any non-sparse\nlanguage model at the time [Bro20]. It achieves strong performance on many NLP datasets. A winner of\nthe best paper award at NeurIPS 2020, this 8-month-old paper already has ~700 citations and m ade\nmainstream media headlines. 1 3 It is now available for commercial use. One potential energy benefit of a\nlarge language model like GPT-3 is that they exhibit f ew-shot generalization, which means that they\ndon’t need to be retrained for every new task like smaller models [Wan20]. Its estimated carbon\nemissions due to training are 552 tCO e and its energy consumption is 1287 MWh.1 4\n2\nTable 4 also lists the neural architecture search for Evolved Transformer, discussed shortly.",
    "page": 7
  },
  {
    "type": "table",
    "content": "1 4\n2\nTable 4 also lists the neural architecture search for Evolved Transformer, discussed shortly.\nFigure 2. Total FLOPS versus number of parameters relative to Transformer (Big) in a log-log graph\n(Table 1). While all are not doing the same tasks, a reason T5 has relatively lower FLOPS relative to its\nnumber of parameters is that it trains until the accuracy is good enough instead of to the best possible\naccuracy. [Kap20] notes that some architectures have a much lower footprint than others at equivalent\naccuracy and suggests that significant power might be saved by revisiting accuracy requirements.\nFigure 3. Accelerator years of computation, energy consumption, and CO e for five large NLP DNNs.\n2\n13 Metz, C., Meet GPT-3.",
    "page": 7
  },
  {
    "type": "table",
    "content": "ars of computation, energy consumption, and CO e for five large NLP DNNs.\n2\n13 Metz, C., Meet GPT-3. It Has Learned to Code (and Blog and Argue), November 24, 2020, N ew York Times.\n14 We measured all the data for Google models. OpenAI measured V100 performance, V100 power, total FLOPS, and\nPUE for GPT-3. We used the US average CO e/KWh for GPT-3 at Microsoft Azure (see Appendix A).\n2\n7",
    "page": 7
  },
  {
    "type": "text",
    "content": "Figures 2 and 3 present the same data graphically. Figure 2 plots the number of parameters on the X axis\nand number of total FLOPS on the Y axis relative to Transformer (Big) [So19] using a log-log graph. Sparsely\nactivated models use many more parameters with much lower total FLOPS. Since performance is not\nnecessarily linear in FLOPS (see [Li21]), Figure 3 shows computation in processor years along with their\nenergy consumption and carbon footprint. Compared to the dense GPT-3, sparsely activated Gshard needs\n~45X fewer processor years, uses ~55X less energy, and reduces gross CO e ~115X and net CO e ~130X.\n2 2\n4. Discussion\nIn this section, we address the additional factors relating to carbon emissions due to training NLP models. We",
    "page": 8
  },
  {
    "type": "text",
    "content": "tion, we address the additional factors relating to carbon emissions due to training NLP models. We\nstart by revisiting the estimate of neural architecture search in [Str19] and end with example benefits of some\nNLP models.\n4.1 Estimating the cost of neural architecture search (NAS)\nThe Evolved Transformer neural architecture search (NAS) was used as an example of an expensive NLP\nmodel [Str19]. Although it is now surpassed by other models in terms of training cost (Table 4), we discuss it\nhere as a concrete example of the complexity of estimating the cost of a ML method retroactively.\nAs Table 4 shows, the actual cost of Evolved Transformer NAS is nearly two orders of magnitude smaller\nthan previously estimated [Str19].",
    "page": 8
  },
  {
    "type": "text",
    "content": "Evolved Transformer NAS is nearly two orders of magnitude smaller\nthan previously estimated [Str19]. Why the discrepancy? The answer is that, in addition to the efficiency of\nGoogle datacenters, there was a confusion in estimating the energy cost of NAS. In Evolved Transformer NAS,\nresearchers used a small p roxy task to search for the best models to save time and money, and then scaled up\nthe found models to full size. Small proxies may not be obvious, which made it hard to estimate the CO e\n2\ncorrectly in retrospect from the NAS paper [So19]. Due to the misunderstanding of the usage of proxy tasks in\nNAS, i t was a ssumed the search was done with full size tasks. Because of this assumption, despite\nconsiderable effort on their part, Strubell e t al.",
    "page": 8
  },
  {
    "type": "text",
    "content": "size tasks. Because of this assumption, despite\nconsiderable effort on their part, Strubell e t al.’ s energy estimate for NAS ended up 18.7X too high for the\naverage organization (see Appendix C) and 88X off in emissions for energy-efficient organizations like Google\n(see Appendix D). This example led us to our first recommendation—that more researchers measure energy\nusage and CO e for computationally intensive projects, and report them when practical, rather than counting\n2\non others to estimate it retrospectively.\nAnother confusion in the general public is the misperception that NAS (and therefore, the cost associated\nwith NAS) is conducted once per model training. In practice, however, N AS is generally not performed once",
    "page": 8
  },
  {
    "type": "text",
    "content": "S) is conducted once per model training. In practice, however, N AS is generally not performed once\nper model training, but once per p roblem d omain+architectural search space combination. For example, the\nEvolved Transformer, found by NAS on translation, can be used for language modeling without a new search\n[So19, Adi20]. Unfortunately, results in the earlier work by [Str19] characterizing NAS were misattributed to\nsingle model training costs in the popular press.\nAs an analogy, NAS is like optimizing the energy efficiency and cost of an LED light bulb with extensive\nsimulations on a supercomputer, training a model is akin to building LED light bulbs, and inference is\nanalogous to all the customers using LEDs to light their homes. The analogous confusion would be claiming",
    "page": 8
  },
  {
    "type": "text",
    "content": "ous to all the customers using LEDs to light their homes. The analogous confusion would be claiming\nthat the one-time upfront supercomputer simulation cost should be included in the CO e cost of every light bulb\n2\nmanufactured. In this analogy, the onetime CO expenditure of the supercomputer simulations can be more\n2\nthan paid back with the improved energy-efficiency of the mass-produced light bulbs, as was the case for the\nactual NAS of [So19] (see next paragraph).\nIn terms of cost-benefit tradeoff, NAS can also lead to improved energy efficiency in training of downstream\napplications, and the benefit can dramatically outweigh the cost. F igure 4 shows that the Evolved Transformer,\nfound by NAS [So19], has 37% fewer parameters and converges to the same accuracy with 25% less energy",
    "page": 8
  },
  {
    "type": "text",
    "content": "und by NAS [So19], has 37% fewer parameters and converges to the same accuracy with 25% less energy\nexpenditure (see Table 1) than the vanilla Transformer (Big) model on WMT English to German translation.\nThe use of Evolved Transformer instead of a regular Transformer architecture saved 48.5 tC O e during the\n2\ntraining of the Meena DNN (see Tables 1 and 4). The savings from this single reuse in Meena are ~15X larger\nthan the energy cost of running the search to discover it. The results of the Evolved Transformer neural\n8",
    "page": 8
  },
  {
    "type": "table",
    "content": "TABLE (Page 8):\n\nFigures 2 and 3 present the same data graphically. Figure 2 plots the number of parameters on the X axis\nand number of total FLOPS on the Y axis relative to Transformer (Big) [So19] using a log-log graph. Sparsely\nactivated models use many more parameters with much lower total FLOPS. Since performance is not\nnecessarily linear in FLOPS (see [Li21]), Figure 3 shows computation in processor years along with their\nenergy consumption and carbon footprint. Compared to the dense GPT-3, sparsely activated Gshard needs\n~45X fewer processor years, uses ~55X less energy, and reduces gross CO e ~115X and net CO e ~130X.\n2 2\n4. Discussion\nIn this section, we address the additional factors relating to carbon emissions due to training NLP models. We",
    "page": 8
  },
  {
    "type": "table",
    "content": "tion, we address the additional factors relating to carbon emissions due to training NLP models. We\nstart by revisiting the estimate of neural architecture search in [Str19] and end with example benefits of some\nNLP models.\n4.1 Estimating the cost of neural architecture search (NAS)\nThe Evolved Transformer neural architecture search (NAS) was used as an example of an expensive NLP\nmodel [Str19]. Although it is now surpassed by other models in terms of training cost (Table 4), we discuss it\nhere as a concrete example of the complexity of estimating the cost of a ML method retroactively.\nAs Table 4 shows, the actual cost of Evolved Transformer NAS is nearly two orders of magnitude smaller\nthan previously estimated [Str19].",
    "page": 8
  },
  {
    "type": "table",
    "content": "Evolved Transformer NAS is nearly two orders of magnitude smaller\nthan previously estimated [Str19]. Why the discrepancy? The answer is that, in addition to the efficiency of\nGoogle datacenters, there was a confusion in estimating the energy cost of NAS. In Evolved Transformer NAS,\nresearchers used a small p roxy task to search for the best models to save time and money, and then scaled up\nthe found models to full size. Small proxies may not be obvious, which made it hard to estimate the CO e\n2\ncorrectly in retrospect from the NAS paper [So19]. Due to the misunderstanding of the usage of proxy tasks in\nNAS, i t was a ssumed the search was done with full size tasks. Because of this assumption, despite\nconsiderable effort on their part, Strubell e t al.",
    "page": 8
  },
  {
    "type": "table",
    "content": "size tasks. Because of this assumption, despite\nconsiderable effort on their part, Strubell e t al.’ s energy estimate for NAS ended up 18.7X too high for the\naverage organization (see Appendix C) and 88X off in emissions for energy-efficient organizations like Google\n(see Appendix D). This example led us to our first recommendation—that more researchers measure energy\nusage and CO e for computationally intensive projects, and report them when practical, rather than counting\n2\non others to estimate it retrospectively.\nAnother confusion in the general public is the misperception that NAS (and therefore, the cost associated\nwith NAS) is conducted once per model training. In practice, however, N AS is generally not performed once",
    "page": 8
  },
  {
    "type": "table",
    "content": "S) is conducted once per model training. In practice, however, N AS is generally not performed once\nper model training, but once per p roblem d omain+architectural search space combination. For example, the\nEvolved Transformer, found by NAS on translation, can be used for language modeling without a new search\n[So19, Adi20]. Unfortunately, results in the earlier work by [Str19] characterizing NAS were misattributed to\nsingle model training costs in the popular press.\nAs an analogy, NAS is like optimizing the energy efficiency and cost of an LED light bulb with extensive\nsimulations on a supercomputer, training a model is akin to building LED light bulbs, and inference is\nanalogous to all the customers using LEDs to light their homes. The analogous confusion would be claiming",
    "page": 8
  },
  {
    "type": "table",
    "content": "ous to all the customers using LEDs to light their homes. The analogous confusion would be claiming\nthat the one-time upfront supercomputer simulation cost should be included in the CO e cost of every light bulb\n2\nmanufactured. In this analogy, the onetime CO expenditure of the supercomputer simulations can be more\n2\nthan paid back with the improved energy-efficiency of the mass-produced light bulbs, as was the case for the\nactual NAS of [So19] (see next paragraph).\nIn terms of cost-benefit tradeoff, NAS can also lead to improved energy efficiency in training of downstream\napplications, and the benefit can dramatically outweigh the cost. F igure 4 shows that the Evolved Transformer,\nfound by NAS [So19], has 37% fewer parameters and converges to the same accuracy with 25% less energy",
    "page": 8
  },
  {
    "type": "table",
    "content": "und by NAS [So19], has 37% fewer parameters and converges to the same accuracy with 25% less energy\nexpenditure (see Table 1) than the vanilla Transformer (Big) model on WMT English to German translation.\nThe use of Evolved Transformer instead of a regular Transformer architecture saved 48.5 tC O e during the\n2\ntraining of the Meena DNN (see Tables 1 and 4). The savings from this single reuse in Meena are ~15X larger\nthan the energy cost of running the search to discover it. The results of the Evolved Transformer neural\n8",
    "page": 8
  },
  {
    "type": "table",
    "content": "TABLE (Page 8):\n | N AS is generally not performed once\nper model training, but once per p roblem d omain+architectural search space combination. For example, the | ",
    "page": 8
  },
  {
    "type": "table",
    "content": "TABLE (Page 8):\n | F igure 4 shows that the Evolved Transformer\nfound by NAS [So19], has 37% fewer parameters and converges to the same accuracy with 25% less energy | \nexpenditure (see Table 1) than the vanilla Transformer (Big) model on WMT English to German translation. | \nThe use of Evolved Transformer instead of a regular Transformer architecture saved 48.5 t | \ntraining of the Meena DNN (see Tables 1 and 4). The savings from this single reuse in Meena are ~15X larger | \nthan the energy cost of running the search to discover it. The results of the Evolved Transformer neural | ",
    "page": 8
  },
  {
    "type": "text",
    "content": "architecture search have been open-sourced. It can readily be used by anyone training ML models for NLP\nproblems, similar to how a Transformer-style model can be used for NLP problems [Evo19].1 5\nIt would be beneficial to compare the cost-savings ratio of the Evolved Transformer NAS to previous work\ndeveloping more efficient architectures. Unfortunately, as others have pointed out [Dod19, Str19], the full cost\nof model development is rarely, if ever, reported in the literature, making it impossible to compare this analysis\nto prior work, and preventing straightforward comparison among different approaches more generally.\nThis lack of training development costs is one example of how adopting higher standards for measuring",
    "page": 9
  },
  {
    "type": "text",
    "content": "is lack of training development costs is one example of how adopting higher standards for measuring\nand reporting ML model energy requirements would lead to a better understanding of cost-accuracy tradeoffs\nin ML models, potentially further reducing overall emissions by empowering more informed ML model\nselection, as the next subsection explains.\nFigure 4: Reproduction of Figure 4 from So e t al. Dots on the blue line represent various sizes of plain\nTransformer NLP models, while dots on the red line represent various sizes of the open-sourced\nEvolved Transformer architecture that was discovered by the neural architecture search run in [So19].\nRed arrows are at 131M and 210M parameters and show that an Evolved Transformer can achieve\nhigher accuracy at less cost: it runs 1.",
    "page": 9
  },
  {
    "type": "text",
    "content": "parameters and show that an Evolved Transformer can achieve\nhigher accuracy at less cost: it runs 1.3X faster and produces 1.3x less CO e.\n2\n4.2 There are more resources used for training than the only final training run\n[Str19] and others point out that it often takes many attempts to get everything set up correctly before the\nfinal training run, so the final training run does not reflect the total cost. Since it’s hard to improve what you\ncan’t measure, one issue is how to account for such costs accurately. Fortunately, an internal Google product\nis underway that will record information about the training process, originally intended to keep track of\ninformation like data provenance. The developers now plan to add energy consumption so that Googlers can",
    "page": 9
  },
  {
    "type": "text",
    "content": "mation like data provenance. The developers now plan to add energy consumption so that Googlers can\nbetter understand the full training lifecycle. A n example of an open source tool to record such information is\nexperiment-impact-tracker [Hen20]. I n addition, the developers of ML Emissions Calculator [Lac19] are\ncurrently working on C odeCarbon, whose goal is to measure/approximate carbon consumption automatically.\nAlas, there will be no way to verify the claims in papers of preliminary training development. A lesson of\ncomputer benchmarking is that requiring the release of all information so that others could recreate your results\nwas an effective deterrent to fudging the numbers. If more computationally intensive ML papers included",
    "page": 9
  },
  {
    "type": "text",
    "content": "an effective deterrent to fudging the numbers. If more computationally intensive ML papers included\nenergy consumption and carbon footprint of the final training run with sufficient details that others could check,\n15 Reuse reduces overall development effort and energy usage. For example, implementations of EfficientNets, Efficient-\nDets [Tan19], developed via NAS for image-classification and object-detection, were forked on GitHub >4000 times.\n9",
    "page": 9
  },
  {
    "type": "table",
    "content": "TABLE (Page 9):\n\narchitecture search have been open-sourced. It can readily be used by anyone training ML models for NLP\nproblems, similar to how a Transformer-style model can be used for NLP problems [Evo19].1 5\nIt would be beneficial to compare the cost-savings ratio of the Evolved Transformer NAS to previous work\ndeveloping more efficient architectures. Unfortunately, as others have pointed out [Dod19, Str19], the full cost\nof model development is rarely, if ever, reported in the literature, making it impossible to compare this analysis\nto prior work, and preventing straightforward comparison among different approaches more generally.\nThis lack of training development costs is one example of how adopting higher standards for measuring",
    "page": 9
  },
  {
    "type": "table",
    "content": "is lack of training development costs is one example of how adopting higher standards for measuring\nand reporting ML model energy requirements would lead to a better understanding of cost-accuracy tradeoffs\nin ML models, potentially further reducing overall emissions by empowering more informed ML model\nselection, as the next subsection explains.\nFigure 4: Reproduction of Figure 4 from So e t al. Dots on the blue line represent various sizes of plain\nTransformer NLP models, while dots on the red line represent various sizes of the open-sourced\nEvolved Transformer architecture that was discovered by the neural architecture search run in [So19].\nRed arrows are at 131M and 210M parameters and show that an Evolved Transformer can achieve\nhigher accuracy at less cost: it runs 1.",
    "page": 9
  },
  {
    "type": "table",
    "content": "parameters and show that an Evolved Transformer can achieve\nhigher accuracy at less cost: it runs 1.3X faster and produces 1.3x less CO e.\n2\n4.2 There are more resources used for training than the only final training run\n[Str19] and others point out that it often takes many attempts to get everything set up correctly before the\nfinal training run, so the final training run does not reflect the total cost. Since it’s hard to improve what you\ncan’t measure, one issue is how to account for such costs accurately. Fortunately, an internal Google product\nis underway that will record information about the training process, originally intended to keep track of\ninformation like data provenance. The developers now plan to add energy consumption so that Googlers can",
    "page": 9
  },
  {
    "type": "table",
    "content": "mation like data provenance. The developers now plan to add energy consumption so that Googlers can\nbetter understand the full training lifecycle. A n example of an open source tool to record such information is\nexperiment-impact-tracker [Hen20]. I n addition, the developers of ML Emissions Calculator [Lac19] are\ncurrently working on C odeCarbon, whose goal is to measure/approximate carbon consumption automatically.\nAlas, there will be no way to verify the claims in papers of preliminary training development. A lesson of\ncomputer benchmarking is that requiring the release of all information so that others could recreate your results\nwas an effective deterrent to fudging the numbers. If more computationally intensive ML papers included",
    "page": 9
  },
  {
    "type": "table",
    "content": "an effective deterrent to fudging the numbers. If more computationally intensive ML papers included\nenergy consumption and carbon footprint of the final training run with sufficient details that others could check,\n15 Reuse reduces overall development effort and energy usage. For example, implementations of EfficientNets, Efficient-\nDets [Tan19], developed via NAS for image-classification and object-detection, were forked on GitHub >4000 times.\n9",
    "page": 9
  },
  {
    "type": "table",
    "content": "TABLE (Page 9):\narchitecture search have been open-sourced. It can readily be used by anyone training ML models for NLP\nproblems, similar to how a Transformer-style model can be used for NLP problems [Evo19].",
    "page": 9
  },
  {
    "type": "text",
    "content": "that would be a great step forward. Perhaps ML practitioners could study the total lifecycle to develop rules of\nthumb to estimate the overall carbon footprint based on its final training cost.1 6\nThe next subsection also emphasizes the value of measurement.\nFigure 5. Measured vs peak performance, measured system power vs peak chip power (TDP), and\nmeasured vs peak performance/Watt for V100 GPU and TPU v3 (see Table 4 and Appendix A).\n4.3 Measurements are more interesting than extrapolations\nAlthough extrapolations of carbon emissions are relatively easy, more attention should be paid to actual\nexperiments that have been conducted rather than to hypothetical case studies. As a problematic example,",
    "page": 10
  },
  {
    "type": "text",
    "content": "iments that have been conducted rather than to hypothetical case studies. As a problematic example,\n16 Since large NLP models can take a month to train, developers cannot afford to do the full training task many times. Like\n[So19] for NAS, they likely use a smaller task to explore the space for a limited training time. One indication comes from\nthe AutoML work in [Li21]. Their exploration computation cost was roughly equal to the final training cost.\n10",
    "page": 10
  },
  {
    "type": "table",
    "content": "TABLE (Page 10):\n\nthat would be a great step forward. Perhaps ML practitioners could study the total lifecycle to develop rules of\nthumb to estimate the overall carbon footprint based on its final training cost.1 6\nThe next subsection also emphasizes the value of measurement.\nFigure 5. Measured vs peak performance, measured system power vs peak chip power (TDP), and\nmeasured vs peak performance/Watt for V100 GPU and TPU v3 (see Table 4 and Appendix A).\n4.3 Measurements are more interesting than extrapolations\nAlthough extrapolations of carbon emissions are relatively easy, more attention should be paid to actual\nexperiments that have been conducted rather than to hypothetical case studies. As a problematic example,\n16 Since large NLP models can take a month to train, developers cannot afford to do the full training task many times. Like\n[So19] for NAS, they likely use a smaller task to explore the space for a limited training time. One indication comes from\nthe AutoML work in [Li21]. Their exploration computation cost was roughly equal to the final training cost.\n10\n\n",
    "page": 10
  },
  {
    "type": "text",
    "content": "let’s hypothesize what the CO e would be for training Transformer (Big) on the C TS-1 Quartz - Tundra Extreme\n2\nScale supercomputer at L awrence Livermore National Laboratory, o ne of the t op 500 supercomputers (but one\nwhose design is not optimized for ML training). Its ~100,000 cores might use ~75 MWh of power and m ight\ngenerate 32 tCO e, ~10,000 times larger than for TPU v2s at Google (Table 1)1 7.\n2\nThe measurement advice applies to processors as well DNNs. Tables 1 and 2 show that the theoretical\nperformance per Watt is higher than the measured performance per Watt on average by factors of 1.6X for\nTPUs and by 3.5X for GPUs. Figure 5 shows the information in Table 1 graphically. Using theoretical\nperformance per Watt, V100 is 1.",
    "page": 11
  },
  {
    "type": "text",
    "content": "e 5 shows the information in Table 1 graphically. Using theoretical\nperformance per Watt, V100 is 1.5X better than TPU v3, but it's the other way around for measured\nperformance per Watt: TPU v3 is 2.0X better than V100 on average for these large NLP DNNs.\nFigure 6 compares the gross CO e estimates from the ML Emissions [Lac19] and Green Algorithms\n2\n[Lan20] calculators to the processors and programs in this paper at the time of this writing (April 2021).\nCompared to the results in Tables 1 and 4, they differ by factors of 0.53–1.64 and 0.91–2.42 with geometric\nmeans of 0.92 and 1.48, respectively1 8. T he ML Emissions and Green Algorithms calculators do not\nestimate net CO e, which could be up to 10X lower. The figure once again shows the increase in accuracy\n2",
    "page": 11
  },
  {
    "type": "text",
    "content": "te net CO e, which could be up to 10X lower. The figure once again shows the increase in accuracy\n2\nof measurement over indirect calculations. The authors of the Emissions Calculator agree that measurement is\npreferred, with some calculator as the best alternative if measurement is difficult to perform [Luc21].\nThe next discussion topic reminds us that improving the algorithm is often more important than improving\nthe hardware.\nFigure 6. Ratio of ML Emissions and Green Algorithm calculators vs gross CO e in Tables 1 and 4.\n2\n4.4 Standard ML algorithmic techniques can improve energy efficiency\nThere are many algorithmic techniques that can improve the energy efficiency of machine learning models.\nSome techniques can achieve the same accuracy with less overall computation.",
    "page": 11
  },
  {
    "type": "text",
    "content": "achine learning models.\nSome techniques can achieve the same accuracy with less overall computation. Others can use a large,\nalready-trained model as a starting point and yield a lighter-weight, more computationally efficient model with\nalmost the same accuracy. These techniques all serve to reduce the computational cost and therefore energy\nand carbon emissions of models. Some of these techniques include:\n● Distillation transfers the knowledge from large models into smaller, more computationally efficient\nmodels [Hin15, San20].\n● Pruning, q uantization, and e fficient coding can improve the energy efficiency of DNNs 3X–7X [Han15].\n17 We use US averages for kg CO e/KWh and datacenter PUE and assume it runs at 40% of the peak floating point\n2\nperformance of Quartz-Tundra (3.",
    "page": 11
  },
  {
    "type": "text",
    "content": "tacenter PUE and assume it runs at 40% of the peak floating point\n2\nperformance of Quartz-Tundra (3.2 PetaFLOPS/sec). For reference, Figure 5 shows V100 running at 20% of peak.\n18 We picked the closest geographic option per calculator to the actual location in each case. The Green Algorithms paper\nlists Meena CO e as 164t [Lan20], but the calculator result as of April 2020 was 85t for Virgina using Google Cloud.\n2\n11",
    "page": 11
  },
  {
    "type": "table",
    "content": "TABLE (Page 11):\n\nlet’s hypothesize what the CO e would be for training Transformer (Big) on the C TS-1 Quartz - Tundra Extreme\n2\nScale supercomputer at L awrence Livermore National Laboratory, o ne of the t op 500 supercomputers (but one\nwhose design is not optimized for ML training). Its ~100,000 cores might use ~75 MWh of power and m ight\ngenerate 32 tCO e, ~10,000 times larger than for TPU v2s at Google (Table 1)1 7.\n2\nThe measurement advice applies to processors as well DNNs. Tables 1 and 2 show that the theoretical\nperformance per Watt is higher than the measured performance per Watt on average by factors of 1.6X for\nTPUs and by 3.5X for GPUs. Figure 5 shows the information in Table 1 graphically. Using theoretical\nperformance per Watt, V100 is 1.",
    "page": 11
  },
  {
    "type": "table",
    "content": "e 5 shows the information in Table 1 graphically. Using theoretical\nperformance per Watt, V100 is 1.5X better than TPU v3, but it's the other way around for measured\nperformance per Watt: TPU v3 is 2.0X better than V100 on average for these large NLP DNNs.\nFigure 6 compares the gross CO e estimates from the ML Emissions [Lac19] and Green Algorithms\n2\n[Lan20] calculators to the processors and programs in this paper at the time of this writing (April 2021).\nCompared to the results in Tables 1 and 4, they differ by factors of 0.53–1.64 and 0.91–2.42 with geometric\nmeans of 0.92 and 1.48, respectively1 8. T he ML Emissions and Green Algorithms calculators do not\nestimate net CO e, which could be up to 10X lower. The figure once again shows the increase in accuracy\n2",
    "page": 11
  },
  {
    "type": "table",
    "content": "te net CO e, which could be up to 10X lower. The figure once again shows the increase in accuracy\n2\nof measurement over indirect calculations. The authors of the Emissions Calculator agree that measurement is\npreferred, with some calculator as the best alternative if measurement is difficult to perform [Luc21].\nThe next discussion topic reminds us that improving the algorithm is often more important than improving\nthe hardware.\nFigure 6. Ratio of ML Emissions and Green Algorithm calculators vs gross CO e in Tables 1 and 4.\n2\n4.4 Standard ML algorithmic techniques can improve energy efficiency\nThere are many algorithmic techniques that can improve the energy efficiency of machine learning models.\nSome techniques can achieve the same accuracy with less overall computation.",
    "page": 11
  },
  {
    "type": "table",
    "content": "achine learning models.\nSome techniques can achieve the same accuracy with less overall computation. Others can use a large,\nalready-trained model as a starting point and yield a lighter-weight, more computationally efficient model with\nalmost the same accuracy. These techniques all serve to reduce the computational cost and therefore energy\nand carbon emissions of models. Some of these techniques include:\n● Distillation transfers the knowledge from large models into smaller, more computationally efficient\nmodels [Hin15, San20].\n● Pruning, q uantization, and e fficient coding can improve the energy efficiency of DNNs 3X–7X [Han15].\n17 We use US averages for kg CO e/KWh and datacenter PUE and assume it runs at 40% of the peak floating point\n2\nperformance of Quartz-Tundra (3.",
    "page": 11
  },
  {
    "type": "table",
    "content": "tacenter PUE and assume it runs at 40% of the peak floating point\n2\nperformance of Quartz-Tundra (3.2 PetaFLOPS/sec). For reference, Figure 5 shows V100 running at 20% of peak.\n18 We picked the closest geographic option per calculator to the actual location in each case. The Green Algorithms paper\nlists Meena CO e as 164t [Lan20], but the calculator result as of April 2020 was 85t for Virgina using Google Cloud.\n2\n11",
    "page": 11
  },
  {
    "type": "table",
    "content": "TABLE (Page 11):\nalmost the same accuracy. These techniques all serve to reduce the computational cost and therefore energy\nand carbon emissions of models. Some of these techniques include:",
    "page": 11
  },
  {
    "type": "text",
    "content": "● Fine-tuning and t ransfer learning both reuse already-trained representations, rather than starting\ntraining of each NLP task’s parameters from random initialization, for example [Dod20].\n● Sparsely activated mixture-of-expert-style models can provide more than 10X reductions in\ncomputation requirements and energy costs for both training and inference while providing significantly\nhigher accuracy than dense Transformer or LSTM-based models of equivalent computational cost per\ntoken [Sha17,Lep20,Fed21]. Gshard-600B is one example, evaluated in Section 3.\nWe commend the development of such techniques. Some publication venues, such as the E ACL and N AACL\n2021 NLP conferences, have begun specifically soliciting research of this nature by offering “Efficient and",
    "page": 12
  },
  {
    "type": "text",
    "content": "conferences, have begun specifically soliciting research of this nature by offering “Efficient and\nGreen” research tracks, alongside workshops such as S ustaiNLP and E fficientQA. We encourage other\nvenues to follow suit, and hope that many researchers will consider this line of work.\nThe next topic discusses one of our biggest surprises of this investigation, the importance of geography.\n4.5 It matters which datacenter is used, even within the same organization\nWe were amazed by how much it matters w here and w hen a DNN is trained. Moreover, this option is likely\nthe easiest path for ML practitioners to reduce CO e. For example, after reading early drafts of this paper,\n2\nsome colleagues switched to a Google datacenter with a smaller carbon footprint to train a large NLP model.",
    "page": 12
  },
  {
    "type": "text",
    "content": "leagues switched to a Google datacenter with a smaller carbon footprint to train a large NLP model.\nReviewers of early drafts suggested that datacenter energy use is a zero-sum game. They thought that any\ntasks run in a green datacenter simply shift other work to dirtier datacenters, so there is no net gain. It’s not\ntrue, but that speculation reveals many seemingly plausible but incorrect fallacies:\n● Fallacy: Datacenters are fully utilized. Applications are deployed to handle worst case demand\ndepending on the time of day and day of the week, so for much of the time resources are idle [Arm10].\n● Fallacy: Cloud centers can’t grow. Similar to the founding of a new university, cloud companies buy",
    "page": 12
  },
  {
    "type": "text",
    "content": "Fallacy: Cloud centers can’t grow. Similar to the founding of a new university, cloud companies buy\nmuch more land than they need initially at a site so that they can construct more buildings in the future\nwithout first traversing the lengthy process of acquiring land [Bar18].\n● Fallacy: Renewable energy is fixed and can’t grow. There is often an excess of renewable energy at\nsome times of day (see Appendix B). The amount of solar and wind energy is also a function of the\ninvestment as well as weather conditions. Google’s long term renewable energy procurement normally\ninvests in the creation of new renewable energy resources. The greater the use and investment in\nrenewable energy, the more money is available to buy and deploy new solar panels and wind turbines,",
    "page": 12
  },
  {
    "type": "text",
    "content": "renewable energy, the more money is available to buy and deploy new solar panels and wind turbines,\nthereby increasing the renewable energy supply. Thus, it’s n ot the case that Google’s use of renewable\nenergy means other residents must use dirty energy. Appendix B introduces issues around carbon free\nenergy use and investment.\n● Fallacy: Google NLP model training competes with other tasks in the datacenter. Google trains large\nmodels on ML supercomputers that even have their own interconnection network, so ML training is\ndistinct from CPU-only tasks [Jou20]. Tasks for CPUs don’t interfere with TPUs, and vice versa.\n● Fallacy: Training must run in all datacenters. While user facing inference applications need global",
    "page": 12
  },
  {
    "type": "text",
    "content": "Fallacy: Training must run in all datacenters. While user facing inference applications need global\ndistribution in order to provide low-latency access to users all around the world [Jou21], there is no\nproblem to limit ML training computation to a smaller number of (green) datacenters. For example,\nGoogle is currently deploying numerous TPU v4s, many of which will be located in windy Oklahoma,\nwhose net CO e/KWh is even lower than Iowa.\n2\n● Fallacy: There is no business reason to reduce carbon emissions. Reducing climate change certainly\nhas long-term economic benefits for everyone. Google has been carbon neutral since 2007 and has\nprocured enough additional renewable energy to match 100% of its datacenter energy usage since",
    "page": 12
  },
  {
    "type": "text",
    "content": "has\nprocured enough additional renewable energy to match 100% of its datacenter energy usage since\n2017, so the impact of the remaining carbon from training at Google is zero even today. Other\nhyperscalers aim for carbon neutrality by 2025 or 2030, so the whole cloud may become carbon\nneutral. With its new 24/7 local carbon-free energy goal by 2030, Google is now focused on purchasing\ncarbon-free energy to match its hourly load at the same location as its datacenters with the goal to\ndecarbonize its electricity supply (see Appendix B).\nThe next question that arose is whether such green datacenters are available to only a few ML practitioners.\n12",
    "page": 12
  },
  {
    "type": "table",
    "content": "TABLE (Page 12):\n\n● Fine-tuning and t ransfer learning both reuse already-trained representations, rather than starting\ntraining of each NLP task’s parameters from random initialization, for example [Dod20].\n● Sparsely activated mixture-of-expert-style models can provide more than 10X reductions in\ncomputation requirements and energy costs for both training and inference while providing significantly\nhigher accuracy than dense Transformer or LSTM-based models of equivalent computational cost per\ntoken [Sha17,Lep20,Fed21]. Gshard-600B is one example, evaluated in Section 3.\nWe commend the development of such techniques. Some publication venues, such as the E ACL and N AACL\n2021 NLP conferences, have begun specifically soliciting research of this nature by offering “Efficient and",
    "page": 12
  },
  {
    "type": "table",
    "content": "conferences, have begun specifically soliciting research of this nature by offering “Efficient and\nGreen” research tracks, alongside workshops such as S ustaiNLP and E fficientQA. We encourage other\nvenues to follow suit, and hope that many researchers will consider this line of work.\nThe next topic discusses one of our biggest surprises of this investigation, the importance of geography.\n4.5 It matters which datacenter is used, even within the same organization\nWe were amazed by how much it matters w here and w hen a DNN is trained. Moreover, this option is likely\nthe easiest path for ML practitioners to reduce CO e. For example, after reading early drafts of this paper,\n2\nsome colleagues switched to a Google datacenter with a smaller carbon footprint to train a large NLP model.",
    "page": 12
  },
  {
    "type": "table",
    "content": "leagues switched to a Google datacenter with a smaller carbon footprint to train a large NLP model.\nReviewers of early drafts suggested that datacenter energy use is a zero-sum game. They thought that any\ntasks run in a green datacenter simply shift other work to dirtier datacenters, so there is no net gain. It’s not\ntrue, but that speculation reveals many seemingly plausible but incorrect fallacies:\n● Fallacy: Datacenters are fully utilized. Applications are deployed to handle worst case demand\ndepending on the time of day and day of the week, so for much of the time resources are idle [Arm10].\n● Fallacy: Cloud centers can’t grow. Similar to the founding of a new university, cloud companies buy",
    "page": 12
  },
  {
    "type": "table",
    "content": "Fallacy: Cloud centers can’t grow. Similar to the founding of a new university, cloud companies buy\nmuch more land than they need initially at a site so that they can construct more buildings in the future\nwithout first traversing the lengthy process of acquiring land [Bar18].\n● Fallacy: Renewable energy is fixed and can’t grow. There is often an excess of renewable energy at\nsome times of day (see Appendix B). The amount of solar and wind energy is also a function of the\ninvestment as well as weather conditions. Google’s long term renewable energy procurement normally\ninvests in the creation of new renewable energy resources. The greater the use and investment in\nrenewable energy, the more money is available to buy and deploy new solar panels and wind turbines,",
    "page": 12
  },
  {
    "type": "table",
    "content": "renewable energy, the more money is available to buy and deploy new solar panels and wind turbines,\nthereby increasing the renewable energy supply. Thus, it’s n ot the case that Google’s use of renewable\nenergy means other residents must use dirty energy. Appendix B introduces issues around carbon free\nenergy use and investment.\n● Fallacy: Google NLP model training competes with other tasks in the datacenter. Google trains large\nmodels on ML supercomputers that even have their own interconnection network, so ML training is\ndistinct from CPU-only tasks [Jou20]. Tasks for CPUs don’t interfere with TPUs, and vice versa.\n● Fallacy: Training must run in all datacenters. While user facing inference applications need global",
    "page": 12
  },
  {
    "type": "table",
    "content": "Fallacy: Training must run in all datacenters. While user facing inference applications need global\ndistribution in order to provide low-latency access to users all around the world [Jou21], there is no\nproblem to limit ML training computation to a smaller number of (green) datacenters. For example,\nGoogle is currently deploying numerous TPU v4s, many of which will be located in windy Oklahoma,\nwhose net CO e/KWh is even lower than Iowa.\n2\n● Fallacy: There is no business reason to reduce carbon emissions. Reducing climate change certainly\nhas long-term economic benefits for everyone. Google has been carbon neutral since 2007 and has\nprocured enough additional renewable energy to match 100% of its datacenter energy usage since",
    "page": 12
  },
  {
    "type": "table",
    "content": "has\nprocured enough additional renewable energy to match 100% of its datacenter energy usage since\n2017, so the impact of the remaining carbon from training at Google is zero even today. Other\nhyperscalers aim for carbon neutrality by 2025 or 2030, so the whole cloud may become carbon\nneutral. With its new 24/7 local carbon-free energy goal by 2030, Google is now focused on purchasing\ncarbon-free energy to match its hourly load at the same location as its datacenters with the goal to\ndecarbonize its electricity supply (see Appendix B).\nThe next question that arose is whether such green datacenters are available to only a few ML practitioners.\n12",
    "page": 12
  },
  {
    "type": "table",
    "content": "TABLE (Page 12):\ncomputation requirements and energy costs for both training and inference while providing significantly\nhigher accuracy than dense Transformer or LSTM-based models of equivalent computational cost per\ntoken [Sha17,Lep20,Fed21]. Gshard-600B is one example, evaluated in Section 3.",
    "page": 12
  },
  {
    "type": "table",
    "content": "TABLE (Page 12):\nWe commend the development of such techniques. Some publication venues, such as the E ACL and N AACL\n2021 NLP conferences, have begun specifically soliciting research of this nature by offering “Efficient and\nGreen” research tracks, alongside workshops such as S ustaiNLP and E fficientQA. We encourage other\nvenues to follow suit, and hope that many researchers will consider this line of work.",
    "page": 12
  },
  {
    "type": "text",
    "content": "4.6 Many have access to energy-optimized datacenters\nThe increasing use of cloud computing has decreased the energy intensity1 9 of datacenters 20% annually\nsince 2010 [Has20]. Access to energy-optimized, low-cost cloud datacenters is not restricted to employees of a\nfew companies; people around the world can rent computers in them using services like Alibaba Cloud,\nAmazon Web Services, Google Cloud Platform, and Microsoft Azure.2 0 Moreover, Alibaba, Amazon, and\nGoogle offer access to their custom processors for DNNs through their cloud service. The popularity of the\npublic cloud is indicated by its annual growth in business by up to 50% since 2010 [Sch21]. Many believe the\ncloud’s efficiencies in cost and energy mean that it is the ultimate future of all datacenters [Arm10, Sch21].",
    "page": 13
  },
  {
    "type": "text",
    "content": "iciencies in cost and energy mean that it is the ultimate future of all datacenters [Arm10, Sch21].\nThe next topic reminds us that reducing cost and energy consumption remains important no matter how\ngreen the cloud becomes.\n4.7 Reducing the cost of training matters too\nThough many have access to these relatively efficient compute resources and c loud companies may\ndramatically reduce their carbon footprint in the future, it’s still important to reduce the economic c ost of\ntraining. Saving money obviously matters to everyone, but ex pensive training of NLP models also makes this\nresearch style unattainable for many researchers2 1 , 22. This inequity of access to state-of-the-art models is",
    "page": 13
  },
  {
    "type": "text",
    "content": "le unattainable for many researchers2 1 , 22. This inequity of access to state-of-the-art models is\nanother strong motivator, alongside environmental concerns, to incentivize the development of energy-efficient\nML models that work as well as their computationally hungrier counterparts.\nOne issue that was difficult for us during our investigation was to put into perspective the 4 to 552 tCO e\n2\nfrom training of these NLP models, which the next subsection explores.\n4.8 How does training a large NLP model compare to other activities?\nGoogle Flights estimate for the emissions of a direct round trip of a whole passenger jet between San\nFrancisco and New York is 180 tCO e (see Table 2 and Appendix A). T5 training emissions are ~26%, Meena\n2",
    "page": 13
  },
  {
    "type": "text",
    "content": "sco and New York is 180 tCO e (see Table 2 and Appendix A). T5 training emissions are ~26%, Meena\n2\nis 53%, Gshard-600B is ~2%, Switch Transformer is 32%, and GPT-3 is ~305% of such a round trip.\nAnother comparison point is to B itcoin. Every purchase that transfers bitcoin currently costs ~700 KWh or\n~0.3 tCO e, equivalent to the CO e produced by ~750,000 credit card swipes. Bitcoin miners use custom chips\n2 2\nthat operate continuously 24/7 until they fail. Estimates of Bitcoin’s impact for 2021 are ~78–121\nTeraWatt-hours and ~37M–58M tCO e [Cri21, Dig21]. Stated alternatively, ~70M people have Bitcoin wallets\n2\nyet Google consumes 1/10th of Bitcoin’s energy to provide services for billions of people, and all of Google’s\nenergy use is offset.",
    "page": 13
  },
  {
    "type": "text",
    "content": "tcoin’s energy to provide services for billions of people, and all of Google’s\nenergy use is offset. If Bitcoin were a country, it would be in the top 30 in CO e; larger than Argentina, whose\n2\npopulation is 45M. The estimated annual carbon footprint of Bitcoin mining this year is equivalent to roughly\n200,000 to 300,000 whole passenger jet SF↔NY round trips.\nIn 2019 t he world saw 39M flights and U S airlines flew 925M passengers, which helps explain why air\ntravel was responsible for 940 MtCO , or ~2.5% of the world's annual CO in 2018 of 33B tCO e [Rit20].\n2 2 2\nFinally, Google publishes its total energy consumption, and for 2019 it was 12.2 TeraWatt-hours [Goo20].\nRow 18 of Table 4 shows the percentage that each NLP model training was of that total. Even if we assume all",
    "page": 13
  },
  {
    "type": "text",
    "content": "Table 4 shows the percentage that each NLP model training was of that total. Even if we assume all\nfour of Google’s large NLP models in Table 4 were trained in 2019, the total represents less than 0.005%. T he\ntraining of those four large NLP models is not a significant fraction of Google’s energy consumption.\n19 Improvement in energy intensity is expressed as energy use per compute instance. [Has20] goes on to say the cloud’s\nincreasing share of datacenters is causing a “notable improvement compared with recent annual efficiency gains in other\nmajor demand sectors (e.g., aviation and industry), which are an order of magnitude lower.”\n20 There are not many cloud companies. With new technologies, initially only a few firms can practice the technology and",
    "page": 13
  },
  {
    "type": "text",
    "content": "cloud companies. With new technologies, initially only a few firms can practice the technology and\nthey sell it to others, but these companies compete. There are many examples. Chemical technologies are in the hands of\na relatively small number of companies; only six or seven institutions worldwide can refine crude oil; just a few firms can\nmanufacture computer chips in the finest technology node (3–5 nm).\n21 To support the goal of making ML more inclusive, G oogle provides free access to a total of ~500 PetaFLOPS/second of\nTPU compute power to help ML researchers around the world participate in advancing the start of the art of ML.\n22 One possible unintended consequence of making training of a model less expensive is that more people will train the",
    "page": 13
  },
  {
    "type": "text",
    "content": "ntended consequence of making training of a model less expensive is that more people will train the\nmodel and increase energy use, but that seems like a better risk than to continue using inefficient models.\n13",
    "page": 13
  },
  {
    "type": "table",
    "content": "TABLE (Page 13):\n\n4.6 Many have access to energy-optimized datacenters\nThe increasing use of cloud computing has decreased the energy intensity1 9 of datacenters 20% annually\nsince 2010 [Has20]. Access to energy-optimized, low-cost cloud datacenters is not restricted to employees of a\nfew companies; people around the world can rent computers in them using services like Alibaba Cloud,\nAmazon Web Services, Google Cloud Platform, and Microsoft Azure.2 0 Moreover, Alibaba, Amazon, and\nGoogle offer access to their custom processors for DNNs through their cloud service. The popularity of the\npublic cloud is indicated by its annual growth in business by up to 50% since 2010 [Sch21]. Many believe the",
    "page": 13
  },
  {
    "type": "table",
    "content": "oud is indicated by its annual growth in business by up to 50% since 2010 [Sch21]. Many believe the\ncloud’s efficiencies in cost and energy mean that it is the ultimate future of all datacenters [Arm10, Sch21].\nThe next topic reminds us that reducing cost and energy consumption remains important no matter how\ngreen the cloud becomes.\n4.7 Reducing the cost of training matters too\nThough many have access to these relatively efficient compute resources and c loud companies may\ndramatically reduce their carbon footprint in the future, it’s still important to reduce the economic c ost of\ntraining. Saving money obviously matters to everyone, but ex pensive training of NLP models also makes this\nresearch style unattainable for many researchers2 1 , 22.",
    "page": 13
  },
  {
    "type": "table",
    "content": "ive training of NLP models also makes this\nresearch style unattainable for many researchers2 1 , 22. This inequity of access to state-of-the-art models is\nanother strong motivator, alongside environmental concerns, to incentivize the development of energy-efficient\nML models that work as well as their computationally hungrier counterparts.\nOne issue that was difficult for us during our investigation was to put into perspective the 4 to 552 tCO e\n2\nfrom training of these NLP models, which the next subsection explores.\n4.8 How does training a large NLP model compare to other activities?\nGoogle Flights estimate for the emissions of a direct round trip of a whole passenger jet between San\nFrancisco and New York is 180 tCO e (see Table 2 and Appendix A). T5 training emissions are ~26%, Meena\n2",
    "page": 13
  },
  {
    "type": "table",
    "content": "sco and New York is 180 tCO e (see Table 2 and Appendix A). T5 training emissions are ~26%, Meena\n2\nis 53%, Gshard-600B is ~2%, Switch Transformer is 32%, and GPT-3 is ~305% of such a round trip.\nAnother comparison point is to B itcoin. Every purchase that transfers bitcoin currently costs ~700 KWh or\n~0.3 tCO e, equivalent to the CO e produced by ~750,000 credit card swipes. Bitcoin miners use custom chips\n2 2\nthat operate continuously 24/7 until they fail. Estimates of Bitcoin’s impact for 2021 are ~78–121\nTeraWatt-hours and ~37M–58M tCO e [Cri21, Dig21]. Stated alternatively, ~70M people have Bitcoin wallets\n2\nyet Google consumes 1/10th of Bitcoin’s energy to provide services for billions of people, and all of Google’s\nenergy use is offset.",
    "page": 13
  },
  {
    "type": "table",
    "content": "tcoin’s energy to provide services for billions of people, and all of Google’s\nenergy use is offset. If Bitcoin were a country, it would be in the top 30 in CO e; larger than Argentina, whose\n2\npopulation is 45M. The estimated annual carbon footprint of Bitcoin mining this year is equivalent to roughly\n200,000 to 300,000 whole passenger jet SF↔NY round trips.\nIn 2019 t he world saw 39M flights and U S airlines flew 925M passengers, which helps explain why air\ntravel was responsible for 940 MtCO , or ~2.5% of the world's annual CO in 2018 of 33B tCO e [Rit20].\n2 2 2\nFinally, Google publishes its total energy consumption, and for 2019 it was 12.2 TeraWatt-hours [Goo20].\nRow 18 of Table 4 shows the percentage that each NLP model training was of that total. Even if we assume all",
    "page": 13
  },
  {
    "type": "table",
    "content": "Table 4 shows the percentage that each NLP model training was of that total. Even if we assume all\nfour of Google’s large NLP models in Table 4 were trained in 2019, the total represents less than 0.005%. T he\ntraining of those four large NLP models is not a significant fraction of Google’s energy consumption.\n19 Improvement in energy intensity is expressed as energy use per compute instance. [Has20] goes on to say the cloud’s\nincreasing share of datacenters is causing a “notable improvement compared with recent annual efficiency gains in other\nmajor demand sectors (e.g., aviation and industry), which are an order of magnitude lower.”\n20 There are not many cloud companies. With new technologies, initially only a few firms can practice the technology and",
    "page": 13
  },
  {
    "type": "table",
    "content": "cloud companies. With new technologies, initially only a few firms can practice the technology and\nthey sell it to others, but these companies compete. There are many examples. Chemical technologies are in the hands of\na relatively small number of companies; only six or seven institutions worldwide can refine crude oil; just a few firms can\nmanufacture computer chips in the finest technology node (3–5 nm).\n21 To support the goal of making ML more inclusive, G oogle provides free access to a total of ~500 PetaFLOPS/second of\nTPU compute power to help ML researchers around the world participate in advancing the start of the art of ML.\n22 One possible unintended consequence of making training of a model less expensive is that more people will train the",
    "page": 13
  },
  {
    "type": "table",
    "content": "ntended consequence of making training of a model less expensive is that more people will train the\nmodel and increase energy use, but that seems like a better risk than to continue using inefficient models.\n13",
    "page": 13
  },
  {
    "type": "table",
    "content": "TABLE (Page 13):\ndramatically reduce their carbon footprint in the future, it’s still important to reduce the economic c ost of\ntraining. Saving money obviously matters to everyone, but e",
    "page": 13
  },
  {
    "type": "text",
    "content": "Having spent 13 pages on the cost of large NLP models and neural architecture search, we conclude our\ndiscussion with three examples of the potential benefits of NLP models.\n4.9 Are the benefits of NLP models worth the energy cost?\nA recent example of a societal benefit of NLP is the C OVID-19 Research Explorer, which helps scientists\nand researchers efficiently pore through articles for answers or evidence to COVID-19-related questions. It is\npowered by B ERT, a Transformer-style model trained for the biomedical domain [Hal20].2 3 Its training\nconsumed ~2.8 MWh and produced 0.13 tCO e, about one-tenth of a SF-NY round trip by one passenger.2 4\n2\nA more widespread example is the use o f BERT in search. English is the most popular language on the\nweb.",
    "page": 14
  },
  {
    "type": "text",
    "content": "e widespread example is the use o f BERT in search. English is the most popular language on the\nweb. This use of BERT takes models that learn from improvements in English and applies them to other\nlanguages. In particular, BERT significantly improved featured snippets—short text summary at the top of\nGoogle research results—in languages like Hindi, Korean, and Portuguese.\nFigure 7: Reproduction of Figure 6 from [Lep20] with annotations. Translation quality comparison of\nmultilingual Mixture of Expert (MoE) Transformer models trained with GShard showing the increase in\nBLEU score versus a separate baseline Transformer model trained on each language pair for 100\nlanguages to English. MoE models have large model capacity but are only partially activated for any\ngiven token.",
    "page": 14
  },
  {
    "type": "text",
    "content": "English. MoE models have large model capacity but are only partially activated for any\ngiven token. The source languages are grouped on the x-axis by the resources available for each\nlanguage in billions of speakers, with languages like French and Spanish on the left (>1B examples)\nand languages like Sindhi and Yoruba on the right (<1M examples). The BLEU score improvements\nfrom larger models and multilingual training are high for all languages but are even higher for\nlow-resource languages—the graph’s right-hand side is higher than the left—so Yoruba translation\nquality benefits more than Spanish translation quality.\nA final example is the GShard multilingual translation model itself. B ender & Gebru e t al. [Ben21] raise",
    "page": 14
  },
  {
    "type": "text",
    "content": "example is the GShard multilingual translation model itself. B ender & Gebru e t al. [Ben21] raise\nseveral legitimate issues in the development and use of large language models. Creating such models\nrequires careful attention to issues of fairness and bias [Ben21, Gar19, Joh20, Kuc18, Mer19], but they also\nhave the potential to benefit people everywhere. F or example, o ur large scale translation models (M4) have\n23 Despite targeting a narrow audience of scientists, COVID explorer served 1000 queries per day at launch. It drew\ninterest from Pfizer, Bristol Myers Squibb, AstraZeneca, Regeneron, British Medical Journal, European Food Safety\nAuthority, and the National Institute of Health. Pfizer’s Director of Global Medical Epidemiology used the tool daily; it led to",
    "page": 14
  },
  {
    "type": "text",
    "content": "nstitute of Health. Pfizer’s Director of Global Medical Epidemiology used the tool daily; it led to\nPfizer epidemiology research group to adapt the underlying ML models for systematic reviews and literature search.\n24 Training COVID Explorer took 6 days on 64 TPU v3s running in Oklahoma. It used ~2.8 MWh and 0.13 net tCO e.\n2\n14",
    "page": 14
  },
  {
    "type": "table",
    "content": "TABLE (Page 14):\n\nHaving spent 13 pages on the cost of large NLP models and neural architecture search, we conclude our\ndiscussion with three examples of the potential benefits of NLP models.\n4.9 Are the benefits of NLP models worth the energy cost?\nA recent example of a societal benefit of NLP is the C OVID-19 Research Explorer, which helps scientists\nand researchers efficiently pore through articles for answers or evidence to COVID-19-related questions. It is\npowered by B ERT, a Transformer-style model trained for the biomedical domain [Hal20].2 3 Its training\nconsumed ~2.8 MWh and produced 0.13 tCO e, about one-tenth of a SF-NY round trip by one passenger.2 4\n2\nA more widespread example is the use o f BERT in search. English is the most popular language on the\nweb.",
    "page": 14
  },
  {
    "type": "table",
    "content": "e widespread example is the use o f BERT in search. English is the most popular language on the\nweb. This use of BERT takes models that learn from improvements in English and applies them to other\nlanguages. In particular, BERT significantly improved featured snippets—short text summary at the top of\nGoogle research results—in languages like Hindi, Korean, and Portuguese.\nFigure 7: Reproduction of Figure 6 from [Lep20] with annotations. Translation quality comparison of\nmultilingual Mixture of Expert (MoE) Transformer models trained with GShard showing the increase in\nBLEU score versus a separate baseline Transformer model trained on each language pair for 100\nlanguages to English. MoE models have large model capacity but are only partially activated for any\ngiven token.",
    "page": 14
  },
  {
    "type": "table",
    "content": "English. MoE models have large model capacity but are only partially activated for any\ngiven token. The source languages are grouped on the x-axis by the resources available for each\nlanguage in billions of speakers, with languages like French and Spanish on the left (>1B examples)\nand languages like Sindhi and Yoruba on the right (<1M examples). The BLEU score improvements\nfrom larger models and multilingual training are high for all languages but are even higher for\nlow-resource languages—the graph’s right-hand side is higher than the left—so Yoruba translation\nquality benefits more than Spanish translation quality.\nA final example is the GShard multilingual translation model itself. B ender & Gebru e t al. [Ben21] raise",
    "page": 14
  },
  {
    "type": "table",
    "content": "example is the GShard multilingual translation model itself. B ender & Gebru e t al. [Ben21] raise\nseveral legitimate issues in the development and use of large language models. Creating such models\nrequires careful attention to issues of fairness and bias [Ben21, Gar19, Joh20, Kuc18, Mer19], but they also\nhave the potential to benefit people everywhere. F or example, o ur large scale translation models (M4) have\n23 Despite targeting a narrow audience of scientists, COVID explorer served 1000 queries per day at launch. It drew\ninterest from Pfizer, Bristol Myers Squibb, AstraZeneca, Regeneron, British Medical Journal, European Food Safety\nAuthority, and the National Institute of Health. Pfizer’s Director of Global Medical Epidemiology used the tool daily; it led to",
    "page": 14
  },
  {
    "type": "table",
    "content": "nstitute of Health. Pfizer’s Director of Global Medical Epidemiology used the tool daily; it led to\nPfizer epidemiology research group to adapt the underlying ML models for systematic reviews and literature search.\n24 Training COVID Explorer took 6 days on 64 TPU v3s running in Oklahoma. It used ~2.8 MWh and 0.13 net tCO e.\n2\n14",
    "page": 14
  },
  {
    "type": "table",
    "content": "TABLE (Page 14):\nseveral legitimate issues in the development and use of large language models. Creating such models | \nrequires careful attention to issues of fairness and bias [Ben21, Gar19, Joh20, Kuc18, Mer19], but they also | \nhave the potential to benefit people everywhere. | o ur large scale translation models (M4) have",
    "page": 14
  },
  {
    "type": "text",
    "content": "already been used to translate billions of queries annually for each mid-to-low resource language2 5 with 2B\nspeakers globally for these languages. Figure 7, from the GShard paper [Lep20], shows substantial\nimprovements for translation of 100 different languages to English. The blue line on the top in the left\nrepresents the 600B parameter multi-lingual translation MoE model of GShard. The dashed black line near the\nbottom is for a traditional dense DNN that is fully activated for every token. The dense DNN requires ~10X\nmore computational resources to train than the 600B sparse MoE model, despite substantially lower translation\nquality. Figure 7 shows the larger MoE model, the larger the BLEU score gains were across all languages; the\nlines rarely cross.",
    "page": 15
  },
  {
    "type": "text",
    "content": "larger MoE model, the larger the BLEU score gains were across all languages; the\nlines rarely cross. The 600B MoE model improves average quality +13.5 BLEU, 7.4 higher than the 2.3B dense\nmodel.\nGShard-600B’s emissions (Table 4) are 4.3 tCO e —3.5 passenger SF-NY round trips—from consuming 24\n2\nMWh to train the model that could have 2B users; the amortized per-user CO e impact of model training would\n2\nbe less than the CO e impact of sending one text message2 6.\n2\n5. Conclusion\nGlobal climate change is a threat to economies, human health, and the environment, and the ML community\nneeds to do its share to limit its carbon emissions.2 7 We’re thankful that papers like [Lac19, Str19, Sch20,\nHen20] helped make the ML community aware of this important issue. Improving the energy efficiency of",
    "page": 15
  },
  {
    "type": "text",
    "content": "n20] helped make the ML community aware of this important issue. Improving the energy efficiency of\nalgorithms, datacenters, hardware, and software has long been a business priority for Google and other Cloud\ncompanies. For example, Gshard-600B operates much more efficiently than other large NLP models and ML\naccelerators are more efficient than off-the-shelf hardware.\nAs mentioned in the introduction, we make three suggestions for publications on compute intensive models\nthat could eventually help reduce their CO e footprint: report energy consumed and CO e explicitly, ML\n2 2\nconferences should reward improvements in efficiency as well as traditional metrics, and include the time and\nnumber of processors for training to help everyone understand its cost.",
    "page": 15
  },
  {
    "type": "text",
    "content": "cs, and include the time and\nnumber of processors for training to help everyone understand its cost. We believe power will be included in\nupcoming MLPerf benchmarks, which is an important step in the right direction.\nIf the ML community working on computationally intensive models starts competing on training quality and\ncarbon footprint rather than on accuracy alone, the most efficient datacenters and hardware might see the\nhighest ML demand. If paired with publication incentives to improve emission metrics in addition to accuracy,\nwe can imagine a virtuous cycle that slows the growth of the carbon footprint of ML by accelerating innovations\nin the efficiency and cost of algorithms, systems, hardware, datacenters, and carbon free energy.\nAcknowledgements",
    "page": 15
  },
  {
    "type": "text",
    "content": "cy and cost of algorithms, systems, hardware, datacenters, and carbon free energy.\nAcknowledgements\nWe wish to express our thanks to colleagues at Google and elsewhere who helped shape and improve this\npaper. Emma Strubell made several suggestions of ideas and organization of the paper, including suggesting\nadding data about the five large models. We thank Christopher Berner, Ilya Sutskever, OpenAI, and Microsoft\nfor sharing information about GPT-3. Dmitry Lepikhin and Zongwei Zhou did a great deal of work to measure\nthe performance and power of GPUs and TPUs in Google datacenters. Hallie Cramer, Anna Escuer, Elke\nMichlmayr, Kelli Wright, and Nick Zakrasek helped with the sections on energy and CO e emissions at Google.\n2\nTim Kraska suggested a revised organization of this paper.",
    "page": 15
  },
  {
    "type": "text",
    "content": "on energy and CO e emissions at Google.\n2\nTim Kraska suggested a revised organization of this paper. We thank Daniel Adiwardana, Gabriel Bender,\nAndrei Broder, Charina Chou, Jesse Dodge, Oren Etzioni, Orhan Firat, Ananya Ganesh, Robbie Gonzalez,\nDavid Grangier, Marsden Hanna, Urs Hölzle, Sheng Li, Sasha Luccioni, Preston McAfee, Andrew McCallum,\nEsteban Real, Stven Ross, Brennan Saeta, Roy Schwartz, Victor Schmidt, Ian Schneider, Aarush Selvan,\nNoah A. Smith, Zak Stone, Kate Weber, and Cliff Young for their help and feedback on the manuscript.\n25 In our setup for Figure 7, low resource languages have less than 1M training examples, mid resource languages have\nless than 10M training examples, and high resource languages have more than 1B training examples.\n26 An S MS message is 0.",
    "page": 15
  },
  {
    "type": "text",
    "content": "examples, and high resource languages have more than 1B training examples.\n26 An S MS message is 0.014 g of CO . That is larger than 24 MWh / 2B, which yields about 0.005 g of CO .\n2 2\n27 We did not address the carbon footprint of ML in phones and other edge devices. It would be an excellent topic for\nanother paper.\n15",
    "page": 15
  },
  {
    "type": "table",
    "content": "TABLE (Page 15):\n\nalready been used to translate billions of queries annually for each mid-to-low resource language2 5 with 2B\nspeakers globally for these languages. Figure 7, from the GShard paper [Lep20], shows substantial\nimprovements for translation of 100 different languages to English. The blue line on the top in the left\nrepresents the 600B parameter multi-lingual translation MoE model of GShard. The dashed black line near the\nbottom is for a traditional dense DNN that is fully activated for every token. The dense DNN requires ~10X\nmore computational resources to train than the 600B sparse MoE model, despite substantially lower translation\nquality. Figure 7 shows the larger MoE model, the larger the BLEU score gains were across all languages; the\nlines rarely cross.",
    "page": 15
  },
  {
    "type": "table",
    "content": "larger MoE model, the larger the BLEU score gains were across all languages; the\nlines rarely cross. The 600B MoE model improves average quality +13.5 BLEU, 7.4 higher than the 2.3B dense\nmodel.\nGShard-600B’s emissions (Table 4) are 4.3 tCO e —3.5 passenger SF-NY round trips—from consuming 24\n2\nMWh to train the model that could have 2B users; the amortized per-user CO e impact of model training would\n2\nbe less than the CO e impact of sending one text message2 6.\n2\n5. Conclusion\nGlobal climate change is a threat to economies, human health, and the environment, and the ML community\nneeds to do its share to limit its carbon emissions.2 7 We’re thankful that papers like [Lac19, Str19, Sch20,\nHen20] helped make the ML community aware of this important issue. Improving the energy efficiency of",
    "page": 15
  },
  {
    "type": "table",
    "content": "n20] helped make the ML community aware of this important issue. Improving the energy efficiency of\nalgorithms, datacenters, hardware, and software has long been a business priority for Google and other Cloud\ncompanies. For example, Gshard-600B operates much more efficiently than other large NLP models and ML\naccelerators are more efficient than off-the-shelf hardware.\nAs mentioned in the introduction, we make three suggestions for publications on compute intensive models\nthat could eventually help reduce their CO e footprint: report energy consumed and CO e explicitly, ML\n2 2\nconferences should reward improvements in efficiency as well as traditional metrics, and include the time and\nnumber of processors for training to help everyone understand its cost.",
    "page": 15
  },
  {
    "type": "table",
    "content": "cs, and include the time and\nnumber of processors for training to help everyone understand its cost. We believe power will be included in\nupcoming MLPerf benchmarks, which is an important step in the right direction.\nIf the ML community working on computationally intensive models starts competing on training quality and\ncarbon footprint rather than on accuracy alone, the most efficient datacenters and hardware might see the\nhighest ML demand. If paired with publication incentives to improve emission metrics in addition to accuracy,\nwe can imagine a virtuous cycle that slows the growth of the carbon footprint of ML by accelerating innovations\nin the efficiency and cost of algorithms, systems, hardware, datacenters, and carbon free energy.\nAcknowledgements",
    "page": 15
  },
  {
    "type": "table",
    "content": "cy and cost of algorithms, systems, hardware, datacenters, and carbon free energy.\nAcknowledgements\nWe wish to express our thanks to colleagues at Google and elsewhere who helped shape and improve this\npaper. Emma Strubell made several suggestions of ideas and organization of the paper, including suggesting\nadding data about the five large models. We thank Christopher Berner, Ilya Sutskever, OpenAI, and Microsoft\nfor sharing information about GPT-3. Dmitry Lepikhin and Zongwei Zhou did a great deal of work to measure\nthe performance and power of GPUs and TPUs in Google datacenters. Hallie Cramer, Anna Escuer, Elke\nMichlmayr, Kelli Wright, and Nick Zakrasek helped with the sections on energy and CO e emissions at Google.\n2\nTim Kraska suggested a revised organization of this paper.",
    "page": 15
  },
  {
    "type": "table",
    "content": "on energy and CO e emissions at Google.\n2\nTim Kraska suggested a revised organization of this paper. We thank Daniel Adiwardana, Gabriel Bender,\nAndrei Broder, Charina Chou, Jesse Dodge, Oren Etzioni, Orhan Firat, Ananya Ganesh, Robbie Gonzalez,\nDavid Grangier, Marsden Hanna, Urs Hölzle, Sheng Li, Sasha Luccioni, Preston McAfee, Andrew McCallum,\nEsteban Real, Stven Ross, Brennan Saeta, Roy Schwartz, Victor Schmidt, Ian Schneider, Aarush Selvan,\nNoah A. Smith, Zak Stone, Kate Weber, and Cliff Young for their help and feedback on the manuscript.\n25 In our setup for Figure 7, low resource languages have less than 1M training examples, mid resource languages have\nless than 10M training examples, and high resource languages have more than 1B training examples.\n26 An S MS message is 0.",
    "page": 15
  },
  {
    "type": "table",
    "content": "examples, and high resource languages have more than 1B training examples.\n26 An S MS message is 0.014 g of CO . That is larger than 24 MWh / 2B, which yields about 0.005 g of CO .\n2 2\n27 We did not address the carbon footprint of ML in phones and other edge devices. It would be an excellent topic for\nanother paper.\n15",
    "page": 15
  },
  {
    "type": "text",
    "content": "References\n[Adi20] Adiwardana, D., L uong, M., R. So, D., Hall, J., Fiedel, N., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade,\nG., Lu, Y., and Le. Q. T owards a Human-like Open-Domain Chatbot. a rXiv preprint arXiv:2001.09977.\n[Arm10] Armbrust, M., Fox, A., Griffith, R., Joseph, A.D., Katz, R., Konwinski, A., Lee, G., Patterson, D., Rabkin, A.,\nStoica, I. and Zaharia, M., 2010. A view of cloud computing. C ommunications of the ACM, 53(4), pp.50-58.\n[Bar19] Barr, J. December 3, 2019. Amazon EC2 Update,\naws.amazon.com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips\n-for-high-performance-cost-effective-inferencing/.\n[Bro20] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam , P., Sastry, G.,\nAskell, A., Agarwal, S.",
    "page": 16
  },
  {
    "type": "text",
    "content": "biah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam , P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J.,\nWinter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S.,\nRadford, A., Sutskever, I., Amodei, D. July 22, 2020. Language models are few-shot learners. NeurIPS 2020.\narXiv preprint arXiv:2005.14165.\n[Ben21] Bender, E., Gebru, T., McMillan-Major, A. Shmitchell, S. On the Dangers of Stochastic Parrots: Can Language\nModels Be Too Big? FAccT 2021. h ttp://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf.\n[Car21] Carbon Offset Research and Education, 2021, Carbon Offset Guide, h ttps://www.offsetguide.org/.\n[Cha19] Chang, K.W.",
    "page": 16
  },
  {
    "type": "text",
    "content": "esearch and Education, 2021, Carbon Offset Guide, h ttps://www.offsetguide.org/.\n[Cha19] Chang, K.W., Prabhakaran, V. and Ordonez, V., 2019, November. Bias and fairness in natural language\nprocessing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): Tutorial Abstracts.\nhttps://arxiv.org/pdf/1908.09635.pdf.\n[Cri21] Criddle, C., February 10, 2021. Bitcoin consumes more electricity than Argentina,\nwww.bbc.com/news/technology-56012952.\n[Dig21] Digiconomist, 2021, Bitcoin Energy Consumption Index, h ttps://digiconomist.net/bitcoin-energy-consumption/ .\n[Dod19] Dodge, J., Gururangan, S., Card, D., Schwartz, R., and Smith, N., 2019. Show Your Work: Improved Reporting",
    "page": 16
  },
  {
    "type": "text",
    "content": "J., Gururangan, S., Card, D., Schwartz, R., and Smith, N., 2019. Show Your Work: Improved Reporting\nof Experimental Results. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP).w ww.aclweb.org/anthology/D19-1224/.\n[Dod20] Dodge, J., Ilharco, G., Schwartz, R., Farhadi, A., Hajishirzi, H. and Smith, N., 2020. Fine-tuning pretrained\nlanguage models: Weight initializations, data orders, and early stopping. a rXiv preprint arXiv:2002.06305.\n[Evo19] Apache-licensed Evolved Transformer open-source implementation in tensorflow/tensor2tensor GitHub\nrepository.\nhttps://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py",
    "page": 16
  },
  {
    "type": "text",
    "content": "https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py\n[Fed21] Fedus, W., Zoph, B., Shazeer, N., January 11, 2021, Switch Transformers: Scaling to Trillion Parameter Models\nwith Simple and Efficient Sparsity h ttps://arxiv.org/abs/2101.03961.\n[Gar19] Garg, S., Perot, V., Limtiaco, N., Taly, A., Chi, E.H. and Beutel, A., 2019, January. Counterfactual fairness in text\nclassification through robustness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society\n(pp. 219-226). h ttps://research.google/pubs/pub47670/ .\n[Goo16] Google, December 2016, Achieving Our 100% Renewable Energy Purchasing Goal and Going Beyond,\nhttps://static.\ngoogleusercontent.com/media/www.google.",
    "page": 16
  },
  {
    "type": "text",
    "content": "ble Energy Purchasing Goal and Going Beyond,\nhttps://static.\ngoogleusercontent.com/media/www.google.com/en//green/pdf/achieving-100-renewable-energy-purchasing-goal\n.pdf.\n[Goo20] Google, Environmental Report 2020,\nhttps://www.gstatic.com/gumdrop/sustainability/google-2020-environmental-report.pdf.\n[Goo21] Google, February 2021, 24/7 Carbon-Free Energy: Methodologies and Metrics,\nhttps://www.gstatic.com/gumdrop/sustainability/24x7-carbon-free-energy-methodologies-metrics.pdf.\n[Gup20] Gupta, U., Kim, Y.G., Lee, S., Tse, J., Lee, H.H.S., Wei, G.Y., Brooks, D. and Wu, C.J., 2020. Chasing Carbon:\nThe Elusive Environmental Footprint of Computing. a rXiv preprint arXiv:2011.02839.\n[Hal20] Hall, K., May 4, 2020, An NLU-Powered Tool to Explore COVID-19,\nhttps://ai.googleblog.",
    "page": 16
  },
  {
    "type": "text",
    "content": "2839.\n[Hal20] Hall, K., May 4, 2020, An NLU-Powered Tool to Explore COVID-19,\nhttps://ai.googleblog.com/2020/05/an-nlu-powered-tool-to-explore-covid-19.html.\n[Han15] Han, S., Pool, J., Tran, J. and Dally, W.J., 2015. Learning both weights and connections for efficient neural\nnetworks. ICLR 2016. a rXiv preprint arXiv:1510.00149.\n[Hen20] Henderson, P., Hu, J., Romoff, J., Brunskill, E., Jurafsky, D. and Pineau, J., 2020. Towards the systematic\nreporting of the energy and carbon footprints of machine learning. Journal of Machine Learning Research.\nhttps://jmlr.org/papers/v21/20-312.html\n[Her20] Hernandez, D. and Brown, T.B., 2020. Measuring the algorithmic efficiency of neural networks. arXiv preprint\narXiv:2005.04305. h ttps://arxiv.org/abs/2005.04305.\n[Hin15] Hinton, G., Vinyals, O.",
    "page": 16
  },
  {
    "type": "text",
    "content": "arXiv preprint\narXiv:2005.04305. h ttps://arxiv.org/abs/2005.04305.\n[Hin15] Hinton, G., Vinyals, O. and Dean, J., 2015. Distilling the knowledge in a neural network. a rXiv preprint\narXiv:1503.02531.\n[Höl20] Hölzle, U., Feb 27, 2020. datacenters are more energy efficient than ever.\nblog.google/outreach-initiatives/sustainability/data-centers-energy-efficient\n[Joh20] Johnson, M., April 22, 2020, A Scalable Approach to Reducing Gender Bias in Google Translate,\nhttps://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html .\n16",
    "page": 16
  },
  {
    "type": "table",
    "content": "TABLE (Page 16):\n\nReferences\n[Adi20] Adiwardana, D., L uong, M., R. So, D., Hall, J., Fiedel, N., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade,\nG., Lu, Y., and Le. Q. T owards a Human-like Open-Domain Chatbot. a rXiv preprint arXiv:2001.09977.\n[Arm10] Armbrust, M., Fox, A., Griffith, R., Joseph, A.D., Katz, R., Konwinski, A., Lee, G., Patterson, D., Rabkin, A.,\nStoica, I. and Zaharia, M., 2010. A view of cloud computing. C ommunications of the ACM, 53(4), pp.50-58.\n[Bar19] Barr, J. December 3, 2019. Amazon EC2 Update,\naws.amazon.com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips\n-for-high-performance-cost-effective-inferencing/.\n[Bro20] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam , P., Sastry, G.,\nAskell, A.",
    "page": 16
  },
  {
    "type": "table",
    "content": "yder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam , P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J.,\nWinter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S.,\nRadford, A., Sutskever, I., Amodei, D. July 22, 2020. Language models are few-shot learners. NeurIPS 2020.\narXiv preprint arXiv:2005.14165.\n[Ben21] Bender, E., Gebru, T., McMillan-Major, A. Shmitchell, S. On the Dangers of Stochastic Parrots: Can Language\nModels Be Too Big? FAccT 2021. h ttp://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf.\n[Car21] Carbon Offset Research and Education, 2021, Carbon Offset Guide, h ttps://www.offsetguide.org/.\n[Cha19] Chang, K.",
    "page": 16
  },
  {
    "type": "table",
    "content": "Research and Education, 2021, Carbon Offset Guide, h ttps://www.offsetguide.org/.\n[Cha19] Chang, K.W., Prabhakaran, V. and Ordonez, V., 2019, November. Bias and fairness in natural language\nprocessing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): Tutorial Abstracts.\nhttps://arxiv.org/pdf/1908.09635.pdf.\n[Cri21] Criddle, C., February 10, 2021. Bitcoin consumes more electricity than Argentina,\nwww.bbc.com/news/technology-56012952.\n[Dig21] Digiconomist, 2021, Bitcoin Energy Consumption Index, h ttps://digiconomist.net/bitcoin-energy-consumption/ .\n[Dod19] Dodge, J., Gururangan, S., Card, D., Schwartz, R., and Smith, N., 2019.",
    "page": 16
  },
  {
    "type": "table",
    "content": "nergy-consumption/ .\n[Dod19] Dodge, J., Gururangan, S., Card, D., Schwartz, R., and Smith, N., 2019. Show Your Work: Improved Reporting\nof Experimental Results. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP).w ww.aclweb.org/anthology/D19-1224/.\n[Dod20] Dodge, J., Ilharco, G., Schwartz, R., Farhadi, A., Hajishirzi, H. and Smith, N., 2020. Fine-tuning pretrained\nlanguage models: Weight initializations, data orders, and early stopping. a rXiv preprint arXiv:2002.06305.\n[Evo19] Apache-licensed Evolved Transformer open-source implementation in tensorflow/tensor2tensor GitHub\nrepository.\nhttps://github.",
    "page": 16
  },
  {
    "type": "table",
    "content": "ransformer open-source implementation in tensorflow/tensor2tensor GitHub\nrepository.\nhttps://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py\n[Fed21] Fedus, W., Zoph, B., Shazeer, N., January 11, 2021, Switch Transformers: Scaling to Trillion Parameter Models\nwith Simple and Efficient Sparsity h ttps://arxiv.org/abs/2101.03961.\n[Gar19] Garg, S., Perot, V., Limtiaco, N., Taly, A., Chi, E.H. and Beutel, A., 2019, January. Counterfactual fairness in text\nclassification through robustness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society\n(pp. 219-226). h ttps://research.google/pubs/pub47670/ .\n[Goo16] Google, December 2016, Achieving Our 100% Renewable Energy Purchasing Goal and Going Beyond,\nhttps://static.\ngoogleusercontent.",
    "page": 16
  },
  {
    "type": "table",
    "content": "eving Our 100% Renewable Energy Purchasing Goal and Going Beyond,\nhttps://static.\ngoogleusercontent.com/media/www.google.com/en//green/pdf/achieving-100-renewable-energy-purchasing-goal\n.pdf.\n[Goo20] Google, Environmental Report 2020,\nhttps://www.gstatic.com/gumdrop/sustainability/google-2020-environmental-report.pdf.\n[Goo21] Google, February 2021, 24/7 Carbon-Free Energy: Methodologies and Metrics,\nhttps://www.gstatic.com/gumdrop/sustainability/24x7-carbon-free-energy-methodologies-metrics.pdf.\n[Gup20] Gupta, U., Kim, Y.G., Lee, S., Tse, J., Lee, H.H.S., Wei, G.Y., Brooks, D. and Wu, C.J., 2020. Chasing Carbon:\nThe Elusive Environmental Footprint of Computing. a rXiv preprint arXiv:2011.02839.\n[Hal20] Hall, K., May 4, 2020, An NLU-Powered Tool to Explore COVID-19,\nhttps://ai.googleblog.",
    "page": 16
  },
  {
    "type": "table",
    "content": "2839.\n[Hal20] Hall, K., May 4, 2020, An NLU-Powered Tool to Explore COVID-19,\nhttps://ai.googleblog.com/2020/05/an-nlu-powered-tool-to-explore-covid-19.html.\n[Han15] Han, S., Pool, J., Tran, J. and Dally, W.J., 2015. Learning both weights and connections for efficient neural\nnetworks. ICLR 2016. a rXiv preprint arXiv:1510.00149.\n[Hen20] Henderson, P., Hu, J., Romoff, J., Brunskill, E., Jurafsky, D. and Pineau, J., 2020. Towards the systematic\nreporting of the energy and carbon footprints of machine learning. Journal of Machine Learning Research.\nhttps://jmlr.org/papers/v21/20-312.html\n[Her20] Hernandez, D. and Brown, T.B., 2020. Measuring the algorithmic efficiency of neural networks. arXiv preprint\narXiv:2005.04305. h ttps://arxiv.org/abs/2005.04305.\n[Hin15] Hinton, G., Vinyals, O.",
    "page": 16
  },
  {
    "type": "table",
    "content": "arXiv preprint\narXiv:2005.04305. h ttps://arxiv.org/abs/2005.04305.\n[Hin15] Hinton, G., Vinyals, O. and Dean, J., 2015. Distilling the knowledge in a neural network. a rXiv preprint\narXiv:1503.02531.\n[Höl20] Hölzle, U., Feb 27, 2020. datacenters are more energy efficient than ever.\nblog.google/outreach-initiatives/sustainability/data-centers-energy-efficient\n[Joh20] Johnson, M., April 22, 2020, A Scalable Approach to Reducing Gender Bias in Google Translate,\nhttps://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html .\n16",
    "page": 16
  },
  {
    "type": "table",
    "content": "TABLE (Page 16):\nAdiwardana, D., L uong, M., R. So, D., Hall, J., Fiedel, N., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade,\nG., Lu, Y., and Le. Q.",
    "page": 16
  },
  {
    "type": "table",
    "content": "TABLE (Page 16):\nGupta, U., Kim, Y.G., Lee, S., Tse, J., Lee, H.H.S., Wei, G.Y., Brooks, D. and Wu, C.J., 2020. Chasing Carbon:\nThe Elusive Environmental Footprint of Computing. a rXiv preprint arXiv:2011.02839.",
    "page": 16
  },
  {
    "type": "text",
    "content": "[Jou21] Jouppi, N., Yoon, D-H, Jablin, T., Kurian, G., Laudon, J., Li, S., Ma, P., Ma, X., Patil, N.,Prasad, S., Young, C.,\nZhou, Z., and Patterson, D., May 2021. Ten Lessons From Three Generations Shaped Google’s TPUv4i, to\nappear, the 48th International Symposium on Computer Architecture.\n[Kap20] Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J. and\nAmodei, D., 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.\n[Kär18] Kärcher B. Formation and radiative forcing of contrail cirrus. N ature communications . 2018 May 8;9(1):1-7.\nhttps://www.nature.com/articles/s41467-018-04068-0.\n[Kuc18] Kuczmarski, J. and Johnson, M., 2018. Gender-aware natural language\ntranslation.w ww.tdcommons.org/dpubs_series/1577/.",
    "page": 17
  },
  {
    "type": "text",
    "content": "Johnson, M., 2018. Gender-aware natural language\ntranslation.w ww.tdcommons.org/dpubs_series/1577/.\n[Lac19] Lacoste, A., Luccioni, A., Schmidt, V. and Dandres, T., 2019. Quantifying the carbon emissions of machine\nlearning. a rXiv preprint arXiv:1910.09700.\n[Lan20] Lannelongue, L., Grealey, J. and Inouye, M., 2020. Green algorithms: Quantifying the carbon footprint of\ncomputation. a rXiv: 2007.07610.\n[Leo19] Leopold, G. March 19, 2019, AWS to Offer Nvidia’s T4 GPUs for AI Inferencing,\nwww.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/ .\n[Lep20] Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N. and Chen, Z., 2020. GShard:\nScaling giant models with conditional computation and automatic sharding. a rXiv preprint arXiv:2006.16668.",
    "page": 17
  },
  {
    "type": "text",
    "content": "giant models with conditional computation and automatic sharding. a rXiv preprint arXiv:2006.16668.\n[Li21] Li, S., Tan, M., Pang, R., Li, A., Cheng, L., Le, Q. and Jouppi, N.P., 2021. Searching for Fast Model Families on\nDatacenter Accelerators. a rXiv preprint arXiv:2102.05610.\n[Liu18] Liu, H., Simonyan, K. and Yang, Y., 2018. Darts: Differentiable architecture search. a rXiv preprint\narXiv:1806.09055.\n[Luc21] Luccioni, A., and Schmidt, V.. March 2021, Private Communication.\n[Mas20] Masanet, E., Shehabi, A., Lei, N., Smith, S. and Koomey, J., 2020. Recalibrating global datacenter energy-use\nestimates. S cience, 367(6481), pp.984-986.\nhttps://datacenters.lbl.gov/sites/default/files/Masanet_et_al_Science_2020.full_.pdf.\n[Mas21] Masanet, E.",
    "page": 17
  },
  {
    "type": "text",
    "content": "://datacenters.lbl.gov/sites/default/files/Masanet_et_al_Science_2020.full_.pdf.\n[Mas21] Masanet, E., March 24, 2021, D ata Center Energy Analysis: Past, Present, and Future, lecture at UCSB.\n[Mer19] Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K. and Galstyan, A., 2019. A survey on bias and fairness in\nmachine learning. arXiv preprint arXiv:1908.09635. h ttps://arxiv.org/pdf/1908.09635.pdf.\n[Pha18] Pham, H., Guan, M., Zoph, B., Le, Q. and Dean, J., 2018, July. Efficient neural architecture search via\nparameters sharing. In International Conference on Machine Learning (pp. 4095-4104). PMLR. a rXiv preprint\narXiv:1802.03268.\n[Rad20] Radovanovic, A. April 22, 2020, Our datacenters now work harder when the sun shines and wind blows,\nhttps://blog.",
    "page": 17
  },
  {
    "type": "text",
    "content": "A. April 22, 2020, Our datacenters now work harder when the sun shines and wind blows,\nhttps://blog.google/inside-google/infrastructure/data-centers-work-harder-sun-shines-wind-blows\n[Raf19] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W. and Liu, P.J., 2019.\nExploring the limits of transfer learning with a unified text-to-text transformer. a rXiv preprint arXiv:1910.10683.\n[Rit20] Ritchie, H., October 22, 2020, Climate change and flying: what share of global CO2 emissions come from\naviation? h ttps://ourworldindata.org/co2-emissions-from-aviation .\n[Ryo14] Ryor, J.N. and Tawney, L.E.T.H.A., 2014. Utility-Scale Renewable Energy: Understanding Cost Parity. Paris:\nWorld Resources Institute.\nhttps://www.ctc-n.org/sites/www.ctc-n.",
    "page": 17
  },
  {
    "type": "text",
    "content": "Understanding Cost Parity. Paris:\nWorld Resources Institute.\nhttps://www.ctc-n.org/sites/www.ctc-n.org/files/resources/wri14_factsheets_utility_scale_v4.pdf.\n[San20] Sanh, V., Debut, L., Chaumond, J. and Wolf, T., 2019. DistilBERT, a distilled version of BERT: smaller, faster,\ncheaper and lighter. a rXiv preprint arXiv:1910.01108.\n[Sch20] Schwartz, R., Dodge, J., Smith, N.A. and Etzioni, O., 2020. Green AI. C ommunications of the ACM, 63(12),\npp.54-63. h ttps://cacm.acm.org/magazines/2020/12/248800-green-ai/fulltext.\n[Sch21] Schleier-Smith, J., Sreekanti, V., Khandelwal, A., Carreira, J., Yadwadkar, N., Popa, R., Joseph E. Gonzalez,J.,\nIon Stoica, I., and David A. Patterson, D., 2021 What Serverless Computing Is and Should Become: The Next",
    "page": 17
  },
  {
    "type": "text",
    "content": "oica, I., and David A. Patterson, D., 2021 What Serverless Computing Is and Should Become: The Next\nPhase of Cloud Computing, C ommunications of the ACM, 6 4(5).\n[Sha17] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G. and Dean, J., 2017. Outrageously large\nneural networks: The sparsely-gated mixture-of-experts layer. ICLR 2017. a rXiv preprint arXiv:1701.06538.\n[So19] So, D., Le, Q. and Liang, C., 2019, May. The Evolved Transformer. In International Conference on Machine\nLearning 2019 (pp. 5877-5886). PMLR. a rXiv preprint arXiv:1901.11117.\n[Str19] Strubell, E., Ganesh, A. and McCallum, A., 2019. Energy and policy considerations for deep learning in NLP.\nACL 2019. a rXiv preprint arXiv:1906.02243.\n[Sut21] Sutskever, I. Personal Communication, February 4, 2021.",
    "page": 17
  },
  {
    "type": "text",
    "content": ". a rXiv preprint arXiv:1906.02243.\n[Sut21] Sutskever, I. Personal Communication, February 4, 2021.\n[Tan19] Tan, M. and Le, Q., 2019, May. EfficientNet: Rethinking model scaling for convolutional neural networks. In\nInternational Conference on Machine Learning (pp. 6105-6114). PMLR. a rXiv preprint arXiv:1905.11946.\n[USE21] US Energy Information Administration, 2021, FAQ How much carbon dioxide is produced per kilowatt hour of\nU.S. electricity generation? h ttps://www.eia.gov/tools/faqs/faq.php?id=74&t=11.\n[Vas17] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L. and Polosukhin, I., 2017.\nAttention is all you need. NeurIPS 2017. a rXiv preprint arXiv:1706.03762.\n[Wan20] Wang, Y., Yao, Q., Kwok, J.T. and Ni, L.M., 2020.",
    "page": 17
  },
  {
    "type": "text",
    "content": "PS 2017. a rXiv preprint arXiv:1706.03762.\n[Wan20] Wang, Y., Yao, Q., Kwok, J.T. and Ni, L.M., 2020. Generalizing from a few examples: A survey on few-shot\nlearning. A CM Computing Surveys, 53(3), pp.1-34.\n17",
    "page": 17
  },
  {
    "type": "table",
    "content": "TABLE (Page 17):\n\n[Jou21] Jouppi, N., Yoon, D-H, Jablin, T., Kurian, G., Laudon, J., Li, S., Ma, P., Ma, X., Patil, N.,Prasad, S., Young, C.,\nZhou, Z., and Patterson, D., May 2021. Ten Lessons From Three Generations Shaped Google’s TPUv4i, to\nappear, the 48th International Symposium on Computer Architecture.\n[Kap20] Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J. and\nAmodei, D., 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.\n[Kär18] Kärcher B. Formation and radiative forcing of contrail cirrus. N ature communications . 2018 May 8;9(1):1-7.\nhttps://www.nature.com/articles/s41467-018-04068-0.\n[Kuc18] Kuczmarski, J. and Johnson, M., 2018. Gender-aware natural language\ntranslation.w ww.tdcommons.",
    "page": 17
  },
  {
    "type": "table",
    "content": "c18] Kuczmarski, J. and Johnson, M., 2018. Gender-aware natural language\ntranslation.w ww.tdcommons.org/dpubs_series/1577/.\n[Lac19] Lacoste, A., Luccioni, A., Schmidt, V. and Dandres, T., 2019. Quantifying the carbon emissions of machine\nlearning. a rXiv preprint arXiv:1910.09700.\n[Lan20] Lannelongue, L., Grealey, J. and Inouye, M., 2020. Green algorithms: Quantifying the carbon footprint of\ncomputation. a rXiv: 2007.07610.\n[Leo19] Leopold, G. March 19, 2019, AWS to Offer Nvidia’s T4 GPUs for AI Inferencing,\nwww.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/ .\n[Lep20] Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N. and Chen, Z., 2020. GShard:\nScaling giant models with conditional computation and automatic sharding.",
    "page": 17
  },
  {
    "type": "table",
    "content": "nd Chen, Z., 2020. GShard:\nScaling giant models with conditional computation and automatic sharding. a rXiv preprint arXiv:2006.16668.\n[Li21] Li, S., Tan, M., Pang, R., Li, A., Cheng, L., Le, Q. and Jouppi, N.P., 2021. Searching for Fast Model Families on\nDatacenter Accelerators. a rXiv preprint arXiv:2102.05610.\n[Liu18] Liu, H., Simonyan, K. and Yang, Y., 2018. Darts: Differentiable architecture search. a rXiv preprint\narXiv:1806.09055.\n[Luc21] Luccioni, A., and Schmidt, V.. March 2021, Private Communication.\n[Mas20] Masanet, E., Shehabi, A., Lei, N., Smith, S. and Koomey, J., 2020. Recalibrating global datacenter energy-use\nestimates. S cience, 367(6481), pp.984-986.\nhttps://datacenters.lbl.gov/sites/default/files/Masanet_et_al_Science_2020.full_.pdf.\n[Mas21] Masanet, E.",
    "page": 17
  },
  {
    "type": "table",
    "content": "://datacenters.lbl.gov/sites/default/files/Masanet_et_al_Science_2020.full_.pdf.\n[Mas21] Masanet, E., March 24, 2021, D ata Center Energy Analysis: Past, Present, and Future, lecture at UCSB.\n[Mer19] Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K. and Galstyan, A., 2019. A survey on bias and fairness in\nmachine learning. arXiv preprint arXiv:1908.09635. h ttps://arxiv.org/pdf/1908.09635.pdf.\n[Pha18] Pham, H., Guan, M., Zoph, B., Le, Q. and Dean, J., 2018, July. Efficient neural architecture search via\nparameters sharing. In International Conference on Machine Learning (pp. 4095-4104). PMLR. a rXiv preprint\narXiv:1802.03268.\n[Rad20] Radovanovic, A. April 22, 2020, Our datacenters now work harder when the sun shines and wind blows,\nhttps://blog.",
    "page": 17
  },
  {
    "type": "table",
    "content": "A. April 22, 2020, Our datacenters now work harder when the sun shines and wind blows,\nhttps://blog.google/inside-google/infrastructure/data-centers-work-harder-sun-shines-wind-blows\n[Raf19] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W. and Liu, P.J., 2019.\nExploring the limits of transfer learning with a unified text-to-text transformer. a rXiv preprint arXiv:1910.10683.\n[Rit20] Ritchie, H., October 22, 2020, Climate change and flying: what share of global CO2 emissions come from\naviation? h ttps://ourworldindata.org/co2-emissions-from-aviation .\n[Ryo14] Ryor, J.N. and Tawney, L.E.T.H.A., 2014. Utility-Scale Renewable Energy: Understanding Cost Parity. Paris:\nWorld Resources Institute.\nhttps://www.ctc-n.org/sites/www.ctc-n.",
    "page": 17
  },
  {
    "type": "table",
    "content": "Understanding Cost Parity. Paris:\nWorld Resources Institute.\nhttps://www.ctc-n.org/sites/www.ctc-n.org/files/resources/wri14_factsheets_utility_scale_v4.pdf.\n[San20] Sanh, V., Debut, L., Chaumond, J. and Wolf, T., 2019. DistilBERT, a distilled version of BERT: smaller, faster,\ncheaper and lighter. a rXiv preprint arXiv:1910.01108.\n[Sch20] Schwartz, R., Dodge, J., Smith, N.A. and Etzioni, O., 2020. Green AI. C ommunications of the ACM, 63(12),\npp.54-63. h ttps://cacm.acm.org/magazines/2020/12/248800-green-ai/fulltext.\n[Sch21] Schleier-Smith, J., Sreekanti, V., Khandelwal, A., Carreira, J., Yadwadkar, N., Popa, R., Joseph E. Gonzalez,J.,\nIon Stoica, I., and David A. Patterson, D., 2021 What Serverless Computing Is and Should Become: The Next",
    "page": 17
  },
  {
    "type": "table",
    "content": "oica, I., and David A. Patterson, D., 2021 What Serverless Computing Is and Should Become: The Next\nPhase of Cloud Computing, C ommunications of the ACM, 6 4(5).\n[Sha17] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G. and Dean, J., 2017. Outrageously large\nneural networks: The sparsely-gated mixture-of-experts layer. ICLR 2017. a rXiv preprint arXiv:1701.06538.\n[So19] So, D., Le, Q. and Liang, C., 2019, May. The Evolved Transformer. In International Conference on Machine\nLearning 2019 (pp. 5877-5886). PMLR. a rXiv preprint arXiv:1901.11117.\n[Str19] Strubell, E., Ganesh, A. and McCallum, A., 2019. Energy and policy considerations for deep learning in NLP.\nACL 2019. a rXiv preprint arXiv:1906.02243.\n[Sut21] Sutskever, I. Personal Communication, February 4, 2021.",
    "page": 17
  },
  {
    "type": "table",
    "content": ". a rXiv preprint arXiv:1906.02243.\n[Sut21] Sutskever, I. Personal Communication, February 4, 2021.\n[Tan19] Tan, M. and Le, Q., 2019, May. EfficientNet: Rethinking model scaling for convolutional neural networks. In\nInternational Conference on Machine Learning (pp. 6105-6114). PMLR. a rXiv preprint arXiv:1905.11946.\n[USE21] US Energy Information Administration, 2021, FAQ How much carbon dioxide is produced per kilowatt hour of\nU.S. electricity generation? h ttps://www.eia.gov/tools/faqs/faq.php?id=74&t=11.\n[Vas17] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L. and Polosukhin, I., 2017.\nAttention is all you need. NeurIPS 2017. a rXiv preprint arXiv:1706.03762.\n[Wan20] Wang, Y., Yao, Q., Kwok, J.T. and Ni, L.M., 2020.",
    "page": 17
  },
  {
    "type": "table",
    "content": "PS 2017. a rXiv preprint arXiv:1706.03762.\n[Wan20] Wang, Y., Yao, Q., Kwok, J.T. and Ni, L.M., 2020. Generalizing from a few examples: A survey on few-shot\nlearning. A CM Computing Surveys, 53(3), pp.1-34.\n17",
    "page": 17
  },
  {
    "type": "text",
    "content": "Appendix A. Details of CO Estimates for Four Large NLP Models in Tables 1 and 4\n2\nWe describe below how we derived the values in Tables 1 and 4.\n● Datacenter Gross CO e/KWh (Table 1, row 4; Table 4, row 7): T he US Average is from [USE21]. For\n2\nGoogle, we used the CO e per KWh in the datacenter based at the time that the DNNs ran. ( H ere is a\n2\nlink for annual CFE% for Google Cloud. ) For Microsoft, we use the 2020 US national average.\n● Datacenter Net CO e/KWh (Table 1, row 5; Table 4, row 8): N o change from above except for Google,\n2\nwhere we used the net CO e per KWh in the datacenter based on the 24/7 carbon-free energy\n2\nmethodology to estimate net carbon emissions at the time2 8 that the DNNs ran (see Section 2.4 and\nAppendix B).",
    "page": 18
  },
  {
    "type": "text",
    "content": "to estimate net carbon emissions at the time2 8 that the DNNs ran (see Section 2.4 and\nAppendix B).\n● PUE (Table 1, row 6; Table 4, row 9): We use the Google datacenter PUE where the DNNs ran\n(published at h ttps://www.google.com/about/datacenters/efficiency/) . OpenAI told us that the PUE for\nthe datacenter where GPT-3 ran was 1.10 [Sut21].\n● Measured Average Power (Table 1, row 9; Table 4, row 12): At Google we measured actual power\nusage rather than use Thermal Design Power (TDP), as TDP is a worst case for the chip. System\npower measurement includes the memory, fans, CPU host, network interface and so on, similar to the\nmethodology of [Str19]. OpenAI measured V100s as running GPT-3 at 330W. GPUs can run on",
    "page": 18
  },
  {
    "type": "text",
    "content": "ilar to the\nmethodology of [Str19]. OpenAI measured V100s as running GPT-3 at 330W. GPUs can run on\naverage closer to its TDP due to GPU's having Turbo Mode and Dynamic Voltage Frequency Scaling,\nnot found in TPU v2/v3.\n● Measured Performance (Table 1, row 10; Table 4, row 13): Profiling data was obtained via Google's\ninternal performance analysis tool, Xprof. Measured FLOPs/s are calculated as the number of\ncomputed operations divided by execution time.\n● Number of Chips (Table 1, row 11; Table 4, row 14): We know the number of processors for the Google\nmodels. N VIDIA’s press release about GPT-3 suggests OpenAI used 10,000 V100 GPUs for GPT-3.\n● Training time (Table 1, row 12; Table 4, row 15): We have the exact training time for Google DNNs.",
    "page": 18
  },
  {
    "type": "text",
    "content": "Training time (Table 1, row 12; Table 4, row 15): We have the exact training time for Google DNNs.\nOpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].\nOpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec [Sut21]. It takes ~14.8 days for 10,000\nGPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS. For the CO e calculation, it doesn’t\n2\nactually matter whether it takes 2 weeks on 10,000 GPUs or 20 weeks on 1,000 GPUs, but we need\none number for Table 4, so we used NVIDIA’s suggestion of 10,000 GPUs.\n● Total Computation (Table 1, row 13; Table 4, row 16): W e calculate from measured performance,\nnumber of chips, and days to train (except for GPT-3, as OpenAI published the total FLOPS).\n● % of Google 2019 Energy Consumption.",
    "page": 18
  },
  {
    "type": "text",
    "content": "rain (except for GPT-3, as OpenAI published the total FLOPS).\n● % of Google 2019 Energy Consumption. (Table 4, row 17): F or all models (even those not actually run\nin Google datacenters or not run in 2019), we calculate the percentage of Google’s total energy\nconsumption of 12.2 Terawatt-hours in 2019 [Goo20].\n● Ratio of round trips (Table 4, row 22). To give perspective on the CO e cost of training a model is\n2\ncompared to other activities, we show the CO e of passenger jets. G oogle Flights calculated the\n2\naverage CO emission for all the direct flights between San Francisco (SFO) and New York (JFK) in its\n2\ndatabase as 90.2t, so the average round trip is 180.4t. (This is for the whole plane, not just for one\npassenger.",
    "page": 18
  },
  {
    "type": "text",
    "content": "0.2t, so the average round trip is 180.4t. (This is for the whole plane, not just for one\npassenger.) Google Flights relies on this E uropean Environmental Agency guidebook for these\ncalculations and includes the minimum bounds for RF and NOx factor from Figure 6b in [Kär18].\n● % Carbon Free Energy (Table 1, row 17; Table 4, row 24). Collected for when the models were run.\n28 All the 2020 datacenter measurements are provisional, awaiting final validation in May 2021\n18",
    "page": 18
  },
  {
    "type": "table",
    "content": "TABLE (Page 18):\n\nAppendix A. Details of CO Estimates for Four Large NLP Models in Tables 1 and 4\n2\nWe describe below how we derived the values in Tables 1 and 4.\n● Datacenter Gross CO e/KWh (Table 1, row 4; Table 4, row 7): T he US Average is from [USE21]. For\n2\nGoogle, we used the CO e per KWh in the datacenter based at the time that the DNNs ran. ( H ere is a\n2\nlink for annual CFE% for Google Cloud. ) For Microsoft, we use the 2020 US national average.\n● Datacenter Net CO e/KWh (Table 1, row 5; Table 4, row 8): N o change from above except for Google,\n2\nwhere we used the net CO e per KWh in the datacenter based on the 24/7 carbon-free energy\n2\nmethodology to estimate net carbon emissions at the time2 8 that the DNNs ran (see Section 2.4 and\nAppendix B).",
    "page": 18
  },
  {
    "type": "table",
    "content": "to estimate net carbon emissions at the time2 8 that the DNNs ran (see Section 2.4 and\nAppendix B).\n● PUE (Table 1, row 6; Table 4, row 9): We use the Google datacenter PUE where the DNNs ran\n(published at h ttps://www.google.com/about/datacenters/efficiency/) . OpenAI told us that the PUE for\nthe datacenter where GPT-3 ran was 1.10 [Sut21].\n● Measured Average Power (Table 1, row 9; Table 4, row 12): At Google we measured actual power\nusage rather than use Thermal Design Power (TDP), as TDP is a worst case for the chip. System\npower measurement includes the memory, fans, CPU host, network interface and so on, similar to the\nmethodology of [Str19]. OpenAI measured V100s as running GPT-3 at 330W. GPUs can run on",
    "page": 18
  },
  {
    "type": "table",
    "content": "ilar to the\nmethodology of [Str19]. OpenAI measured V100s as running GPT-3 at 330W. GPUs can run on\naverage closer to its TDP due to GPU's having Turbo Mode and Dynamic Voltage Frequency Scaling,\nnot found in TPU v2/v3.\n● Measured Performance (Table 1, row 10; Table 4, row 13): Profiling data was obtained via Google's\ninternal performance analysis tool, Xprof. Measured FLOPs/s are calculated as the number of\ncomputed operations divided by execution time.\n● Number of Chips (Table 1, row 11; Table 4, row 14): We know the number of processors for the Google\nmodels. N VIDIA’s press release about GPT-3 suggests OpenAI used 10,000 V100 GPUs for GPT-3.\n● Training time (Table 1, row 12; Table 4, row 15): We have the exact training time for Google DNNs.",
    "page": 18
  },
  {
    "type": "table",
    "content": "Training time (Table 1, row 12; Table 4, row 15): We have the exact training time for Google DNNs.\nOpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].\nOpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec [Sut21]. It takes ~14.8 days for 10,000\nGPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS. For the CO e calculation, it doesn’t\n2\nactually matter whether it takes 2 weeks on 10,000 GPUs or 20 weeks on 1,000 GPUs, but we need\none number for Table 4, so we used NVIDIA’s suggestion of 10,000 GPUs.\n● Total Computation (Table 1, row 13; Table 4, row 16): W e calculate from measured performance,\nnumber of chips, and days to train (except for GPT-3, as OpenAI published the total FLOPS).\n● % of Google 2019 Energy Consumption.",
    "page": 18
  },
  {
    "type": "table",
    "content": "rain (except for GPT-3, as OpenAI published the total FLOPS).\n● % of Google 2019 Energy Consumption. (Table 4, row 17): F or all models (even those not actually run\nin Google datacenters or not run in 2019), we calculate the percentage of Google’s total energy\nconsumption of 12.2 Terawatt-hours in 2019 [Goo20].\n● Ratio of round trips (Table 4, row 22). To give perspective on the CO e cost of training a model is\n2\ncompared to other activities, we show the CO e of passenger jets. G oogle Flights calculated the\n2\naverage CO emission for all the direct flights between San Francisco (SFO) and New York (JFK) in its\n2\ndatabase as 90.2t, so the average round trip is 180.4t. (This is for the whole plane, not just for one\npassenger.",
    "page": 18
  },
  {
    "type": "table",
    "content": "0.2t, so the average round trip is 180.4t. (This is for the whole plane, not just for one\npassenger.) Google Flights relies on this E uropean Environmental Agency guidebook for these\ncalculations and includes the minimum bounds for RF and NOx factor from Figure 6b in [Kär18].\n● % Carbon Free Energy (Table 1, row 17; Table 4, row 24). Collected for when the models were run.\n28 All the 2020 datacenter measurements are provisional, awaiting final validation in May 2021\n18",
    "page": 18
  },
  {
    "type": "text",
    "content": "Appendix B. Carbon Offset and 24/7 Carbon Free Energy\nWhile energy consumption is relatively straightforward, policies to reduce carbon footprint are not. One reason\nis that they have as much to do about economics and accounting as they do about physics. This short\nappendix tries to clarify the distinction between conventional carbon offsets, Google’s goal for 2030 of 24/7\nCarbon Free Energy (CFE) for its global datacenters and campuses, and what it is doing in 2021 to set the\ngroundwork for 2030. Readers interested in greater depth should take a look at [Ryo14, Goog16, Goo21].\nConventional carbon offsets try to create economic incentives to create projects that avoid or remove\nCO e. When pursuing the mitigation of carbon emissions from electricity production and consumption, a\n2",
    "page": 19
  },
  {
    "type": "text",
    "content": ". When pursuing the mitigation of carbon emissions from electricity production and consumption, a\n2\ncompany can match their MWh of consumption with MWh of clean energy through certificates called R ECs\n(R enewable Energy Certificates) . The rules for accounting and compensation, are defined as part of the G HG\nProtocol, under Scope 2 for electricity. Under the current Scope 2 Guidance, 1MWh of energy used in July in,\nsay, Georgia that produces carbon dioxide can be compensated by purchasing 1MWh of CFE in Montana in\nNovember. Typically, the period of accounting is a calendar year. Google achieved carbon neutrality using\nconventional carbon offsets starting in 2007.2 9\nAs part of the G HG Protocol, the W orld Resource Institute defines terms and economic mechanisms to",
    "page": 19
  },
  {
    "type": "text",
    "content": "s part of the G HG Protocol, the W orld Resource Institute defines terms and economic mechanisms to\nensure consistency of claims about carbon. They defined the following [Car21, Ryo14] (also see Figure 8):\n● Additionality: CO e reductions are a dditional if they would not have occurred in the absence of a market\n2\nfor offset credits. Additionality is essential for the quality of carbon offset credits—if their associated\nCO e reductions are not additional, then purchasing offset credits in lieu of reducing your own\n2\nemissions will make climate change worse.\n● The Grid: The transmission and distribution system that connects generators and end-users.\n● Levelized Cost Of Energy (LCOE): The projected total system and operating costs divided by total KWh",
    "page": 19
  },
  {
    "type": "text",
    "content": "evelized Cost Of Energy (LCOE): The projected total system and operating costs divided by total KWh\nproduced over the lifetime of the project or contract.\n● Power Purchase Agreement (PPA): A fixed-price contractual agreement to purchase a power plant’s\nenergy, typically calculated using LCOE.\n● Renewable Energy Certificate (REC) 3 0: A market-based instrument that represents the property rights\nto the environmental, social, and other non-power attributes of renewable electricity generation. The\ngoal is a certificate that ensures the energy purchased is genuinely renewable and not double counted.\nGoogle’s target for 2030 is to go beyond the traditional Scope 2 rules to restrict both the location and the\naccounting period.",
    "page": 19
  },
  {
    "type": "text",
    "content": "to go beyond the traditional Scope 2 rules to restrict both the location and the\naccounting period.\n● Instead of anywhere in a continent, the CFE purchase should be on the same geographically local grid.\n● Instead of the accounting period being one year, the accounting should be within the hour.\nTo achieve 100% 24/7 local CFE, grids would need to offer both real time accounting of the CFE fraction of the\nstandard grid and the generating companies must offer more flexible options to allow consumers to pick CFE\nany time of the day, not just when the wind blows or when the sun shines. Ideally, grid operators and\ngenerating companies will deliver on that vision, and the standards will evolve to certify and quantify the 24/7\nCFE approach. But we are not there yet.",
    "page": 19
  },
  {
    "type": "text",
    "content": "the standards will evolve to certify and quantify the 24/7\nCFE approach. But we are not there yet.\nFigure 8 helps explain what Google is doing today. Google signs long-term contracts as PPAs with\nrenewable energy generating companies to try to cover Google’s electricity consumption.3 1 One benefit of\nlong-term contracts is that they guarantee a reliable income stream for many years and therefore make such\nprojects more easily financeable. To hit its 24/7 target, Google will continue to purchase clean energy from\nvarious sources such as energy storage and energy generation to ensure it has a clean energy supply at all 24\nhours of the day, 7 days a week.\n29 In 2017, Google became the first major company to match 100% of its annual electricity use with renewable",
    "page": 19
  },
  {
    "type": "text",
    "content": "7, Google became the first major company to match 100% of its annual electricity use with renewable\nenergy—purchasing as much clean energy as it consumed —which it has done for three consecutive years.\n30 RECs are more properly called E nergy Attribute Certificates. Europe calls them G uarantees of Origin (G Os) , not RECs.\n31 Google’s more than 50 long-term contracts to purchase renewable energy resulted in more than $7 billion in new capital\ninvestment in renewable energy projects worldwide as of September 2019 [Goo20].\n19",
    "page": 19
  },
  {
    "type": "table",
    "content": "TABLE (Page 19):\n\nAppendix B. Carbon Offset and 24/7 Carbon Free Energy\nWhile energy consumption is relatively straightforward, policies to reduce carbon footprint are not. One reason\nis that they have as much to do about economics and accounting as they do about physics. This short\nappendix tries to clarify the distinction between conventional carbon offsets, Google’s goal for 2030 of 24/7\nCarbon Free Energy (CFE) for its global datacenters and campuses, and what it is doing in 2021 to set the\ngroundwork for 2030. Readers interested in greater depth should take a look at [Ryo14, Goog16, Goo21].\nConventional carbon offsets try to create economic incentives to create projects that avoid or remove\nCO e.",
    "page": 19
  },
  {
    "type": "table",
    "content": "ional carbon offsets try to create economic incentives to create projects that avoid or remove\nCO e. When pursuing the mitigation of carbon emissions from electricity production and consumption, a\n2\ncompany can match their MWh of consumption with MWh of clean energy through certificates called R ECs\n(R enewable Energy Certificates) . The rules for accounting and compensation, are defined as part of the G HG\nProtocol, under Scope 2 for electricity. Under the current Scope 2 Guidance, 1MWh of energy used in July in,\nsay, Georgia that produces carbon dioxide can be compensated by purchasing 1MWh of CFE in Montana in\nNovember. Typically, the period of accounting is a calendar year. Google achieved carbon neutrality using\nconventional carbon offsets starting in 2007.2 9",
    "page": 19
  },
  {
    "type": "table",
    "content": "ndar year. Google achieved carbon neutrality using\nconventional carbon offsets starting in 2007.2 9\nAs part of the G HG Protocol, the W orld Resource Institute defines terms and economic mechanisms to\nensure consistency of claims about carbon. They defined the following [Car21, Ryo14] (also see Figure 8):\n● Additionality: CO e reductions are a dditional if they would not have occurred in the absence of a market\n2\nfor offset credits. Additionality is essential for the quality of carbon offset credits—if their associated\nCO e reductions are not additional, then purchasing offset credits in lieu of reducing your own\n2\nemissions will make climate change worse.\n● The Grid: The transmission and distribution system that connects generators and end-users.",
    "page": 19
  },
  {
    "type": "table",
    "content": "worse.\n● The Grid: The transmission and distribution system that connects generators and end-users.\n● Levelized Cost Of Energy (LCOE): The projected total system and operating costs divided by total KWh\nproduced over the lifetime of the project or contract.\n● Power Purchase Agreement (PPA): A fixed-price contractual agreement to purchase a power plant’s\nenergy, typically calculated using LCOE.\n● Renewable Energy Certificate (REC) 3 0: A market-based instrument that represents the property rights\nto the environmental, social, and other non-power attributes of renewable electricity generation. The\ngoal is a certificate that ensures the energy purchased is genuinely renewable and not double counted.",
    "page": 19
  },
  {
    "type": "table",
    "content": "l is a certificate that ensures the energy purchased is genuinely renewable and not double counted.\nGoogle’s target for 2030 is to go beyond the traditional Scope 2 rules to restrict both the location and the\naccounting period.\n● Instead of anywhere in a continent, the CFE purchase should be on the same geographically local grid.\n● Instead of the accounting period being one year, the accounting should be within the hour.\nTo achieve 100% 24/7 local CFE, grids would need to offer both real time accounting of the CFE fraction of the\nstandard grid and the generating companies must offer more flexible options to allow consumers to pick CFE\nany time of the day, not just when the wind blows or when the sun shines. Ideally, grid operators and",
    "page": 19
  },
  {
    "type": "table",
    "content": "y time of the day, not just when the wind blows or when the sun shines. Ideally, grid operators and\ngenerating companies will deliver on that vision, and the standards will evolve to certify and quantify the 24/7\nCFE approach. But we are not there yet.\nFigure 8 helps explain what Google is doing today. Google signs long-term contracts as PPAs with\nrenewable energy generating companies to try to cover Google’s electricity consumption.3 1 One benefit of\nlong-term contracts is that they guarantee a reliable income stream for many years and therefore make such\nprojects more easily financeable. To hit its 24/7 target, Google will continue to purchase clean energy from\nvarious sources such as energy storage and energy generation to ensure it has a clean energy supply at all 24",
    "page": 19
  },
  {
    "type": "table",
    "content": "urces such as energy storage and energy generation to ensure it has a clean energy supply at all 24\nhours of the day, 7 days a week.\n29 In 2017, Google became the first major company to match 100% of its annual electricity use with renewable\nenergy—purchasing as much clean energy as it consumed —which it has done for three consecutive years.\n30 RECs are more properly called E nergy Attribute Certificates. Europe calls them G uarantees of Origin (G Os) , not RECs.\n31 Google’s more than 50 long-term contracts to purchase renewable energy resulted in more than $7 billion in new capital\ninvestment in renewable energy projects worldwide as of September 2019 [Goo20].\n19",
    "page": 19
  },
  {
    "type": "text",
    "content": "The percentage of CFE for a datacenter is reported ex-post, after load, production, and grid mix data are\nsettled and made available to Google. With the current 24/7 CFE framework, when Google cannot get 100%\nCFE from the grid plus its clean energy contracts in a given hour, the shortfall counts against the goal. When\nthe grid and renewable energy contracts overshoot in a given hour, Google doesn’t get any extra credit for it,\nas the accounting period is reset every hour.3 2 Since Google can estimate how much CFE is expected in a\nspecific region based on the grid and its multi-year clean energy contract, it incentivizes programs to run in this\nregion.3 3\nTables 1 and 4 show this distinction as g ross CO e (energy from the grid) and the n et CO e (after applying\n2 2",
    "page": 20
  },
  {
    "type": "text",
    "content": "4 show this distinction as g ross CO e (energy from the grid) and the n et CO e (after applying\n2 2\nthe 24/7 local renewable energy purchase from the long-term contracts). Since you can’t label electrons, there\nis no guarantee that Google is using exactly the same clean energy that it paid for, but in our view the overall\neffect is the same.\nAlas, Google’s large models in Table 4 were run in the Georgia datacenter, where in the past there was no\nor little difference between gross and net CO e. Regions that have generator companies that can supply clean\n2\nenergy 24/7 and offer marketplaces that allow companies to acquire clean energy at any time of day will be\nmore compelling to expand future growth of compute from a carbon impact perspective. A great example is",
    "page": 20
  },
  {
    "type": "text",
    "content": "compelling to expand future growth of compute from a carbon impact perspective. A great example is\nOklahoma, which allowed Google to average 95.6% net CFE for 2020. This is a case of where the grass\nactually is greener in Oklahoma than in Georgia. As mentioned above, in 2021 many new TPU v4 accelerators\nwill be deployed in windy Oklahoma.\nFigure 8. This figure explains how fixed-floating swaps work for Renewable Energy Certificates (RECs).\n(Reproduced from [Goo16].) Instead of accounting over a full year at a mix of locations as in step 4,\n24/7 CFE does the accounting separately for every hour in the year in the same single location.\n32 Excess CFE from Google projects is used to support other grid load as well as incentivizing additional renewable",
    "page": 20
  },
  {
    "type": "text",
    "content": "om Google projects is used to support other grid load as well as incentivizing additional renewable\ndevelopment by demonstrating demand and driving down prices.\n33 Google even deployed a system in 2020 that s hifts the timing of non-urgent compute tasks (like ML training) to when\ncarbon-free power sources are most plentiful [Rad20]. Its next iteration will even move a task to a new datacenter.\n20",
    "page": 20
  },
  {
    "type": "table",
    "content": "TABLE (Page 20):\n\nThe percentage of CFE for a datacenter is reported ex-post, after load, production, and grid mix data are\nsettled and made available to Google. With the current 24/7 CFE framework, when Google cannot get 100%\nCFE from the grid plus its clean energy contracts in a given hour, the shortfall counts against the goal. When\nthe grid and renewable energy contracts overshoot in a given hour, Google doesn’t get any extra credit for it,\nas the accounting period is reset every hour.3 2 Since Google can estimate how much CFE is expected in a\nspecific region based on the grid and its multi-year clean energy contract, it incentivizes programs to run in this\nregion.3 3\nTables 1 and 4 show this distinction as g ross CO e (energy from the grid) and the n et CO e (after applying\n2 2",
    "page": 20
  },
  {
    "type": "table",
    "content": "4 show this distinction as g ross CO e (energy from the grid) and the n et CO e (after applying\n2 2\nthe 24/7 local renewable energy purchase from the long-term contracts). Since you can’t label electrons, there\nis no guarantee that Google is using exactly the same clean energy that it paid for, but in our view the overall\neffect is the same.\nAlas, Google’s large models in Table 4 were run in the Georgia datacenter, where in the past there was no\nor little difference between gross and net CO e. Regions that have generator companies that can supply clean\n2\nenergy 24/7 and offer marketplaces that allow companies to acquire clean energy at any time of day will be\nmore compelling to expand future growth of compute from a carbon impact perspective. A great example is",
    "page": 20
  },
  {
    "type": "table",
    "content": "compelling to expand future growth of compute from a carbon impact perspective. A great example is\nOklahoma, which allowed Google to average 95.6% net CFE for 2020. This is a case of where the grass\nactually is greener in Oklahoma than in Georgia. As mentioned above, in 2021 many new TPU v4 accelerators\nwill be deployed in windy Oklahoma.\nFigure 8. This figure explains how fixed-floating swaps work for Renewable Energy Certificates (RECs).\n(Reproduced from [Goo16].) Instead of accounting over a full year at a mix of locations as in step 4,\n24/7 CFE does the accounting separately for every hour in the year in the same single location.\n32 Excess CFE from Google projects is used to support other grid load as well as incentivizing additional renewable",
    "page": 20
  },
  {
    "type": "table",
    "content": "om Google projects is used to support other grid load as well as incentivizing additional renewable\ndevelopment by demonstrating demand and driving down prices.\n33 Google even deployed a system in 2020 that s hifts the timing of non-urgent compute tasks (like ML training) to when\ncarbon-free power sources are most plentiful [Rad20]. Its next iteration will even move a task to a new datacenter.\n20",
    "page": 20
  },
  {
    "type": "text",
    "content": "Appendix C. Details of a C O e Estimate for NAS in an Average Datacenter\n2\n[Str19] estimates the CO e for the neural architecture search (NAS) to find the more-efficient Evolved\n2\nTransformer architecture done by [So19] at Google as 626,155 pounds (284 tCO e). The estimate in [Str19]\n2\nwas done for the hypothetical scenario of running the computation on P100 GPUs in the average U.S.\ndatacenter with the average U.S. grid energy mix. The authors of this note represent a superset of the authors\nof [So19], and we agree that the information needed for an accurate estimate was scattered in several\nsubsections in the So e t al. paper, which makes it difficult to determine the actual CO e. This experience is one\n2",
    "page": 21
  },
  {
    "type": "text",
    "content": "e So e t al. paper, which makes it difficult to determine the actual CO e. This experience is one\n2\nreason we suggest that ML conferences encourage future NLP papers that are computationally expensive to\ninclude a calculation of energy consumed and CO e to make sure all the details are included, as it’s difficult to\n2\ndetermine them retrospectively, as we shall see.\nNAS costs in [Str19] are derived from the NAS process described in section 5.2 of [So19]:\n“The search ran for 15K child models, requiring a total of 979M train steps. Over 13K models did not\nmake it past the first hurdle, drastically reducing the resources required to view the 240 thousandth\ntrain step for top models, which would have cost 3.6B training steps for the same number of models\nwithout hurdles.",
    "page": 21
  },
  {
    "type": "text",
    "content": "top models, which would have cost 3.6B training steps for the same number of models\nwithout hurdles. After the search concluded, we then selected the top 20 models and trained them for\nthe full 300K steps, each on a single TPU V.2 chip.”\nThe projection of the So e t al. NAS cost by Strubell e t al. overestimates the actual Evolved Transformer\nsearch cost. Strubell e t al. assumed each evaluation in the search is conducted using a large configuration:\nTransformer (Big) with batch size 32,768. However, So e t al. actually used a small proxy configuration (Section\n3.3 of [So19]) to reduce compute cost (and CO e). This proxy version used Transformer (Base) rather than\n2\nTransformer (Big), reducing the cost/step by 2.3x. It also reduced the training batch size from 32,768 to 4,096",
    "page": 21
  },
  {
    "type": "text",
    "content": "(Big), reducing the cost/step by 2.3x. It also reduced the training batch size from 32,768 to 4,096\nwhile keeping the number of training steps unchanged, reducing the cost/step by a further 8x.\nAs a result, the calculations below suggest that CO e from the misunderstanding about the use of the\n2\nsmaller proxy task were overestimated by a factor of ~18.7:\nAssume the Carbon Emission Estimation Method in [Str19]:\nCO e = num_chips x num_train_steps x hours/train_steps x emission/chip_per_hour\n2\nnum_train_steps = 979,000,000 # From [So19]\nemission_per_chip_per_hour ~= 0.2855296 pounds CO e # From [ Str19] Table 33 4.\n2\nEstimation of Compute Cost in [Str19]:\n8 P100s for batch size 32,768 (packed version) from [Vas17] (4 096 per GPU) :\nnum_chips = 8",
    "page": 21
  },
  {
    "type": "text",
    "content": "Str19]:\n8 P100s for batch size 32,768 (packed version) from [Vas17] (4 096 per GPU) :\nnum_chips = 8\nThe Training speed of Transformer Big on P100 from [Vas17]:\nhours_per_train_steps = 84 hours / 300,000 = 0.00028 (Section 5.2 in [Vas17])\nCO e = 8 * 979,000,000 * 0.00028 * 0.2855296 = 626,155 lbs (284 t)\n2\nEstimation of Compute Cost if using GPUs of the Actual Setting Adopted in [So19]:\n1 P100 for batch size 32,768 / 8=4096 (Section 4.1 second paragraph in [So19]).\nnum_chips = 1 (Section 4.3 in [So19], note that the actual search used one TPU v2 chip to fit the same\nbatch size as one P100)\nTraining speed of Transformer B ase on P100 from [Vas17]:\nhours_per_train_steps = 12 hours / 100,000 = 0.00012 (Section 5.2 in [Vas17])\nCO e = 1 * 979,000,000 * 0.00012 * 0.2855296 = 3 3,544 lbs (15.2 t)",
    "page": 21
  },
  {
    "type": "text",
    "content": ".00012 (Section 5.2 in [Vas17])\nCO e = 1 * 979,000,000 * 0.00012 * 0.2855296 = 3 3,544 lbs (15.2 t)\n2\nAppendix D shows a ~5X further reduction in CO e by adjusting for the hardware and datacenter where the\n2\nNAS occurred rather than for P100s in a hypothetical US average datacenter.\n34 In this calculation, emission_per_chip_per_hour = average power per chip (in Watts) * PUE * lbs CO e per Watt.\n2\n21",
    "page": 21
  },
  {
    "type": "table",
    "content": "TABLE (Page 21):\n\nAppendix C. Details of a C O e Estimate for NAS in an Average Datacenter\n2\n[Str19] estimates the CO e for the neural architecture search (NAS) to find the more-efficient Evolved\n2\nTransformer architecture done by [So19] at Google as 626,155 pounds (284 tCO e). The estimate in [Str19]\n2\nwas done for the hypothetical scenario of running the computation on P100 GPUs in the average U.S.\ndatacenter with the average U.S. grid energy mix. The authors of this note represent a superset of the authors\nof [So19], and we agree that the information needed for an accurate estimate was scattered in several\nsubsections in the So e t al. paper, which makes it difficult to determine the actual CO e. This experience is one\n2",
    "page": 21
  },
  {
    "type": "table",
    "content": "e So e t al. paper, which makes it difficult to determine the actual CO e. This experience is one\n2\nreason we suggest that ML conferences encourage future NLP papers that are computationally expensive to\ninclude a calculation of energy consumed and CO e to make sure all the details are included, as it’s difficult to\n2\ndetermine them retrospectively, as we shall see.\nNAS costs in [Str19] are derived from the NAS process described in section 5.2 of [So19]:\n“The search ran for 15K child models, requiring a total of 979M train steps. Over 13K models did not\nmake it past the first hurdle, drastically reducing the resources required to view the 240 thousandth\ntrain step for top models, which would have cost 3.6B training steps for the same number of models\nwithout hurdles.",
    "page": 21
  },
  {
    "type": "table",
    "content": "top models, which would have cost 3.6B training steps for the same number of models\nwithout hurdles. After the search concluded, we then selected the top 20 models and trained them for\nthe full 300K steps, each on a single TPU V.2 chip.”\nThe projection of the So e t al. NAS cost by Strubell e t al. overestimates the actual Evolved Transformer\nsearch cost. Strubell e t al. assumed each evaluation in the search is conducted using a large configuration:\nTransformer (Big) with batch size 32,768. However, So e t al. actually used a small proxy configuration (Section\n3.3 of [So19]) to reduce compute cost (and CO e). This proxy version used Transformer (Base) rather than\n2\nTransformer (Big), reducing the cost/step by 2.3x. It also reduced the training batch size from 32,768 to 4,096",
    "page": 21
  },
  {
    "type": "table",
    "content": "(Big), reducing the cost/step by 2.3x. It also reduced the training batch size from 32,768 to 4,096\nwhile keeping the number of training steps unchanged, reducing the cost/step by a further 8x.\nAs a result, the calculations below suggest that CO e from the misunderstanding about the use of the\n2\nsmaller proxy task were overestimated by a factor of ~18.7:\nAssume the Carbon Emission Estimation Method in [Str19]:\nCO e = num_chips x num_train_steps x hours/train_steps x emission/chip_per_hour\n2\nnum_train_steps = 979,000,000 # From [So19]\nemission_per_chip_per_hour ~= 0.2855296 pounds CO e # From [ Str19] Table 33 4.\n2\nEstimation of Compute Cost in [Str19]:\n8 P100s for batch size 32,768 (packed version) from [Vas17] (4 096 per GPU) :\nnum_chips = 8",
    "page": 21
  },
  {
    "type": "table",
    "content": "Str19]:\n8 P100s for batch size 32,768 (packed version) from [Vas17] (4 096 per GPU) :\nnum_chips = 8\nThe Training speed of Transformer Big on P100 from [Vas17]:\nhours_per_train_steps = 84 hours / 300,000 = 0.00028 (Section 5.2 in [Vas17])\nCO e = 8 * 979,000,000 * 0.00028 * 0.2855296 = 626,155 lbs (284 t)\n2\nEstimation of Compute Cost if using GPUs of the Actual Setting Adopted in [So19]:\n1 P100 for batch size 32,768 / 8=4096 (Section 4.1 second paragraph in [So19]).\nnum_chips = 1 (Section 4.3 in [So19], note that the actual search used one TPU v2 chip to fit the same\nbatch size as one P100)\nTraining speed of Transformer B ase on P100 from [Vas17]:\nhours_per_train_steps = 12 hours / 100,000 = 0.00012 (Section 5.2 in [Vas17])\nCO e = 1 * 979,000,000 * 0.00012 * 0.2855296 = 3 3,544 lbs (15.2 t)",
    "page": 21
  },
  {
    "type": "table",
    "content": ".00012 (Section 5.2 in [Vas17])\nCO e = 1 * 979,000,000 * 0.00012 * 0.2855296 = 3 3,544 lbs (15.2 t)\n2\nAppendix D shows a ~5X further reduction in CO e by adjusting for the hardware and datacenter where the\n2\nNAS occurred rather than for P100s in a hypothetical US average datacenter.\n34 In this calculation, emission_per_chip_per_hour = average power per chip (in Watts) * PUE * lbs CO e per Watt.\n2\n21",
    "page": 21
  },
  {
    "type": "text",
    "content": "Appendix D. Details of a CO e Estimate for Google’s Actual NAS\n2\nTo calculate the emissions of the actual NAS in [So19] at Google, where the search was actually performed,\nwe must adjust by three more factors beyond the assumptions in Appendix C:\n1. We use Google Georgia datacenter’s PUE from the period in which the search computation was run\n(1.10 in Table 4) instead of the US average in 2018 (1.58).\n2. Strubell e t al. used the US average CO per kilowatt hour (KWh) as calculated by the U.S.\n2\nEnvironmental Protection Agency (EPA) of 0.423 kg per KWh in 2018. For Google, we use the Georgia\ndatacenter’s average CO e/KWh for the month when NAS was performed (0.431 C O e/KWh in Table 4).\n2 2\n3. So e t al. used Google TPU v2 accelerators, not NVIDIA P100 GPUs as modeled in [Str19]. TPU v2s",
    "page": 22
  },
  {
    "type": "text",
    "content": "3. So e t al. used Google TPU v2 accelerators, not NVIDIA P100 GPUs as modeled in [Str19]. TPU v2s\nare much faster, so the search process takes 32,633 TPU v2 hours instead of 117,780 P100 hours. We\nmeasured the power when running the [So19] NAS computation on TPU v2, including the memory,\nfans, network interfaces, and the CPU host. The average power was 208 Watts. [Str19] estimated the\npower per P100 as 189 Watts3 5. The performance/Watt for NAS of TPU v2 improved\n( 117,780 / 32,633 ) * ( 189 / 208 ) or 3.3X.\nOur estimate of the actual NAS search that So e t al. ran at Google after adjusting for the correct datacenter\nPUE, CO e/KWh, and hardware is (6.8 * 24 * 200 * 208 * 1.10 / 1000) * 0.431 / 1000 = 3.2 tCO e (7096 lbs). 3 6\n2 2",
    "page": 22
  },
  {
    "type": "text",
    "content": "and hardware is (6.8 * 24 * 200 * 208 * 1.10 / 1000) * 0.431 / 1000 = 3.2 tCO e (7096 lbs). 3 6\n2 2\nThis actual emissions value is 88X smaller than the incorrect estimate of the carbon emissions of this\nsearch found in Strubell e t al. If we reran the NAS search today on TPU v2s in Google’s Iowa datacenter\nwith 24/7 local, real time net CO e reduction instead of Google’s Georgia datacenter, it would drop from 3.2\n2\ntCO e to 0.6 tCO e (476X smaller). If we reran using newer TPUs, tCO e would shrink further.\n2 2 2\nWhen, where, how, and on which hardware training occurs matters in addition to what DNN is trained,\nwhich is why it’s best to include energy consumed and CO e in a publication rather than relying on others to\n2\nestimate it correctly afterwards.\n35 Strubell e t al.",
    "page": 22
  },
  {
    "type": "text",
    "content": "publication rather than relying on others to\n2\nestimate it correctly afterwards.\n35 Strubell e t al. used a mix of tools to estimate power for GPU, host CPU, and host memory at 189 Watts, which they\nused to estimate NAS. Our measurements for P100 are much higher in Table 4 for Transformer (Big) 296 Watts. We\nincluded everything in the rack like we do for TPUs, including TPU memory, top of rack switch, fans, power supplies, and\nso on. The two systems are running different implementations of the same problem and the CPU hosts are different. One\nissue might be that NVIDIA’s power measurement tool used in [Str18] samples power once a minute, so there may be\nsampling issues.\n36 To put 3.",
    "page": 22
  },
  {
    "type": "text",
    "content": "ment tool used in [Str18] samples power once a minute, so there may be\nsampling issues.\n36 To put 3.2 net tCO e into perspective,Table 1 and Appendix A use Google Flights to calculate the C O e for the average\n2 2\ndirect round trip flights between SFO and JFK as 180.4t. The Boeing 767 that United Airlines flies on that route has 175\nseats. Google Flights uses the historical average of 84.5% seat occupancy, yielding 1.2t of CO e per passenger round\n2\ntrip. Thus, the CO e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.\n2\n22",
    "page": 22
  },
  {
    "type": "table",
    "content": "TABLE (Page 22):\n\nAppendix D. Details of a CO e Estimate for Google’s Actual NAS\n2\nTo calculate the emissions of the actual NAS in [So19] at Google, where the search was actually performed,\nwe must adjust by three more factors beyond the assumptions in Appendix C:\n1. We use Google Georgia datacenter’s PUE from the period in which the search computation was run\n(1.10 in Table 4) instead of the US average in 2018 (1.58).\n2. Strubell e t al. used the US average CO per kilowatt hour (KWh) as calculated by the U.S.\n2\nEnvironmental Protection Agency (EPA) of 0.423 kg per KWh in 2018. For Google, we use the Georgia\ndatacenter’s average CO e/KWh for the month when NAS was performed (0.431 C O e/KWh in Table 4).\n2 2\n3. So e t al.",
    "page": 22
  },
  {
    "type": "table",
    "content": "verage CO e/KWh for the month when NAS was performed (0.431 C O e/KWh in Table 4).\n2 2\n3. So e t al. used Google TPU v2 accelerators, not NVIDIA P100 GPUs as modeled in [Str19]. TPU v2s\nare much faster, so the search process takes 32,633 TPU v2 hours instead of 117,780 P100 hours. We\nmeasured the power when running the [So19] NAS computation on TPU v2, including the memory,\nfans, network interfaces, and the CPU host. The average power was 208 Watts. [Str19] estimated the\npower per P100 as 189 Watts3 5. The performance/Watt for NAS of TPU v2 improved\n( 117,780 / 32,633 ) * ( 189 / 208 ) or 3.3X.\nOur estimate of the actual NAS search that So e t al. ran at Google after adjusting for the correct datacenter\nPUE, CO e/KWh, and hardware is (6.8 * 24 * 200 * 208 * 1.10 / 1000) * 0.431 / 1000 = 3.",
    "page": 22
  },
  {
    "type": "table",
    "content": "t datacenter\nPUE, CO e/KWh, and hardware is (6.8 * 24 * 200 * 208 * 1.10 / 1000) * 0.431 / 1000 = 3.2 tCO e (7096 lbs). 3 6\n2 2\nThis actual emissions value is 88X smaller than the incorrect estimate of the carbon emissions of this\nsearch found in Strubell e t al. If we reran the NAS search today on TPU v2s in Google’s Iowa datacenter\nwith 24/7 local, real time net CO e reduction instead of Google’s Georgia datacenter, it would drop from 3.2\n2\ntCO e to 0.6 tCO e (476X smaller). If we reran using newer TPUs, tCO e would shrink further.\n2 2 2\nWhen, where, how, and on which hardware training occurs matters in addition to what DNN is trained,\nwhich is why it’s best to include energy consumed and CO e in a publication rather than relying on others to\n2\nestimate it correctly afterwards.",
    "page": 22
  },
  {
    "type": "table",
    "content": "umed and CO e in a publication rather than relying on others to\n2\nestimate it correctly afterwards.\n35 Strubell e t al. used a mix of tools to estimate power for GPU, host CPU, and host memory at 189 Watts, which they\nused to estimate NAS. Our measurements for P100 are much higher in Table 4 for Transformer (Big) 296 Watts. We\nincluded everything in the rack like we do for TPUs, including TPU memory, top of rack switch, fans, power supplies, and\nso on. The two systems are running different implementations of the same problem and the CPU hosts are different. One\nissue might be that NVIDIA’s power measurement tool used in [Str18] samples power once a minute, so there may be\nsampling issues.\n36 To put 3.",
    "page": 22
  },
  {
    "type": "table",
    "content": "ment tool used in [Str18] samples power once a minute, so there may be\nsampling issues.\n36 To put 3.2 net tCO e into perspective,Table 1 and Appendix A use Google Flights to calculate the C O e for the average\n2 2\ndirect round trip flights between SFO and JFK as 180.4t. The Boeing 767 that United Airlines flies on that route has 175\nseats. Google Flights uses the historical average of 84.5% seat occupancy, yielding 1.2t of CO e per passenger round\n2\ntrip. Thus, the CO e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.\n2\n22",
    "page": 22
  }
]