[
  {
    "rank": 1,
    "score": 4.721322204801323,
    "content": "to\ndeter escalating rates of natural disaster, and based\non the estimated CO 2emissions listed in Table 1,\n1Sources: (1) Air travel and per-capita consump-\ntion:https://bit.ly/2Hw0xWc ; (2) car lifetime:\nhttps://bit.ly/2Qbr0w1 .\n\nmodel training and development likely make up\na substantial portion of the greenhouse gas emis-\nsions attributed to many NLP researchers.\nTo heighten the awareness of the",
    "type": "text"
  },
  {
    "rank": 2,
    "score": 4.1024634006709455,
    "content": "0,1\nnity to this issue and promote mindful practice and,\"Amazon-AWS\n17%\n24%\n30%\n26%\"\n\"policy, we characterize the dollar cost and carbon\",\"Google\n56%\n14%\n15%\n10%\"\nemissions that result from training the neural net-,\"Microsoft\n32%\n23%\n31%\n10%\"\n\"works\nat\nthe core of many state-of-the-art NLP\",\nmodels. We do this by estimating the kilowatts,Table 2: Percent energy sourced from: Renewable (e.g.\nof ene",
    "type": "table"
  },
  {
    "rank": 3,
    "score": 3.7214060540838116,
    "content": "urces are available, model training also incurs a\nsubstantial cost to the environment due to the en-\nergy required to power this hardware for weeks or\nmonths at a time. Though some of this energy may\ncome from renewable or carbon credit-offset re-\nsources, the high energy demands of these models\nare still a concern since (1) energy is not currently\nderived from carbon-neural sources in many loca-\n",
    "type": "text"
  },
  {
    "rank": 4,
    "score": 3.3195804652191327,
    "content": "equired to train a variety of popular,\"hydro, solar, wind), natural gas, coal and nuclear\nfor\"\n,\"the top 3 cloud compute providers (Cook et al., 2017),\"\n\"off-the-shelf NLP models, which can be converted\",\n,\"compared to the United States,4 China5 and Germany\"\n\"to approximate\ncarbon emissions\nand electricity\",\n,\"(Burger, 2019).\"\n\"costs.\nTo estimate the even greater\nresources re-\",\n\"quired to transfe",
    "type": "table"
  },
  {
    "rank": 5,
    "score": 3.2816227769237316,
    "content": "of popular\noff-the-shelf NLP models, which can be converted\nto approximate carbon emissions and electricity\ncosts. To estimate the even greater resources re-\nquired to transfer an existing model to a new task\nor develop new models, we perform a case study\nof the full computational resources required for the\ndevelopment and tuning of a recent state-of-the-art\nNLP pipeline ( Strubell et al. ,2018 ).",
    "type": "text"
  },
  {
    "rank": 6,
    "score": 3.1727852929933134,
    "content": "0,1\n\"4.1\nCost of training\",\n,\"1\n120\n$52–$175\n$5\"\nTable 3 lists CO2 emissions and estimated cost of,\n,\"24\n2880\n$1238–$4205\n$118\"\ntraining the models described in §2.1. Of note is,\n,\"4789\n239,942\n$103k–$350k\n$9870\"\n\"that TPUs are more cost-efﬁcient\nthan GPUs on\",\n,Table 4: Estimated cost in terms of cloud compute and\nworkloads that make sense for that hardware (e.g.,\n,\"electricity for training:\n(1) ",
    "type": "table"
  },
  {
    "rank": 7,
    "score": 2.7309185955564477,
    "content": "t\ntraining a neu-\"\n\"a result,\ntraining a state-of-the-art model now re-\",ral network might better be allocated to heating a\n\"quires substantial computational\nresources which\",\"family’s home.\nIt\nis estimated that we must cut\"\n\"demand considerable\nenergy,\nalong with the as-\",carbon emissions by half over the next decade to\n\"sociated ﬁnancial and environmental\ncosts.\nRe-\",\"deter escalating rates of n",
    "type": "table"
  },
  {
    "rank": 8,
    "score": 2.38766310725761,
    "content": "7Power\nand carbon footprint are omitted for TPUs due to lack of publi c information on power draw for this hardware.\n4 Experimental results\n4.1 Cost of training\nTable 3lists CO 2emissions and estimated cost of\ntraining the models described in §2.1. Of note is\nthat TPUs are more cost-efﬁcient than GPUs on\nworkloads that make sense for that hardware (e.g.\nBERT). We also see that models emit substan-",
    "type": "text"
  },
  {
    "rank": 9,
    "score": 1.8393231457201773,
    "content": "ump-\",\n,\"Transformer (big)\n192\"\n\"tion.\nAs a result\nthese models are costly to\",\n,\"w/ neural architecture search\n626,155\"\n\"train and develop, both ﬁnancially, due to the\",\ncost of hardware and electricity or cloud com-,\n,Table 1: Estimated CO2 emissions from training com-\n\"pute time, and environmentally, due to the car-\",\n,\"mon NLP models, compared to familiar consumption.1\"\n\"bon footprint\nrequired",
    "type": "table"
  },
  {
    "rank": 10,
    "score": 1.8253020055246485,
    "content": "0,1\n\"three\ncloud service providers.\nThe U.S. break-\",\"(2019)\nreport\nthat\nthe BERT\nence. Devlin et al.\"\ndown of energy is comparable to that of the most,base model (110M parameters) was trained on 16\n\"popular cloud compute service, Amazon Web Ser-\",TPU chips for 4 days (96 hours). NVIDIA reports\n\"vices, so we believe this conversion to provide a\",that they can train a BERT model in 3.3 days (79.2\nr",
    "type": "table"
  },
  {
    "rank": 11,
    "score": 1.8114930141175312,
    "content": "0,1\n\"model\ntraining and development\nlikely make up\",\"Consumer\nRenew.\nGas\nCoal\nNuc.\"\na substantial portion of the greenhouse gas emis-,\"China\n22%\n3%\n65%\n4%\"\nsions attributed to many NLP researchers.,\"Germany\n40%\n7%\n38%\n13%\"\nTo heighten the awareness of the NLP commu-,\"United States\n17%\n35%\n27%\n19%\"\nnity to this issue and promote mindful practice and,\"Amazon-AWS\n17%\n24%\n30%\n26%\"\n\"policy, we characte",
    "type": "table"
  },
  {
    "rank": 12,
    "score": 1.4904047895627837,
    "content": "ty of re-,\n,multiple instances of specialized hardware such as\n\"cently successful neural network models\nfor\",\n\"NLP. Based on these ﬁndings, we propose ac-\",\"GPUs or TPUs,\ntherefore limiting access to these\"\ntionable recommendations to reduce costs and,highly accurate models on the basis of ﬁnances.\nimprove equity in NLP research and practice.,\n,\"Even when these expensive computational\nre-\"\n,\"sourc",
    "type": "table"
  },
  {
    "rank": 13,
    "score": 1.4675075654987138,
    "content": "s energy may\n\"abled impressive\naccuracy improvements\nacross\",\"come from renewable or carbon credit-offset\nre-\"\n\"al.,\nmany fundamental NLP tasks\n(Bahdanau et\",\"sources, the high energy demands of these models\"\n\"2015;\nLuong\net\nal.,\n2015; Dozat\nand Man-\",are still a concern since (1) energy is not currently\n\"2017; Vaswani\net\nal.,\n2017),\nwith\nthe\nning,\",derived from carbon-neural sources in many loca-",
    "type": "table"
  },
  {
    "rank": 14,
    "score": 1.4507910891074935,
    "content": "self-attention models that\nhave become commonplace in NLP, nor do they\nextrapolate power to estimates of carbon and dol-\nlar cost of training.\nAnalysis of hyperparameter tuning has been\nperformed in the context of improved algorithms\nfor hyperparameter search ( Bergstra et al. ,2011 ;\nBergstra and Bengio ,2012 ;Snoek et al. ,2012 ). To\nour knowledge there exists to date no analysis of\nthe computat",
    "type": "text"
  },
  {
    "rank": 15,
    "score": 1.4290860040061673,
    "content": "ptible ($1.46/hr–$2.40/ hr)\nand on-demand ($4.50/hr–$8/hr) pricing as lower and upper\nbounds for TPU v2/3; cheaper bulk contracts are available.\n\nModel Hardware Power (W) Hours kWh ·PUE CO 2e Cloud compute cost\nTransformer base P100x8 1415.78 12 27 26 $41–$140\nTransformer big P100x8 1515.43 84 201 192 $289–$981\nELMo P100x3 517.66 336 275 262 $433–$1472\nBERTbase V100x64 12,041.51 79 1507 1438 $3751",
    "type": "text"
  },
  {
    "rank": 16,
    "score": 1.4290860040061673,
    "content": "pute time and non-trivial carbon emissions.\n4.2 Cost of development: Case study\nTo quantify the computational requirements of\nR&D for a new model we study the logs of\nall training required to develop Linguistically-\nInformed Self-Attention ( Strubell et al. ,2018 ), a\nmulti-task model that performs part-of-speech tag-\nging, labeled dependency parsing, predicate detec-\ntion and semantic role labeli",
    "type": "text"
  },
  {
    "rank": 17,
    "score": 1.4290860040061673,
    "content": "e and gigaﬂops\n\"base model\nrequires 10 hours\nto train for 300k\",\n,required during inference. They also measure av-\n\"steps on one TPUv2 core. This equates to 32,623\",\n,erage power draw required during inference on\n\"hours of TPU or 274,120 hours on 8 P100 GPUs.\",\n,GPUs as a function of batch size. Neither work an-\n\"ELMo.\nThe ELMo model\n(Peters et al., 2018)\",\n,alyzes the recurrent and self-attention",
    "type": "table"
  },
  {
    "rank": 18,
    "score": 1.397719374247828,
    "content": "constantly throughout\nthe\"\nof-the-art BLEU score of 29.7 for English to Ger-,\n,6 month duration of the project. Table 4 lists upper\n\"man machine translation,\nan increase of\njust 0.1\",\n,\"and lower bounds of\nthe estimated cost\nin terms\"\n\"BLEU at\nthe cost of at\nleast $150k in on-demand\",\n,of Google Cloud compute and raw electricity re-\ncompute time and non-trivial carbon emissions.,\n,quired to develo",
    "type": "table"
  },
  {
    "rank": 19,
    "score": 0.0,
    "content": "arXiv:1906.02243v1  [cs.CL]  5 Jun 2019Energy and Policy Considerations for Deep Learning in NLP\nEmma Strubell Ananya Ganesh Andrew McCallum\nCollege of Information and Computer Sciences\nUniversity of Massachusetts Amherst\n{strubell, aganesh, mccallum }@cs.umass.edu\nAbstract\nRecent progress in hardware and methodol-\nogy for training neural networks has ushered\nin a new generation of large networks ",
    "type": "text"
  },
  {
    "rank": 20,
    "score": 0.0,
    "content": "models are costly to\ntrain and develop, both ﬁnancially, due to the\ncost of hardware and electricity or cloud com-\npute time, and environmentally, due to the car-\nbon footprint required to fuel modern tensor\nprocessing hardware. In this paper we bring\nthis issue to the attention of NLP researchers\nby quantifying the approximate ﬁnancial and\nenvironmental costs of training a variety of re-\ncently s",
    "type": "text"
  }
]